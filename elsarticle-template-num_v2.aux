\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\emailauthor{halimnoor@usm.my}{Mohd Halim Mohd Noor \corref {cor1}}
\emailauthor{ayo.ige@aaua.edu.ng}{Ayokunle Olalekan Ige}
\Newlabel{cor1}{1}
\babel@aux{english}{}
\Newlabel{1}{a}
\Newlabel{2}{b}
\citation{shamshirband_review_2021}
\citation{wang_deep_2018}
\citation{pierson_deep_2017}
\citation{dixit_deep_2021}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{sec1}{{1}{2}{Introduction}{section.1}{}}
\citation{dong_survey_2021}
\citation{talaei_khoei_deep_2023}
\citation{alzubaidi_review_2021}
\citation{alom_state_art_2019}
\citation{pouyanfar_survey_2018}
\citation{sarker_deep_2021}
\citation{dong_survey_2021}
\citation{talaei_khoei_deep_2023}
\citation{alzubaidi_review_2021}
\citation{alom_state_art_2019}
\citation{pouyanfar_survey_2018}
\citation{sarker_deep_2021}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of related works.}}{5}{table.1}\protected@file@percent }
\newlabel{table_summary_related_works}{{1}{5}{Summary of related works}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Fundamentals of Deep Learning}{6}{section.2}\protected@file@percent }
\newlabel{sec2}{{2}{6}{Fundamentals of Deep Learning}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Layers}{6}{subsection.2.1}\protected@file@percent }
\citation{lecun_deep_2015}
\citation{noauthor_poloclubcnn-explainer_2025}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A graphical representation of a neuron.}}{7}{figure.1}\protected@file@percent }
\newlabel{fig_layers_neuron}{{1}{7}{A graphical representation of a neuron}{figure.1}{}}
\citation{lecun_deep_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A neuron is connected to a local region of the input data.}}{8}{figure.2}\protected@file@percent }
\newlabel{fig_layers_convolution}{{2}{8}{A neuron is connected to a local region of the input data}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Summaries of the feature map using maximum or average pooling to produce a reduced feature map.}}{9}{figure.3}\protected@file@percent }
\newlabel{fig_layers_pooling}{{3}{9}{Summaries of the feature map using maximum or average pooling to produce a reduced feature map}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Attention Mechanisms}{9}{subsection.2.2}\protected@file@percent }
\citation{hu_squeeze-and-excitation_2019}
\citation{gao_global_2018}
\citation{wang_eca-net_2020}
\citation{liu_tam_2021}
\citation{vaswani_attention_2023}
\citation{noauthor_poloclubtransformer-explainer_2025}
\citation{oktay_attention_2018}
\citation{dosovitskiy_image_2021}
\citation{guo_beyond_2023}
\citation{lecun_efficient_2012}
\citation{hochreiter_gradient_2001}
\citation{dugas_incorporating_2000}
\citation{glorot_deep_2011}
\citation{maas_rectifier_2013}
\citation{misra_mish_2020}
\citation{clevert_fast_2016}
\citation{wang_comprehensive_2022}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Activation Functions}{12}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Parameter Learning and Loss Functions}{12}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Sigmoid activation function.}}{13}{figure.4}\protected@file@percent }
\newlabel{fig_activation_functions_sigmoid}{{4}{13}{Sigmoid activation function}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Hyperbolic tangent activation function.}}{13}{figure.5}\protected@file@percent }
\newlabel{fig_activation_functions_tanh}{{5}{13}{Hyperbolic tangent activation function}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Rectified linear unit activation function.}}{13}{figure.6}\protected@file@percent }
\newlabel{fig_activation_functions_relu}{{6}{13}{Rectified linear unit activation function}{figure.6}{}}
\citation{carter_tensorflow_nodate}
\citation{guo_ai_nodate}
\citation{qian_momentum_1999}
\citation{duchi_adaptive_2011}
\citation{kingma_adam_2017}
\citation{prechelt_early_2012}
\citation{srivastava_dropout_2014}
\citation{srivastava_dropout_2014}
\citation{park_analysis_2017}
\citation{liu_dropout_2023}
\citation{tibshirani_regression_1996}
\citation{zou_regularization_2005}
\citation{nakamura_adaptive_2019}
\citation{ioffe_batch_2015}
\citation{ba_layer_2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Regularization Methods}{15}{subsection.2.5}\protected@file@percent }
\citation{cybenko_approximation_1989}
\citation{hornik_multilayer_1989}
\citation{widrow_neural_1994}
\citation{hochreiter_long_1997}
\citation{cho_learning_2014}
\citation{shi_convolutional_2015}
\@writefile{toc}{\contentsline {section}{\numberline {3}Types of Deep Learning}{16}{section.3}\protected@file@percent }
\newlabel{sec3}{{3}{16}{Types of Deep Learning}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Deep Supervised Learning}{16}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Multilayer Perceptron}{16}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Recurrent Neural Network}{16}{subsubsection.3.1.2}\protected@file@percent }
\citation{van_houdt_review_2020}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A fully connected neural network.}}{17}{figure.7}\protected@file@percent }
\newlabel{fig_deep_sv_learning_mlp}{{7}{17}{A fully connected neural network}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A neural network with recurrent connection.}}{18}{figure.8}\protected@file@percent }
\newlabel{fig_deep_sv_learning_rnn}{{8}{18}{A neural network with recurrent connection}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Convolutional Neural Network}{18}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The architecture of an LSTM unit.}}{19}{figure.9}\protected@file@percent }
\newlabel{fig_deep_sv_learning_lstm}{{9}{19}{The architecture of an LSTM unit}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The architecture of an LSTM unit.}}{19}{figure.10}\protected@file@percent }
\newlabel{fig_deep_sv_learning_gru}{{10}{19}{The architecture of an LSTM unit}{figure.10}{}}
\citation{krizhevsky_imagenet_2012}
\citation{zeiler_visualizing_2013}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces A neural network with convolutional and pooling layers followed by fully connected layers.}}{20}{figure.11}\protected@file@percent }
\newlabel{fig_deep_sv_learning_cnn}{{11}{20}{A neural network with convolutional and pooling layers followed by fully connected layers}{figure.11}{}}
\citation{lin_network_2014}
\citation{simonyan_very_2015}
\citation{szegedy_going_2014}
\citation{szegedy_going_2014}
\citation{srivastava_highway_2015}
\citation{he_deep_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The inception module. Adapted from \citep  {szegedy_going_2014}.}}{22}{figure.12}\protected@file@percent }
\newlabel{fig_deep_sv_learning_cnn_inception}{{12}{22}{The inception module. Adapted from \citep {szegedy_going_2014}}{figure.12}{}}
\citation{he_deep_2015}
\citation{he_deep_2015}
\citation{huang_densely_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces A residual connection which explicitly incorporates features from previous layers into the later layers. Adapted from \citep  {he_deep_2015}.}}{23}{figure.13}\protected@file@percent }
\newlabel{fig_deep_sv_learning_cnn_resnet}{{13}{23}{A residual connection which explicitly incorporates features from previous layers into the later layers. Adapted from \citep {he_deep_2015}}{figure.13}{}}
\citation{huang_densely_2017}
\citation{zagoruyko_wide_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces A 5-layer dense block. Adapted from \citep  {huang_densely_2017}}}{24}{figure.14}\protected@file@percent }
\newlabel{fig_deep_sv_learning_cnn_densenet}{{14}{24}{A 5-layer dense block. Adapted from \citep {huang_densely_2017}}{figure.14}{}}
\citation{xie_aggregated_2017}
\citation{xie_aggregated_2017}
\citation{noauthor_imagenet_nodate}
\citation{noauthor_cifar-10_nodate}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces A cardinal block. Adapted from \citep  {xie_aggregated_2017}}}{25}{figure.15}\protected@file@percent }
\newlabel{fig_deep_sv_learning_cnn_resnext}{{15}{25}{A cardinal block. Adapted from \citep {xie_aggregated_2017}}{figure.15}{}}
\citation{chougrad_deep_2018}
\citation{zhao_novel_2020}
\citation{cinar_hybrid_2022}
\citation{sun_self-attentional_2023}
\citation{dong_improved_2023}
\citation{hou_application_2024}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Applications of Deep Supervised Learning Models}{26}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Top five classification errors on ImageNet.}}{27}{figure.16}\protected@file@percent }
\newlabel{fig_cnn_top_five_errors_imagenet}{{16}{27}{Top five classification errors on ImageNet}{figure.16}{}}
\citation{narotamo_deep_2024}
\citation{zhang_heart_2024}
\citation{mohd_noor_deep_2022}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Top one classification errors on CIFAR-10.}}{28}{figure.17}\protected@file@percent }
\newlabel{fig_cnn_top_one_errors_cifar10}{{17}{28}{Top one classification errors on CIFAR-10}{figure.17}{}}
\citation{hinton_practical_2012}
\citation{hinton_fast_2006}
\citation{hinton_deep_2009}
\citation{romero_quantum_2017}
\citation{li_guided_2020}
\citation{mohd_noor_feature_2021}
\citation{li_comprehensive_2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Deep Unsupervised Learning}{29}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces A restricted Boltzmann machine.}}{30}{figure.18}\protected@file@percent }
\newlabel{fig_deep_unsv_learning_rbm}{{18}{30}{A restricted Boltzmann machine}{figure.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces A deep belief network.}}{30}{figure.19}\protected@file@percent }
\newlabel{fig_deep_unsv_learning_dbn}{{19}{30}{A deep belief network}{figure.19}{}}
\citation{ng_sparse_2011}
\citation{rifai_higher_2011}
\citation{vincent_extracting_2008}
\citation{chen_marginalized_2012}
\citation{kingma_auto-encoding_2013}
\citation{goodfellow_generative_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces An autoencoder.}}{31}{figure.20}\protected@file@percent }
\newlabel{fig_deep_unsv_learning_ae}{{20}{31}{An autoencoder}{figure.20}{}}
\citation{odena_conditional_2017}
\citation{chen_infogan_2016}
\citation{weng_gan_2019}
\citation{karras_progressive_2018}
\citation{karras_style-based_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces A generative adversarial network.}}{32}{figure.21}\protected@file@percent }
\newlabel{fig_deep_unsv_learning_gan}{{21}{32}{A generative adversarial network}{figure.21}{}}
\citation{Tan_2020_CVPR}
\citation{Otter}
\citation{esteva2019guide}
\citation{soori2023artificial}
\citation{hernandez2019systematic}
\citation{lecun_gradient-based_1998}
\citation{simard_best_2003,matsugu_subject_2003}
\citation{krizhevsky_imagenet_2012}
\@writefile{toc}{\contentsline {section}{\numberline {4}State-of-the-art Deep Learning Applications}{33}{section.4}\protected@file@percent }
\newlabel{sec4}{{4}{33}{State-of-the-art Deep Learning Applications}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Computer Vision}{33}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Image Classification}{33}{subsubsection.4.1.1}\protected@file@percent }
\citation{zhao_well-classified_2022}
\citation{huang_asymmetric_2023}
\citation{park_robust_2023}
\citation{nanni_building_2023}
\citation{wortsman_model_2022}
\citation{loh_multi-symmetry_2023}
\citation{xiao_early_2021}
\citation{wu_cvt_2021}
\citation{peng_conformer_2023}
\citation{tu_maxvit_2022}
\citation{ma_convolutional_2024}
\citation{tu_maxvit_2022}
\citation{zhao_well-classified_2022}
\citation{huang_asymmetric_2023}
\citation{park_robust_2023}
\citation{peng_conformer_2023}
\citation{tu_maxvit_2022}
\citation{ma_convolutional_2024}
\citation{girshick_rich_2014}
\citation{girshick_fast_2015}
\citation{ren_faster_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces The architecture of MaxViT \citep  {tu_maxvit_2022}.}}{36}{figure.22}\protected@file@percent }
\newlabel{fig_img_clf_MaxViT}{{22}{36}{The architecture of MaxViT \citep {tu_maxvit_2022}}{figure.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Object Detection}{36}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Summary of state-of-the-art image classification.}}{37}{table.2}\protected@file@percent }
\newlabel{table_summary_imgclf_studies}{{2}{37}{Summary of state-of-the-art image classification}{table.2}{}}
\citation{redmon_you_2016}
\citation{redmon_yolov3_2018}
\citation{bochkovskiy_yolov4_2020}
\citation{liu_ssd_2016}
\citation{lin_focal_2020}
\citation{lin_feature_2017}
\citation{tan_efficientdet_2020}
\citation{tan_efficientnet_2019}
\citation{bodla_soft-nmsimproving_2017}
\citation{liu_adaptive_2019}
\citation{husham_al-badri_adaptive_2023}
\citation{jiang_non-maximum_2024}
\citation{shi_similarity_2024}
\citation{carion_end_end_2020}
\citation{zhu_deformable_2020}
\citation{dai_dynamic_2021}
\citation{jia_detrs_2023}
\citation{huang_teach-detr_2023}
\citation{dai_dynamic_2021}
\citation{husham_al-badri_adaptive_2023}
\citation{jiang_non-maximum_2024}
\citation{shi_similarity_2024}
\citation{dai_dynamic_2021}
\citation{jia_detrs_2023}
\citation{huang_teach-detr_2023}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Summary of state-of-the-art object detection.}}{41}{table.3}\protected@file@percent }
\newlabel{table_summary_objdet_studies}{{3}{41}{Summary of state-of-the-art object detection}{table.3}{}}
\citation{long_fully_2015}
\citation{noh_learning_2015}
\citation{badrinarayanan_segnet_2017}
\citation{chaurasia_linknet_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces The architecture of dynamic DeTR \citep  {dai_dynamic_2021}.}}{42}{figure.23}\protected@file@percent }
\newlabel{fig_obj_det_Dynamic-DETR}{{23}{42}{The architecture of dynamic DeTR \citep {dai_dynamic_2021}}{figure.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Image Segmentation}{42}{subsubsection.4.1.3}\protected@file@percent }
\citation{he_mask_2017}
\citation{liu_path_2018}
\citation{chen_masklab_2018}
\citation{liu_multi-stage_2023}
\citation{liu_covariance_2022}
\citation{strudel_segmenter_2021}
\citation{hatamizadeh_global_2023}
\citation{shi_transformer_2023}
\citation{hatamizadeh_global_2023}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces The architecture of global context ViT \citep  {hatamizadeh_global_2023}.}}{44}{figure.24}\protected@file@percent }
\newlabel{fig_img_seg_GC-ViT}{{24}{44}{The architecture of global context ViT \citep {hatamizadeh_global_2023}}{figure.24}{}}
\citation{liu_multi-stage_2023}
\citation{liu_covariance_2022}
\citation{strudel_segmenter_2021}
\citation{hatamizadeh_global_2023}
\citation{shi_transformer_2023}
\citation{kingma_auto-encoding_2013}
\citation{goodfellow_generative_2014}
\citation{odena_conditional_2017}
\citation{radford_unsupervised_2015}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Summary of state-of-the-art image segmentation.}}{45}{table.4}\protected@file@percent }
\newlabel{table_summary_imgseg_studies}{{4}{45}{Summary of state-of-the-art image segmentation}{table.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Image Generation}{45}{subsubsection.4.1.4}\protected@file@percent }
\citation{zhang_stackgan_2017}
\citation{zhang_stackgan_2018}
\citation{zhang_photographic_2018}
\citation{zhu_dm-gan_2019}
\citation{xu_attngan_2018}
\citation{sun_resfpa-gan_2019}
\citation{cai_dualattn-gan_2019}
\citation{xu_attngan_2018}
\citation{tao_df-gan_2022}
\citation{yang_dmf-gan_2024}
\citation{jiang_-gan_2024}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces The architecture of AttnGAN \citep  {xu_attngan_2018}.}}{47}{figure.25}\protected@file@percent }
\newlabel{fig_image_gen_AttnGAN}{{25}{47}{The architecture of AttnGAN \citep {xu_attngan_2018}}{figure.25}{}}
\citation{yang_dmf-gan_2024}
\citation{tao_df-gan_2022}
\citation{yang_dmf-gan_2024}
\citation{jiang_-gan_2024}
\citation{calvo_intelligent_2000,yu_latent_2008}
\citation{arevian_recurrent_2007}
\citation{huang_optimization_2020}
\citation{wang_convolutional_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces The architecture of DMF-GAN \citep  {yang_dmf-gan_2024}.}}{48}{figure.26}\protected@file@percent }
\newlabel{fig_image_gen_DMF-GAN}{{26}{48}{The architecture of DMF-GAN \citep {yang_dmf-gan_2024}}{figure.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Natural Language Processing}{48}{subsection.4.2}\protected@file@percent }
\citation{liu_bidirectional_2019}
\citation{lin_structured_2017}
\citation{li_bidirectional_2020}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Summary of state-of-the-art image generation.}}{49}{table.5}\protected@file@percent }
\newlabel{table_summary_imggen_studies}{{5}{49}{Summary of state-of-the-art image generation}{table.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Text Classification}{49}{subsubsection.4.2.1}\protected@file@percent }
\citation{devlin_bert_2018}
\citation{lan_albert_2019}
\citation{liu_roberta_2019}
\citation{he_deberta_2020}
\citation{rodrawangpai_improving_2022}
\citation{murfi_bert-based_2024}
\citation{hao_sentiment_2023}
\citation{wang_joint_2018}
\citation{yan_r-transformer_bilstm_2023}
\citation{zhu_bert-based_2023}
\citation{zhu_bert-based_2023}
\citation{rodrawangpai_improving_2022}
\citation{murfi_bert-based_2024}
\citation{hao_sentiment_2023}
\citation{yan_r-transformer_bilstm_2023}
\citation{zhu_bert-based_2023}
\citation{cho_learning_2014,sutskever_sequence_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces The architecture of BERT-MSL \citep  {zhu_bert-based_2023}.}}{52}{figure.27}\protected@file@percent }
\newlabel{fig_text_clf_BERT-MSL}{{27}{52}{The architecture of BERT-MSL \citep {zhu_bert-based_2023}}{figure.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Neural Machine Translation}{52}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Summary of state-of-the-art text classification.}}{53}{table.6}\protected@file@percent }
\newlabel{table_summary_textclf_studies}{{6}{53}{Summary of state-of-the-art text classification}{table.6}{}}
\citation{stahlberg_neural_2020}
\citation{bahdanau_neural_2014}
\citation{bahdanau_neural_2014}
\citation{luong_effective_2015}
\citation{vaswani_attention_2023}
\citation{luong_effective_2015}
\citation{wu_googles_2016}
\citation{vaswani_attention_2023}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces The architecture of an encoder-decoder. Adapted from \citep  {stahlberg_neural_2020}.}}{54}{figure.28}\protected@file@percent }
\newlabel{fig_nmt_encoder_decoder}{{28}{54}{The architecture of an encoder-decoder. Adapted from \citep {stahlberg_neural_2020}}{figure.28}{}}
\citation{wu_googles_2016}
\citation{lupo_encoding_2023}
\citation{rippeth_improving_2023}
\citation{wu_study_2022}
\citation{kim_towards_2023}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces The architecture of Google Neural Machine Translation \citep  {wu_googles_2016}.}}{55}{figure.29}\protected@file@percent }
\newlabel{fig_nmt_GNMT}{{29}{55}{The architecture of Google Neural Machine Translation \citep {wu_googles_2016}}{figure.29}{}}
\citation{gezmu_transformers_2022}
\citation{araabi_optimizing_2020}
\citation{meetei_cues_2023}
\citation{faheem_improving_2024}
\citation{li_towards_2024}
\citation{lupo_encoding_2023}
\citation{rippeth_improving_2023}
\citation{wu_study_2022}
\citation{kim_towards_2023}
\citation{faheem_improving_2024}
\citation{li_towards_2024}
\citation{nie_attention-based_2017}
\citation{yin_neural_2015}
\citation{li_deep_2016}
\citation{wu_building_2020}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Summary of state-of-the-art neural machine translation.}}{57}{table.7}\protected@file@percent }
\newlabel{table_summary_nmt_studies}{{7}{57}{Summary of state-of-the-art neural machine translation}{table.7}{}}
\citation{li_incremental_2019}
\citation{alrowili_biom-transformers_2021}
\citation{alrowili_exploring_2022}
\citation{ma_t-bertsum_2021}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Text Generation}{58}{subsubsection.4.2.3}\protected@file@percent }
\citation{li_feature-aware_2023}
\citation{kwon_class_2024}
\citation{xie_pre-trained_2022}
\citation{hajipoor_gptgan_2025}
\citation{ma_t-bertsum_2021}
\citation{li_feature-aware_2023}
\citation{kwon_class_2024}
\citation{xie_pre-trained_2022}
\citation{hajipoor_gptgan_2025}
\citation{weiser1991computer}
\citation{he2020developing}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Summary of state-of-the-art text generation.}}{60}{table.8}\protected@file@percent }
\newlabel{table_summary_textgen_studies}{{8}{60}{Summary of state-of-the-art text generation}{table.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Time Series and Pervasive Computing}{60}{subsection.4.3}\protected@file@percent }
\citation{ige2022survey}
\citation{mohd_noor_feature_2021}
\citation{singh2017stock}
\citation{zhang2021hoba}
\citation{lei2020deep}
\citation{zhang2019comprehensive}
\citation{ige2022survey}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Human Activity Recognition}{61}{subsubsection.4.3.1}\protected@file@percent }
\citation{dang2020sensor}
\citation{gao2021danhar}
\citation{gupta2021deep}
\citation{erdacs2021human}
\citation{mohd_noor_feature_2021}
\citation{ragab2020random}
\citation{banjarey2022human}
\citation{baraka_similarity_2023}
\citation{baraka_deep_2024}
\citation{shuvo2020hybrid}
\citation{han2022human}
\citation{ige2023wsense}
\citation{deep_hybrid_2019}
\citation{luwe_wearable_2022,shi_novel_2023}
\citation{dua_inception_2023}
\citation{imran_smart-wearable_2024}
\citation{nafea_sensor-based_2021}
\citation{khan_attention_2021}
\citation{ige_deep_2023}
\citation{gao_danhar_2021}
\citation{agac_resource-efficient_2024}
\citation{tang_triple_2022}
\citation{misra_rotate_2020}
\citation{khan_attention_2021}
\citation{chen_transformer_2022}
\citation{sun_efficient_2024}
\citation{lattanzi_are_2025}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces The architecture of multi-head CNN model \citep  {khan_attention_2021}.}}{65}{figure.30}\protected@file@percent }
\newlabel{fig_har_multi-head_cnn}{{30}{65}{The architecture of multi-head CNN model \citep {khan_attention_2021}}{figure.30}{}}
\citation{chan_unified_2021}
\citation{jimale_fully_2022}
\citation{lupion_data_2024}
\citation{kia_human_2024}
\citation{mohammadzadeh_cgan-based_2025}
\citation{dua_inception_2023}
\citation{ige_deep_2023}
\citation{agac_resource-efficient_2024}
\citation{sun_efficient_2024}
\citation{lupion_data_2024}
\citation{kia_human_2024}
\citation{mohammadzadeh_cgan-based_2025}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Summary of state-of-the-art human activity recognition.}}{67}{table.9}\protected@file@percent }
\newlabel{table_summary_har_studies}{{9}{67}{Summary of state-of-the-art human activity recognition}{table.9}{}}
\citation{Nassif2019}
\citation{Tirumala,app11083603}
\citation{khalil2019}
\citation{singh2021spoken}
\citation{jiao2016accent}
\citation{sanchez2022age}
\citation{alnuaim2022speaker}
\citation{srivastava2022speech}
\citation{padmanabhan2015review,Nassif2019}
\citation{mukhamadiyev2022automatic}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Speech Recognition}{68}{subsubsection.4.3.2}\protected@file@percent }
\citation{lu2020automatic}
\citation{HEMA2023109492}
\citation{shewalkar2019performance}
\citation{prabhavalkar2017comparison}
\citation{graves2012sequence}
\citation{chan2015listen}
\citation{jaitly2016online}
\citation{raffel2017online}
\citation{sak2017recurrent}
\citation{szucbeamsearch2019,li2018seq2seq}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Sequence-to-Sequence}}{70}{figure.31}\protected@file@percent }
\newlabel{fig_speech_seq2seq}{{31}{70}{Sequence-to-Sequence}{figure.31}{}}
\citation{chiugoogle}
\citation{prabhavalkar2017comparison}
\citation{baevski_wav2vec_2020}
\citation{radford_robust_2022}
\citation{gulati_conformer_2020}
\citation{stooke_aligner-encoders_2025}
\citation{zhang_breaking_2025}
\citation{wang_disentangled-transformer_2024}
\citation{shakhadri_samba-asr_2025}
\citation{radford_robust_2022}
\citation{stooke_aligner-encoders_2025}
\citation{zhang_breaking_2025}
\citation{wang_disentangled-transformer_2024}
\citation{shakhadri_samba-asr_2025}
\citation{ozbayoglu2020deep}
\citation{singh2017stock}
\citation{lei2020deep}
\citation{shen2021new}
\citation{wang2020portfolio}
\citation{chen2024deep}
\citation{ahnouch2023model}
\citation{SUN2020101160}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Summary of state-of-the-art speech recognition.}}{73}{table.10}\protected@file@percent }
\newlabel{table_summary_speech_studies}{{10}{73}{Summary of state-of-the-art speech recognition}{table.10}{}}
\citation{abedin2021deep}
\citation{wang_sudf-rs_2023}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Finance}{74}{subsubsection.4.3.3}\protected@file@percent }
\citation{nikou2019stock}
\citation{cai2018financial}
\citation{gudelek2017deep}
\citation{eapen2019novel}
\citation{chen_deep_2024}
\citation{mozaffari_predictive_2024}
\citation{li_transformer_2025}
\citation{zhou_informer_2021}
\citation{berti_tlob_2025}
\citation{wang_sudf-rs_2023}
\citation{chen_deep_2024}
\citation{mozaffari_predictive_2024}
\citation{li_transformer_2025}
\citation{berti_tlob_2025}
\citation{LIU2021107187}
\citation{tsao2023heart}
\citation{LIU2021107187}
\citation{LLamedo2011}
\citation{MATHEWS201853}
\citation{zhu2019}
\citation{desai2021low}
\citation{crippa2015multi}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Electrocardiogram (ECG) Classification}{76}{subsubsection.4.3.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Summary of state-of-the-art finance applications.}}{77}{table.11}\protected@file@percent }
\newlabel{table_summary_finance_studies}{{11}{77}{Summary of state-of-the-art finance applications}{table.11}{}}
\citation{acharya2017deep}
\citation{baloglu2019classification}
\citation{singh2018classification}
\citation{prabhakararao2020attentive}
\citation{wang2023single}
\citation{sowmya2022contemplate}
\citation{rai2022hybrid}
\citation{Banerjee2020}
\citation{kusuma_ecg_2022}
\citation{alamatsaz_lightweight_2024}
\citation{chen_automated_2022}
\citation{wang_automated_2021}
\citation{zhang_ecg-based_2020}
\citation{sun_arrhythmia_2024}
\citation{huang_ecg_2024}
\citation{aghaomidi_ecg-sleepnet_2024}
\citation{hasani_liquid_2021}
\citation{zhu_electrocardiogram_2019}
\citation{yang_data_2024}
\citation{msigwa_iot-driven_2024}
\citation{yang_data_2024}
\citation{sun_arrhythmia_2024}
\citation{alamatsaz_lightweight_2024}
\citation{chen_automated_2022}
\citation{huang_ecg_2024}
\citation{aghaomidi_ecg-sleepnet_2024}
\citation{yang_data_2024}
\citation{msigwa_iot-driven_2024}
\citation{Schirrmeister2017}
\citation{gao2021complex}
\citation{Schirrmeister2017}
\citation{Ang7802578}
\citation{shen2022aberrated}
\citation{boonyaki2020}
\citation{sharma2021automated,vaquerizo2023explainable}
\citation{modir2023systematic}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces The architecture of CECG-GAN \citep  {yang_data_2024}.}}{81}{figure.32}\protected@file@percent }
\newlabel{fig_ecg_gan_ae}{{32}{81}{The architecture of CECG-GAN \citep {yang_data_2024}}{figure.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Summary of state-of-the-art ECG classification.}}{82}{table.12}\protected@file@percent }
\newlabel{table_summary_ecg_studies}{{12}{82}{Summary of state-of-the-art ECG classification}{table.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5}Electroencephalography (EEG) Classification}{82}{subsubsection.4.3.5}\protected@file@percent }
\citation{altaheri2023deep}
\citation{brunner2007spatial,delorme2007enhanced,jafarifarmand2019eeg}
\citation{zhang2019novel}
\citation{kumar2016deep}
\citation{tibrewal2022}
\citation{Schirrmeister2017}
\citation{dai2019eeg}
\citation{LiMingaiLSTM}
\citation{Feng2020novel}
\citation{hwang_improving_2023}
\citation{zhao_deep_2015}
\citation{xia_novel_2023}
\citation{hermawan_multi_2024}
\citation{abdulwahhab_detection_2024}
\citation{pandey_subject_2022}
\citation{hassouneh_development_2020}
\citation{pan_multimodal_2024}
\citation{wang_multimodal_2023}
\citation{pan_multimodal_2024}
\citation{wang_multimodal_2023}
\citation{song_eeggan-net_2024}
\citation{corley_deep_2025}
\citation{cai_dhct-gan_2025}
\citation{hermawan_multi_2024}
\citation{pan_multimodal_2024}
\citation{wang_multimodal_2023}
\citation{abdulwahhab_detection_2024}
\citation{song_eeggan-net_2024}
\citation{corley_deep_2025}
\citation{cai_dhct-gan_2025}
\citation{le_application_2025}
\citation{vukicevic_versatile_2025}
\citation{kirillov_segment_2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Robotics}{86}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Object Identification}{86}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Summary of state-of-the-art EEG classification.}}{87}{table.13}\protected@file@percent }
\newlabel{table_summary_eeg_studies}{{13}{87}{Summary of state-of-the-art EEG classification}{table.13}{}}
\citation{liu_automatic_2025}
\citation{ge_deep_2025}
\citation{dai_advanced_2024}
\citation{le_application_2025}
\citation{vukicevic_versatile_2025}
\citation{kirillov_segment_2023}
\citation{ge_deep_2025}
\citation{dai_advanced_2024}
\citation{alotaibi_deep_2024}
\citation{misir_drivable_2024}
\citation{chen_rethinking_2017}
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces Summary of state-of-the-art robot object identification.}}{88}{table.14}\protected@file@percent }
\newlabel{table_summary_objidn_studies}{{14}{88}{Summary of state-of-the-art robot object identification}{table.14}{}}
\citation{cao_orchard_2024}
\citation{liu_single-stage_2025}
\citation{alotaibi_deep_2024}
\citation{misir_drivable_2024}
\citation{cao_orchard_2024}
\citation{liu_single-stage_2025}
\citation{huang_survey_2025}
\citation{asuzu_humanrobot_2025}
\citation{chen_synergai_2024}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Path Extraction and Navigation}{89}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Human-Robot Interaction}{89}{subsubsection.4.4.3}\protected@file@percent }
\citation{allgeuer_when_2024}
\citation{wang_i_2024}
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces Summary of state-of-the-art path extraction and navigation.}}{90}{table.15}\protected@file@percent }
\newlabel{table_summary_nav_studies}{{15}{90}{Summary of state-of-the-art path extraction and navigation}{table.15}{}}
\citation{allgeuer_when_2024}
\citation{asuzu_humanrobot_2025}
\citation{chen_synergai_2024}
\citation{allgeuer_when_2024}
\citation{wang_i_2024}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces A grounded chat architecture \citep  {allgeuer_when_2024}.}}{91}{figure.33}\protected@file@percent }
\newlabel{fig_grounded_chat_architecture}{{33}{91}{A grounded chat architecture \citep {allgeuer_when_2024}}{figure.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Research Challenges}{91}{section.5}\protected@file@percent }
\newlabel{sec5}{{5}{91}{Research Challenges}{section.5}{}}
\citation{tonekaboni_what_2019}
\citation{tjoa_enhancing_2023}
\citation{li_hybrid_2024}
\citation{termritthikun_explainable_2023}
\citation{xiong_explainable_2022}
\citation{fernandes_intrinsic_2023}
\@writefile{lot}{\contentsline {table}{\numberline {16}{\ignorespaces Summary of state-of-the-art human-robot interaction.}}{92}{table.16}\protected@file@percent }
\newlabel{table_summary_hri_studies}{{16}{92}{Summary of state-of-the-art human-robot interaction}{table.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Ethical Issues}{92}{subsection.5.1}\protected@file@percent }
\citation{freire_e-recruitment_2021}
\citation{dass_detecting_2023}
\citation{gicic2023intelligent}
\citation{schaaf_towards_2021}
\citation{giloni_benn_2022}
\citation{iosifidis_fairness-enhancing_2019,kehrenberg_tuning_2020}
\citation{jain_increasing_2023}
\citation{yang_algorithmic_2023}
\citation{raiaan_systematic_2024}
\citation{pruning8794944}
\citation{Yang_CVPR}
\citation{NEURIPS2021_376c6b9f}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Technical Issues}{94}{subsection.5.2}\protected@file@percent }
\citation{ying_enhancing_2024}
\citation{wang_optimizing_2024}
\citation{yan_triple-inertial_2025}
\citation{krasnoproshin_random_2024}
\citation{adversarial2018}
\citation{szegedy2013intriguing}
\citation{tabacof2016adversarial}
\citation{zhang2020adversarial,jiang2019black,esmaeilpour2019robust}
\citation{munappy_data_2022}
\citation{luca_impact_2022}
\citation{zhuang_comprehensive_2020}
\citation{mumuni_data_2022}
\citation{hu_survey_2023,murtaza_synthetic_2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Domain-specific Issues}{96}{subsection.5.3}\protected@file@percent }
\citation{li_survey_2024}
\citation{cacciarelli_active_2024}
\citation{tan_large_2024}
\citation{gui_survey_2024}
\citation{altabrawee_stclr_2025}
\citation{altabrawee_repeat_2024}
\citation{lee_predicting_2021}
\citation{dosovitskiy_image_2021}
\citation{carion_end_end_2020}
\citation{peng_conformer_2023}
\citation{tu_maxvit_2022}
\citation{devlin_bert_2018}
\@writefile{toc}{\contentsline {section}{\numberline {6}Summary and Future Directions}{98}{section.6}\protected@file@percent }
\newlabel{sec6}{{6}{98}{Summary and Future Directions}{section.6}{}}
\citation{khan_attention_2021,ige_deep_2023}
\citation{gao_danhar_2021,agac_resource-efficient_2024,tang_triple_2022}
\citation{lupion_data_2024,kia_human_2024}
\citation{chan_unified_2021}
\citation{alamatsaz_lightweight_2024,hermawan_multi_2024}
\citation{sun_arrhythmia_2024,chen_automated_2022}
\citation{dai2019eeg}
\citation{yang_data_2024,msigwa_iot-driven_2024,song_eeggan-net_2024}
\citation{corley_deep_2025}
\citation{cai_dhct-gan_2025}
\citation{zhu_electrocardiogram_2019}
\citation{yang_data_2024}
\citation{wang_joint_2018}
\citation{zhu_bert-based_2023}
\citation{wu_study_2022,kim_towards_2023}
\citation{li_feature-aware_2023}
\citation{kwon_class_2024}
\citation{tao_df-gan_2022}
\citation{yang_dmf-gan_2024}
\citation{ma_t-bertsum_2021}
\citation{xie_pre-trained_2022}
\citation{li_incremental_2019}
\citation{wang_i_2024}
\citation{chen_synergai_2024}
\citation{allgeuer_when_2024}
\citation{asuzu_humanrobot_2025}
\citation{zhao_well-classified_2022}
\citation{huang_asymmetric_2023}
\citation{maldonado2023owadapt}
\citation{zhu2024irda}
\citation{belhaouari2024oversampling}
\citation{wang2024class}
\citation{ircio2023minimum}
\citation{moles2024exploring}
\citation{zhu2024sfpl}
\citation{jimale_subject_2023}
\citation{zhang_large_2023}
\citation{butlin_consciousness_2023}
\citation{marra_statistical_2024,colelough_neuro-symbolic_2025,bhuyan_neuro-symbolic_2024}
\citation{wu_survey_2025,klusch_quantum_2024}
\citation{shrestha_survey_2022,kudithipudi_neuromorphic_2025}
\citation{fei_towards_2022}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{103}{section.7}\protected@file@percent }
\newlabel{sec7}{{7}{103}{Conclusion}{section.7}{}}
\bibstyle{elsarticle-num}
\bibdata{elsarticle-template-num_v2}
\bibcite{shamshirband_review_2021}{{1}{}{{}}{{}}}
\bibcite{wang_deep_2018}{{2}{}{{}}{{}}}
\bibcite{pierson_deep_2017}{{3}{}{{}}{{}}}
\bibcite{dixit_deep_2021}{{4}{}{{}}{{}}}
\bibcite{dong_survey_2021}{{5}{}{{}}{{}}}
\bibcite{talaei_khoei_deep_2023}{{6}{}{{}}{{}}}
\bibcite{alzubaidi_review_2021}{{7}{}{{}}{{}}}
\bibcite{alom_state_art_2019}{{8}{}{{}}{{}}}
\bibcite{pouyanfar_survey_2018}{{9}{}{{}}{{}}}
\bibcite{sarker_deep_2021}{{10}{}{{}}{{}}}
\bibcite{lecun_deep_2015}{{11}{}{{}}{{}}}
\bibcite{noauthor_poloclubcnn-explainer_2025}{{12}{}{{}}{{}}}
\bibcite{hu_squeeze-and-excitation_2019}{{13}{}{{}}{{}}}
\bibcite{gao_global_2018}{{14}{}{{}}{{}}}
\bibcite{wang_eca-net_2020}{{15}{}{{}}{{}}}
\bibcite{liu_tam_2021}{{16}{}{{}}{{}}}
\bibcite{vaswani_attention_2023}{{17}{}{{}}{{}}}
\bibcite{noauthor_poloclubtransformer-explainer_2025}{{18}{}{{}}{{}}}
\bibcite{oktay_attention_2018}{{19}{}{{}}{{}}}
\bibcite{dosovitskiy_image_2021}{{20}{}{{}}{{}}}
\bibcite{guo_beyond_2023}{{21}{}{{}}{{}}}
\bibcite{lecun_efficient_2012}{{22}{}{{}}{{}}}
\bibcite{hochreiter_gradient_2001}{{23}{}{{}}{{}}}
\bibcite{dugas_incorporating_2000}{{24}{}{{}}{{}}}
\bibcite{glorot_deep_2011}{{25}{}{{}}{{}}}
\bibcite{maas_rectifier_2013}{{26}{}{{}}{{}}}
\bibcite{misra_mish_2020}{{27}{}{{}}{{}}}
\bibcite{clevert_fast_2016}{{28}{}{{}}{{}}}
\bibcite{wang_comprehensive_2022}{{29}{}{{}}{{}}}
\bibcite{carter_tensorflow_nodate}{{30}{}{{}}{{}}}
\bibcite{guo_ai_nodate}{{31}{}{{}}{{}}}
\bibcite{qian_momentum_1999}{{32}{}{{}}{{}}}
\bibcite{duchi_adaptive_2011}{{33}{}{{}}{{}}}
\bibcite{kingma_adam_2017}{{34}{}{{}}{{}}}
\bibcite{prechelt_early_2012}{{35}{}{{}}{{}}}
\bibcite{srivastava_dropout_2014}{{36}{}{{}}{{}}}
\bibcite{park_analysis_2017}{{37}{}{{}}{{}}}
\bibcite{liu_dropout_2023}{{38}{}{{}}{{}}}
\bibcite{tibshirani_regression_1996}{{39}{}{{}}{{}}}
\bibcite{zou_regularization_2005}{{40}{}{{}}{{}}}
\bibcite{nakamura_adaptive_2019}{{41}{}{{}}{{}}}
\bibcite{ioffe_batch_2015}{{42}{}{{}}{{}}}
\bibcite{ba_layer_2016}{{43}{}{{}}{{}}}
\bibcite{cybenko_approximation_1989}{{44}{}{{}}{{}}}
\bibcite{hornik_multilayer_1989}{{45}{}{{}}{{}}}
\bibcite{widrow_neural_1994}{{46}{}{{}}{{}}}
\bibcite{hochreiter_long_1997}{{47}{}{{}}{{}}}
\bibcite{cho_learning_2014}{{48}{}{{}}{{}}}
\bibcite{shi_convolutional_2015}{{49}{}{{}}{{}}}
\bibcite{van_houdt_review_2020}{{50}{}{{}}{{}}}
\bibcite{krizhevsky_imagenet_2012}{{51}{}{{}}{{}}}
\bibcite{zeiler_visualizing_2013}{{52}{}{{}}{{}}}
\bibcite{lin_network_2014}{{53}{}{{}}{{}}}
\bibcite{simonyan_very_2015}{{54}{}{{}}{{}}}
\bibcite{szegedy_going_2014}{{55}{}{{}}{{}}}
\bibcite{srivastava_highway_2015}{{56}{}{{}}{{}}}
\bibcite{he_deep_2015}{{57}{}{{}}{{}}}
\bibcite{huang_densely_2017}{{58}{}{{}}{{}}}
\bibcite{zagoruyko_wide_2017}{{59}{}{{}}{{}}}
\bibcite{xie_aggregated_2017}{{60}{}{{}}{{}}}
\bibcite{noauthor_imagenet_nodate}{{61}{}{{}}{{}}}
\bibcite{noauthor_cifar-10_nodate}{{62}{}{{}}{{}}}
\bibcite{chougrad_deep_2018}{{63}{}{{}}{{}}}
\bibcite{zhao_novel_2020}{{64}{}{{}}{{}}}
\bibcite{cinar_hybrid_2022}{{65}{}{{}}{{}}}
\bibcite{sun_self-attentional_2023}{{66}{}{{}}{{}}}
\bibcite{dong_improved_2023}{{67}{}{{}}{{}}}
\bibcite{hou_application_2024}{{68}{}{{}}{{}}}
\bibcite{narotamo_deep_2024}{{69}{}{{}}{{}}}
\bibcite{zhang_heart_2024}{{70}{}{{}}{{}}}
\bibcite{mohd_noor_deep_2022}{{71}{}{{}}{{}}}
\bibcite{hinton_practical_2012}{{72}{}{{}}{{}}}
\bibcite{hinton_fast_2006}{{73}{}{{}}{{}}}
\bibcite{hinton_deep_2009}{{74}{}{{}}{{}}}
\bibcite{romero_quantum_2017}{{75}{}{{}}{{}}}
\bibcite{li_guided_2020}{{76}{}{{}}{{}}}
\bibcite{mohd_noor_feature_2021}{{77}{}{{}}{{}}}
\bibcite{li_comprehensive_2023}{{78}{}{{}}{{}}}
\bibcite{ng_sparse_2011}{{79}{}{{}}{{}}}
\bibcite{rifai_higher_2011}{{80}{}{{}}{{}}}
\bibcite{vincent_extracting_2008}{{81}{}{{}}{{}}}
\bibcite{chen_marginalized_2012}{{82}{}{{}}{{}}}
\bibcite{kingma_auto-encoding_2013}{{83}{}{{}}{{}}}
\bibcite{goodfellow_generative_2014}{{84}{}{{}}{{}}}
\bibcite{odena_conditional_2017}{{85}{}{{}}{{}}}
\bibcite{chen_infogan_2016}{{86}{}{{}}{{}}}
\bibcite{weng_gan_2019}{{87}{}{{}}{{}}}
\bibcite{karras_progressive_2018}{{88}{}{{}}{{}}}
\bibcite{karras_style-based_2019}{{89}{}{{}}{{}}}
\bibcite{Tan_2020_CVPR}{{90}{}{{}}{{}}}
\bibcite{Otter}{{91}{}{{}}{{}}}
\bibcite{esteva2019guide}{{92}{}{{}}{{}}}
\bibcite{soori2023artificial}{{93}{}{{}}{{}}}
\bibcite{hernandez2019systematic}{{94}{}{{}}{{}}}
\bibcite{lecun_gradient-based_1998}{{95}{}{{}}{{}}}
\bibcite{simard_best_2003}{{96}{}{{}}{{}}}
\bibcite{matsugu_subject_2003}{{97}{}{{}}{{}}}
\bibcite{zhao_well-classified_2022}{{98}{}{{}}{{}}}
\bibcite{huang_asymmetric_2023}{{99}{}{{}}{{}}}
\bibcite{park_robust_2023}{{100}{}{{}}{{}}}
\bibcite{nanni_building_2023}{{101}{}{{}}{{}}}
\bibcite{wortsman_model_2022}{{102}{}{{}}{{}}}
\bibcite{loh_multi-symmetry_2023}{{103}{}{{}}{{}}}
\bibcite{xiao_early_2021}{{104}{}{{}}{{}}}
\bibcite{wu_cvt_2021}{{105}{}{{}}{{}}}
\bibcite{peng_conformer_2023}{{106}{}{{}}{{}}}
\bibcite{tu_maxvit_2022}{{107}{}{{}}{{}}}
\bibcite{ma_convolutional_2024}{{108}{}{{}}{{}}}
\bibcite{girshick_rich_2014}{{109}{}{{}}{{}}}
\bibcite{girshick_fast_2015}{{110}{}{{}}{{}}}
\bibcite{ren_faster_2015}{{111}{}{{}}{{}}}
\bibcite{redmon_you_2016}{{112}{}{{}}{{}}}
\bibcite{redmon_yolov3_2018}{{113}{}{{}}{{}}}
\bibcite{bochkovskiy_yolov4_2020}{{114}{}{{}}{{}}}
\bibcite{liu_ssd_2016}{{115}{}{{}}{{}}}
\bibcite{lin_focal_2020}{{116}{}{{}}{{}}}
\bibcite{lin_feature_2017}{{117}{}{{}}{{}}}
\bibcite{tan_efficientdet_2020}{{118}{}{{}}{{}}}
\bibcite{tan_efficientnet_2019}{{119}{}{{}}{{}}}
\bibcite{bodla_soft-nmsimproving_2017}{{120}{}{{}}{{}}}
\bibcite{liu_adaptive_2019}{{121}{}{{}}{{}}}
\bibcite{husham_al-badri_adaptive_2023}{{122}{}{{}}{{}}}
\bibcite{jiang_non-maximum_2024}{{123}{}{{}}{{}}}
\bibcite{shi_similarity_2024}{{124}{}{{}}{{}}}
\bibcite{carion_end_end_2020}{{125}{}{{}}{{}}}
\bibcite{zhu_deformable_2020}{{126}{}{{}}{{}}}
\bibcite{dai_dynamic_2021}{{127}{}{{}}{{}}}
\bibcite{jia_detrs_2023}{{128}{}{{}}{{}}}
\bibcite{huang_teach-detr_2023}{{129}{}{{}}{{}}}
\bibcite{long_fully_2015}{{130}{}{{}}{{}}}
\bibcite{noh_learning_2015}{{131}{}{{}}{{}}}
\bibcite{badrinarayanan_segnet_2017}{{132}{}{{}}{{}}}
\bibcite{chaurasia_linknet_2017}{{133}{}{{}}{{}}}
\bibcite{he_mask_2017}{{134}{}{{}}{{}}}
\bibcite{liu_path_2018}{{135}{}{{}}{{}}}
\bibcite{chen_masklab_2018}{{136}{}{{}}{{}}}
\bibcite{liu_multi-stage_2023}{{137}{}{{}}{{}}}
\bibcite{liu_covariance_2022}{{138}{}{{}}{{}}}
\bibcite{strudel_segmenter_2021}{{139}{}{{}}{{}}}
\bibcite{hatamizadeh_global_2023}{{140}{}{{}}{{}}}
\bibcite{shi_transformer_2023}{{141}{}{{}}{{}}}
\bibcite{radford_unsupervised_2015}{{142}{}{{}}{{}}}
\bibcite{zhang_stackgan_2017}{{143}{}{{}}{{}}}
\bibcite{zhang_stackgan_2018}{{144}{}{{}}{{}}}
\bibcite{zhang_photographic_2018}{{145}{}{{}}{{}}}
\bibcite{zhu_dm-gan_2019}{{146}{}{{}}{{}}}
\bibcite{xu_attngan_2018}{{147}{}{{}}{{}}}
\bibcite{sun_resfpa-gan_2019}{{148}{}{{}}{{}}}
\bibcite{cai_dualattn-gan_2019}{{149}{}{{}}{{}}}
\bibcite{tao_df-gan_2022}{{150}{}{{}}{{}}}
\bibcite{yang_dmf-gan_2024}{{151}{}{{}}{{}}}
\bibcite{jiang_-gan_2024}{{152}{}{{}}{{}}}
\bibcite{calvo_intelligent_2000}{{153}{}{{}}{{}}}
\bibcite{yu_latent_2008}{{154}{}{{}}{{}}}
\bibcite{arevian_recurrent_2007}{{155}{}{{}}{{}}}
\bibcite{huang_optimization_2020}{{156}{}{{}}{{}}}
\bibcite{wang_convolutional_2019}{{157}{}{{}}{{}}}
\bibcite{liu_bidirectional_2019}{{158}{}{{}}{{}}}
\bibcite{lin_structured_2017}{{159}{}{{}}{{}}}
\bibcite{li_bidirectional_2020}{{160}{}{{}}{{}}}
\bibcite{devlin_bert_2018}{{161}{}{{}}{{}}}
\bibcite{lan_albert_2019}{{162}{}{{}}{{}}}
\bibcite{liu_roberta_2019}{{163}{}{{}}{{}}}
\bibcite{he_deberta_2020}{{164}{}{{}}{{}}}
\bibcite{rodrawangpai_improving_2022}{{165}{}{{}}{{}}}
\bibcite{murfi_bert-based_2024}{{166}{}{{}}{{}}}
\bibcite{hao_sentiment_2023}{{167}{}{{}}{{}}}
\bibcite{wang_joint_2018}{{168}{}{{}}{{}}}
\bibcite{yan_r-transformer_bilstm_2023}{{169}{}{{}}{{}}}
\bibcite{zhu_bert-based_2023}{{170}{}{{}}{{}}}
\bibcite{sutskever_sequence_2014}{{171}{}{{}}{{}}}
\bibcite{stahlberg_neural_2020}{{172}{}{{}}{{}}}
\bibcite{bahdanau_neural_2014}{{173}{}{{}}{{}}}
\bibcite{luong_effective_2015}{{174}{}{{}}{{}}}
\bibcite{wu_googles_2016}{{175}{}{{}}{{}}}
\bibcite{lupo_encoding_2023}{{176}{}{{}}{{}}}
\bibcite{rippeth_improving_2023}{{177}{}{{}}{{}}}
\bibcite{wu_study_2022}{{178}{}{{}}{{}}}
\bibcite{kim_towards_2023}{{179}{}{{}}{{}}}
\bibcite{gezmu_transformers_2022}{{180}{}{{}}{{}}}
\bibcite{araabi_optimizing_2020}{{181}{}{{}}{{}}}
\bibcite{meetei_cues_2023}{{182}{}{{}}{{}}}
\bibcite{faheem_improving_2024}{{183}{}{{}}{{}}}
\bibcite{li_towards_2024}{{184}{}{{}}{{}}}
\bibcite{nie_attention-based_2017}{{185}{}{{}}{{}}}
\bibcite{yin_neural_2015}{{186}{}{{}}{{}}}
\bibcite{li_deep_2016}{{187}{}{{}}{{}}}
\bibcite{wu_building_2020}{{188}{}{{}}{{}}}
\bibcite{li_incremental_2019}{{189}{}{{}}{{}}}
\bibcite{alrowili_biom-transformers_2021}{{190}{}{{}}{{}}}
\bibcite{alrowili_exploring_2022}{{191}{}{{}}{{}}}
\bibcite{ma_t-bertsum_2021}{{192}{}{{}}{{}}}
\bibcite{li_feature-aware_2023}{{193}{}{{}}{{}}}
\bibcite{kwon_class_2024}{{194}{}{{}}{{}}}
\bibcite{xie_pre-trained_2022}{{195}{}{{}}{{}}}
\bibcite{hajipoor_gptgan_2025}{{196}{}{{}}{{}}}
\bibcite{weiser1991computer}{{197}{}{{}}{{}}}
\bibcite{he2020developing}{{198}{}{{}}{{}}}
\bibcite{ige2022survey}{{199}{}{{}}{{}}}
\bibcite{singh2017stock}{{200}{}{{}}{{}}}
\bibcite{zhang2021hoba}{{201}{}{{}}{{}}}
\bibcite{lei2020deep}{{202}{}{{}}{{}}}
\bibcite{zhang2019comprehensive}{{203}{}{{}}{{}}}
\bibcite{dang2020sensor}{{204}{}{{}}{{}}}
\bibcite{gao2021danhar}{{205}{}{{}}{{}}}
\bibcite{gupta2021deep}{{206}{}{{}}{{}}}
\bibcite{erdacs2021human}{{207}{}{{}}{{}}}
\bibcite{ragab2020random}{{208}{}{{}}{{}}}
\bibcite{banjarey2022human}{{209}{}{{}}{{}}}
\bibcite{baraka_similarity_2023}{{210}{}{{}}{{}}}
\bibcite{baraka_deep_2024}{{211}{}{{}}{{}}}
\bibcite{shuvo2020hybrid}{{212}{}{{}}{{}}}
\bibcite{han2022human}{{213}{}{{}}{{}}}
\bibcite{ige2023wsense}{{214}{}{{}}{{}}}
\bibcite{deep_hybrid_2019}{{215}{}{{}}{{}}}
\bibcite{luwe_wearable_2022}{{216}{}{{}}{{}}}
\bibcite{shi_novel_2023}{{217}{}{{}}{{}}}
\bibcite{dua_inception_2023}{{218}{}{{}}{{}}}
\bibcite{imran_smart-wearable_2024}{{219}{}{{}}{{}}}
\bibcite{nafea_sensor-based_2021}{{220}{}{{}}{{}}}
\bibcite{khan_attention_2021}{{221}{}{{}}{{}}}
\bibcite{ige_deep_2023}{{222}{}{{}}{{}}}
\bibcite{gao_danhar_2021}{{223}{}{{}}{{}}}
\bibcite{agac_resource-efficient_2024}{{224}{}{{}}{{}}}
\bibcite{tang_triple_2022}{{225}{}{{}}{{}}}
\bibcite{misra_rotate_2020}{{226}{}{{}}{{}}}
\bibcite{chen_transformer_2022}{{227}{}{{}}{{}}}
\bibcite{sun_efficient_2024}{{228}{}{{}}{{}}}
\bibcite{lattanzi_are_2025}{{229}{}{{}}{{}}}
\bibcite{chan_unified_2021}{{230}{}{{}}{{}}}
\bibcite{jimale_fully_2022}{{231}{}{{}}{{}}}
\bibcite{lupion_data_2024}{{232}{}{{}}{{}}}
\bibcite{kia_human_2024}{{233}{}{{}}{{}}}
\bibcite{mohammadzadeh_cgan-based_2025}{{234}{}{{}}{{}}}
\bibcite{Nassif2019}{{235}{}{{}}{{}}}
\bibcite{Tirumala}{{236}{}{{}}{{}}}
\bibcite{app11083603}{{237}{}{{}}{{}}}
\bibcite{khalil2019}{{238}{}{{}}{{}}}
\bibcite{singh2021spoken}{{239}{}{{}}{{}}}
\bibcite{jiao2016accent}{{240}{}{{}}{{}}}
\bibcite{sanchez2022age}{{241}{}{{}}{{}}}
\bibcite{alnuaim2022speaker}{{242}{}{{}}{{}}}
\bibcite{srivastava2022speech}{{243}{}{{}}{{}}}
\bibcite{padmanabhan2015review}{{244}{}{{}}{{}}}
\bibcite{mukhamadiyev2022automatic}{{245}{}{{}}{{}}}
\bibcite{lu2020automatic}{{246}{}{{}}{{}}}
\bibcite{HEMA2023109492}{{247}{}{{}}{{}}}
\bibcite{shewalkar2019performance}{{248}{}{{}}{{}}}
\bibcite{prabhavalkar2017comparison}{{249}{}{{}}{{}}}
\bibcite{graves2012sequence}{{250}{}{{}}{{}}}
\bibcite{chan2015listen}{{251}{}{{}}{{}}}
\bibcite{jaitly2016online}{{252}{}{{}}{{}}}
\bibcite{raffel2017online}{{253}{}{{}}{{}}}
\bibcite{sak2017recurrent}{{254}{}{{}}{{}}}
\bibcite{szucbeamsearch2019}{{255}{}{{}}{{}}}
\bibcite{li2018seq2seq}{{256}{}{{}}{{}}}
\bibcite{chiugoogle}{{257}{}{{}}{{}}}
\bibcite{baevski_wav2vec_2020}{{258}{}{{}}{{}}}
\bibcite{radford_robust_2022}{{259}{}{{}}{{}}}
\bibcite{gulati_conformer_2020}{{260}{}{{}}{{}}}
\bibcite{stooke_aligner-encoders_2025}{{261}{}{{}}{{}}}
\bibcite{zhang_breaking_2025}{{262}{}{{}}{{}}}
\bibcite{wang_disentangled-transformer_2024}{{263}{}{{}}{{}}}
\bibcite{shakhadri_samba-asr_2025}{{264}{}{{}}{{}}}
\bibcite{ozbayoglu2020deep}{{265}{}{{}}{{}}}
\bibcite{shen2021new}{{266}{}{{}}{{}}}
\bibcite{wang2020portfolio}{{267}{}{{}}{{}}}
\bibcite{chen2024deep}{{268}{}{{}}{{}}}
\bibcite{ahnouch2023model}{{269}{}{{}}{{}}}
\bibcite{SUN2020101160}{{270}{}{{}}{{}}}
\bibcite{abedin2021deep}{{271}{}{{}}{{}}}
\bibcite{wang_sudf-rs_2023}{{272}{}{{}}{{}}}
\bibcite{nikou2019stock}{{273}{}{{}}{{}}}
\bibcite{cai2018financial}{{274}{}{{}}{{}}}
\bibcite{gudelek2017deep}{{275}{}{{}}{{}}}
\bibcite{eapen2019novel}{{276}{}{{}}{{}}}
\bibcite{chen_deep_2024}{{277}{}{{}}{{}}}
\bibcite{mozaffari_predictive_2024}{{278}{}{{}}{{}}}
\bibcite{li_transformer_2025}{{279}{}{{}}{{}}}
\bibcite{zhou_informer_2021}{{280}{}{{}}{{}}}
\bibcite{berti_tlob_2025}{{281}{}{{}}{{}}}
\bibcite{LIU2021107187}{{282}{}{{}}{{}}}
\bibcite{tsao2023heart}{{283}{}{{}}{{}}}
\bibcite{LLamedo2011}{{284}{}{{}}{{}}}
\bibcite{MATHEWS201853}{{285}{}{{}}{{}}}
\bibcite{zhu2019}{{286}{}{{}}{{}}}
\bibcite{desai2021low}{{287}{}{{}}{{}}}
\bibcite{crippa2015multi}{{288}{}{{}}{{}}}
\bibcite{acharya2017deep}{{289}{}{{}}{{}}}
\bibcite{baloglu2019classification}{{290}{}{{}}{{}}}
\bibcite{singh2018classification}{{291}{}{{}}{{}}}
\bibcite{prabhakararao2020attentive}{{292}{}{{}}{{}}}
\bibcite{wang2023single}{{293}{}{{}}{{}}}
\bibcite{sowmya2022contemplate}{{294}{}{{}}{{}}}
\bibcite{rai2022hybrid}{{295}{}{{}}{{}}}
\bibcite{Banerjee2020}{{296}{}{{}}{{}}}
\bibcite{kusuma_ecg_2022}{{297}{}{{}}{{}}}
\bibcite{alamatsaz_lightweight_2024}{{298}{}{{}}{{}}}
\bibcite{chen_automated_2022}{{299}{}{{}}{{}}}
\bibcite{wang_automated_2021}{{300}{}{{}}{{}}}
\bibcite{zhang_ecg-based_2020}{{301}{}{{}}{{}}}
\bibcite{sun_arrhythmia_2024}{{302}{}{{}}{{}}}
\bibcite{huang_ecg_2024}{{303}{}{{}}{{}}}
\bibcite{aghaomidi_ecg-sleepnet_2024}{{304}{}{{}}{{}}}
\bibcite{hasani_liquid_2021}{{305}{}{{}}{{}}}
\bibcite{zhu_electrocardiogram_2019}{{306}{}{{}}{{}}}
\bibcite{yang_data_2024}{{307}{}{{}}{{}}}
\bibcite{msigwa_iot-driven_2024}{{308}{}{{}}{{}}}
\bibcite{Schirrmeister2017}{{309}{}{{}}{{}}}
\bibcite{gao2021complex}{{310}{}{{}}{{}}}
\bibcite{Ang7802578}{{311}{}{{}}{{}}}
\bibcite{shen2022aberrated}{{312}{}{{}}{{}}}
\bibcite{boonyaki2020}{{313}{}{{}}{{}}}
\bibcite{sharma2021automated}{{314}{}{{}}{{}}}
\bibcite{vaquerizo2023explainable}{{315}{}{{}}{{}}}
\bibcite{modir2023systematic}{{316}{}{{}}{{}}}
\bibcite{altaheri2023deep}{{317}{}{{}}{{}}}
\bibcite{brunner2007spatial}{{318}{}{{}}{{}}}
\bibcite{delorme2007enhanced}{{319}{}{{}}{{}}}
\bibcite{jafarifarmand2019eeg}{{320}{}{{}}{{}}}
\bibcite{zhang2019novel}{{321}{}{{}}{{}}}
\bibcite{kumar2016deep}{{322}{}{{}}{{}}}
\bibcite{tibrewal2022}{{323}{}{{}}{{}}}
\bibcite{dai2019eeg}{{324}{}{{}}{{}}}
\bibcite{LiMingaiLSTM}{{325}{}{{}}{{}}}
\bibcite{Feng2020novel}{{326}{}{{}}{{}}}
\bibcite{hwang_improving_2023}{{327}{}{{}}{{}}}
\bibcite{zhao_deep_2015}{{328}{}{{}}{{}}}
\bibcite{xia_novel_2023}{{329}{}{{}}{{}}}
\bibcite{hermawan_multi_2024}{{330}{}{{}}{{}}}
\bibcite{abdulwahhab_detection_2024}{{331}{}{{}}{{}}}
\bibcite{pandey_subject_2022}{{332}{}{{}}{{}}}
\bibcite{hassouneh_development_2020}{{333}{}{{}}{{}}}
\bibcite{pan_multimodal_2024}{{334}{}{{}}{{}}}
\bibcite{wang_multimodal_2023}{{335}{}{{}}{{}}}
\bibcite{song_eeggan-net_2024}{{336}{}{{}}{{}}}
\bibcite{corley_deep_2025}{{337}{}{{}}{{}}}
\bibcite{cai_dhct-gan_2025}{{338}{}{{}}{{}}}
\bibcite{le_application_2025}{{339}{}{{}}{{}}}
\bibcite{vukicevic_versatile_2025}{{340}{}{{}}{{}}}
\bibcite{kirillov_segment_2023}{{341}{}{{}}{{}}}
\bibcite{liu_automatic_2025}{{342}{}{{}}{{}}}
\bibcite{ge_deep_2025}{{343}{}{{}}{{}}}
\bibcite{dai_advanced_2024}{{344}{}{{}}{{}}}
\bibcite{alotaibi_deep_2024}{{345}{}{{}}{{}}}
\bibcite{misir_drivable_2024}{{346}{}{{}}{{}}}
\bibcite{chen_rethinking_2017}{{347}{}{{}}{{}}}
\bibcite{cao_orchard_2024}{{348}{}{{}}{{}}}
\bibcite{liu_single-stage_2025}{{349}{}{{}}{{}}}
\bibcite{huang_survey_2025}{{350}{}{{}}{{}}}
\bibcite{asuzu_humanrobot_2025}{{351}{}{{}}{{}}}
\bibcite{chen_synergai_2024}{{352}{}{{}}{{}}}
\bibcite{allgeuer_when_2024}{{353}{}{{}}{{}}}
\bibcite{wang_i_2024}{{354}{}{{}}{{}}}
\bibcite{tonekaboni_what_2019}{{355}{}{{}}{{}}}
\bibcite{tjoa_enhancing_2023}{{356}{}{{}}{{}}}
\bibcite{li_hybrid_2024}{{357}{}{{}}{{}}}
\bibcite{termritthikun_explainable_2023}{{358}{}{{}}{{}}}
\bibcite{xiong_explainable_2022}{{359}{}{{}}{{}}}
\bibcite{fernandes_intrinsic_2023}{{360}{}{{}}{{}}}
\bibcite{freire_e-recruitment_2021}{{361}{}{{}}{{}}}
\bibcite{dass_detecting_2023}{{362}{}{{}}{{}}}
\bibcite{gicic2023intelligent}{{363}{}{{}}{{}}}
\bibcite{schaaf_towards_2021}{{364}{}{{}}{{}}}
\bibcite{giloni_benn_2022}{{365}{}{{}}{{}}}
\bibcite{iosifidis_fairness-enhancing_2019}{{366}{}{{}}{{}}}
\bibcite{kehrenberg_tuning_2020}{{367}{}{{}}{{}}}
\bibcite{jain_increasing_2023}{{368}{}{{}}{{}}}
\bibcite{yang_algorithmic_2023}{{369}{}{{}}{{}}}
\bibcite{raiaan_systematic_2024}{{370}{}{{}}{{}}}
\bibcite{pruning8794944}{{371}{}{{}}{{}}}
\bibcite{Yang_CVPR}{{372}{}{{}}{{}}}
\bibcite{NEURIPS2021_376c6b9f}{{373}{}{{}}{{}}}
\bibcite{ying_enhancing_2024}{{374}{}{{}}{{}}}
\bibcite{wang_optimizing_2024}{{375}{}{{}}{{}}}
\bibcite{yan_triple-inertial_2025}{{376}{}{{}}{{}}}
\bibcite{krasnoproshin_random_2024}{{377}{}{{}}{{}}}
\bibcite{adversarial2018}{{378}{}{{}}{{}}}
\bibcite{szegedy2013intriguing}{{379}{}{{}}{{}}}
\bibcite{tabacof2016adversarial}{{380}{}{{}}{{}}}
\bibcite{zhang2020adversarial}{{381}{}{{}}{{}}}
\bibcite{jiang2019black}{{382}{}{{}}{{}}}
\bibcite{esmaeilpour2019robust}{{383}{}{{}}{{}}}
\bibcite{munappy_data_2022}{{384}{}{{}}{{}}}
\bibcite{luca_impact_2022}{{385}{}{{}}{{}}}
\bibcite{zhuang_comprehensive_2020}{{386}{}{{}}{{}}}
\bibcite{mumuni_data_2022}{{387}{}{{}}{{}}}
\bibcite{hu_survey_2023}{{388}{}{{}}{{}}}
\bibcite{murtaza_synthetic_2023}{{389}{}{{}}{{}}}
\bibcite{li_survey_2024}{{390}{}{{}}{{}}}
\bibcite{cacciarelli_active_2024}{{391}{}{{}}{{}}}
\bibcite{tan_large_2024}{{392}{}{{}}{{}}}
\bibcite{gui_survey_2024}{{393}{}{{}}{{}}}
\bibcite{altabrawee_stclr_2025}{{394}{}{{}}{{}}}
\bibcite{altabrawee_repeat_2024}{{395}{}{{}}{{}}}
\bibcite{lee_predicting_2021}{{396}{}{{}}{{}}}
\bibcite{maldonado2023owadapt}{{397}{}{{}}{{}}}
\bibcite{zhu2024irda}{{398}{}{{}}{{}}}
\bibcite{belhaouari2024oversampling}{{399}{}{{}}{{}}}
\bibcite{wang2024class}{{400}{}{{}}{{}}}
\bibcite{ircio2023minimum}{{401}{}{{}}{{}}}
\bibcite{moles2024exploring}{{402}{}{{}}{{}}}
\bibcite{zhu2024sfpl}{{403}{}{{}}{{}}}
\bibcite{jimale_subject_2023}{{404}{}{{}}{{}}}
\bibcite{zhang_large_2023}{{405}{}{{}}{{}}}
\bibcite{butlin_consciousness_2023}{{406}{}{{}}{{}}}
\bibcite{marra_statistical_2024}{{407}{}{{}}{{}}}
\bibcite{colelough_neuro-symbolic_2025}{{408}{}{{}}{{}}}
\bibcite{bhuyan_neuro-symbolic_2024}{{409}{}{{}}{{}}}
\bibcite{wu_survey_2025}{{410}{}{{}}{{}}}
\bibcite{klusch_quantum_2024}{{411}{}{{}}{{}}}
\bibcite{shrestha_survey_2022}{{412}{}{{}}{{}}}
\bibcite{kudithipudi_neuromorphic_2025}{{413}{}{{}}{{}}}
\bibcite{fei_towards_2022}{{414}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{156}
