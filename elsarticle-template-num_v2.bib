@ARTICLE{shamshirband_review_2021,
  
  AUTHOR = {Shamshirband, Shahab and Fathi, Mahdis and Dehzangi, Abdollah and Chronopoulos, Anthony Theodore and Alinejad-Rokny, Hamid},
  URL = {https://www.sciencedirect.com/science/article/pii/S1532046420302550},
  DATE = {2021-01},
  DOI = {10.1016/j.jbi.2020.103627},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\DTP5MYW9\\S1532046420302550.html:text/html},
  ISSN = {1532-0464},
  JOURNALTITLE = {Journal of Biomedical Informatics},
  KEYWORDS = {Deep neural network,Diagnostics tools,Health data analytics,Healthcare applications,Machine learning},
  PAGES = {103627},
  SHORTTITLE = {A review on deep learning approaches in healthcare systems},
  TITLE = {A review on deep learning approaches in healthcare systems: {Taxonomies}, challenges, and open issues},
  URLDATE = {2023-11-02},
  VOLUME = {113},
}

@ARTICLE{pierson_deep_2017,
  
  AUTHOR = {Pierson, Harry A. and Gashler, Michael S.},
  URL = {https://doi.org/10.1080/01691864.2017.1365009},
  DATE = {2017-08},
  DOI = {10.1080/01691864.2017.1365009},
  ISSN = {0169-1864},
  JOURNALTITLE = {Advanced Robotics},
  KEYWORDS = {artificial intelligence,Deep neural networks,human–robot interaction},
  NOTE = {Publisher: Taylor \& Francis \_eprint: https://doi.org/10.1080/01691864.2017.1365009},
  NUMBER = {16},
  PAGES = {821--835},
  SHORTTITLE = {Deep learning in robotics},
  TITLE = {Deep learning in robotics: a review of recent research},
  URLDATE = {2023-11-02},
  VOLUME = {31},
}

@ARTICLE{dixit_deep_2021,
  
  AUTHOR = {Dixit, Priyanka and Silakari, Sanjay},
  URL = {https://www.sciencedirect.com/science/article/pii/S1574013720304172},
  DATE = {2021-02},
  DOI = {10.1016/j.cosrev.2020.100317},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\UQNT8NMA\\S1574013720304172.html:text/html},
  ISSN = {1574-0137},
  JOURNALTITLE = {Computer Science Review},
  KEYWORDS = {Attack,Cybersecurity,Deep learning,Supervised and unsupervised},
  PAGES = {100317},
  SHORTTITLE = {Deep {Learning} {Algorithms} for {Cybersecurity} {Applications}},
  TITLE = {Deep {Learning} {Algorithms} for {Cybersecurity} {Applications}: {A} {Technological} and {Status} {Review}},
  URLDATE = {2023-11-02},
  VOLUME = {39},
}

@ARTICLE{wang_deep_2018,
  
  AUTHOR = {Wang, Jinjiang and Ma, Yulin and Zhang, Laibin and Gao, Robert X. and Wu, Dazhong},
  URL = {https://www.sciencedirect.com/science/article/pii/S0278612518300037},
  DATE = {2018-07},
  DOI = {10.1016/j.jmsy.2018.01.003},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\ZG5XHNMY\\S0278612518300037.html:text/html},
  ISSN = {0278-6125},
  JOURNALTITLE = {Journal of Manufacturing Systems},
  KEYWORDS = {Deep learning,Computational intelligence,Data analytics,Smart manufacturing},
  PAGES = {144--156},
  SERIES = {Special {Issue} on {Smart} {Manufacturing}},
  SHORTTITLE = {Deep learning for smart manufacturing},
  TITLE = {Deep learning for smart manufacturing: {Methods} and applications},
  URLDATE = {2023-11-02},
  VOLUME = {48},
}

@ARTICLE{talaei_khoei_deep_2023,
  AUTHOR = {Talaei Khoei, Tala and Ould Slimane, Hadjar and Kaabouch, Naima},
  DATE = {2023},
  JOURNALTITLE = {Neural Computing and Applications},
  NOTE = {Publisher: Springer},
  NUMBER = {31},
  PAGES = {23103--23124},
  TITLE = {Deep learning: {Systematic} review, models, challenges, and research directions},
  VOLUME = {35},
}

@ARTICLE{dong_survey_2021,
  AUTHOR = {Dong, Shi and Wang, Ping and Abbas, Khushnood},
  DATE = {2021},
  JOURNALTITLE = {Computer Science Review},
  NOTE = {Publisher: Elsevier},
  PAGES = {100379},
  TITLE = {A survey on deep learning and its applications},
  VOLUME = {40},
}

@ARTICLE{alzubaidi_review_2021,
  AUTHOR = {Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J and Al-Dujaili, Ayad and Duan, Ye and Al-Shamma, Omran and Santamaria, Jose and Fadhel, Mohammed A and Al-Amidie, Muthana and Farhan, Laith},
  DATE = {2021},
  JOURNALTITLE = {Journal of big Data},
  NOTE = {Publisher: Springer},
  PAGES = {1--74},
  TITLE = {Review of deep learning: concepts, {CNN} architectures, challenges, applications, future directions},
  VOLUME = {8},
}

@ARTICLE{alom_state_art_2019,
  AUTHOR = {Alom, Md Zahangir and Taha, Tarek M and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Hasan, Mahmudul and Van Essen, Brian C and Awwal, Abdul AS and Asari, Vijayan K},
  DATE = {2019},
  JOURNALTITLE = {electronics},
  NOTE = {Publisher: Multidisciplinary Digital Publishing Institute},
  NUMBER = {3},
  PAGES = {292},
  TITLE = {A state-of-the-art survey on deep learning theory and architectures},
  VOLUME = {8},
}

@ARTICLE{pouyanfar_survey_2018,
  AUTHOR = {Pouyanfar, Samira and Sadiq, Saad and Yan, Yilin and Tian, Haiman and Tao, Yudong and Reyes, Maria Presa and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, Sundaraja S},
  DATE = {2018},
  JOURNALTITLE = {ACM Computing Surveys (CSUR)},
  NOTE = {Publisher: ACM New York, NY, USA},
  NUMBER = {5},
  PAGES = {1--36},
  TITLE = {A survey on deep learning: {Algorithms}, techniques, and applications},
  VOLUME = {51},
}

@ARTICLE{sarker_deep_2021,
  AUTHOR = {Sarker, Iqbal H},
  DATE = {2021},
  JOURNALTITLE = {SN Computer Science},
  NOTE = {Publisher: Springer},
  NUMBER = {6},
  PAGES = {420},
  TITLE = {Deep learning: a comprehensive overview on techniques, taxonomy, applications and research directions},
  VOLUME = {2},
}

@ARTICLE{apicella_survey_2021,
  
  AUTHOR = {Apicella, Andrea and Donnarumma, Francesco and Isgrò, Francesco and Prevete, Roberto},
  URL = {https://www.sciencedirect.com/science/article/pii/S0893608021000344},
  DATE = {2021-06},
  DOI = {10.1016/j.neunet.2021.01.026},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\MGKFDPVH\\S0893608021000344.html:text/html},
  ISSN = {0893-6080},
  JOURNALTITLE = {Neural Networks},
  KEYWORDS = {Machine learning,Activation functions,Learnable activation functions,Neural networks,Trainable activation functions},
  PAGES = {14--32},
  TITLE = {A survey on modern trainable activation functions},
  URLDATE = {2023-11-04},
  VOLUME = {138},
}

@INBOOK{lecun_efficient_2012,
  
  AUTHOR = {LeCun, Yann A. and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus-Robert},
  EDITOR = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  LOCATION = {Berlin, Heidelberg},
  PUBLISHER = {Springer Berlin Heidelberg},
  URL = {https://doi.org/10.1007/978-3-642-35289-8_3},
  BOOKTITLE = {Neural Networks: Tricks of the Trade: Second Edition},
  DATE = {2012},
  DOI = {10.1007/978-3-642-35289-8_3},
  ISBN = {978-3-642-35289-8},
  PAGES = {9--48},
  TITLE = {Efficient BackProp},
}

@INCOLLECTION{hochreiter_gradient_2001,
  AUTHOR = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, Jürgen and others},
  PUBLISHER = {A field guide to dynamical recurrent neural networks. IEEE Press In},
  BOOKTITLE = {A {Field} {Guide} to {Dynamical} {Recurrent} {Networks}},
  DATE = {2001},
  TITLE = {Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
}

@ARTICLE{sonoda_neural_2017,
  
  AUTHOR = {Sonoda, Sho and Murata, Noboru},
  URL = {https://www.sciencedirect.com/science/article/pii/S1063520315001748},
  DATE = {2017-09},
  DOI = {10.1016/j.acha.2015.12.005},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\FD679TQ9\\S1063520315001748.html:text/html},
  ISSN = {1063-5203},
  JOURNALTITLE = {Applied and Computational Harmonic Analysis},
  KEYWORDS = {Admissibility condition,Backprojection filter,Bounded extension to,Integral representation,Lizorkin distribution,Neural network,Radon transform,Rectified linear unit (ReLU),Ridgelet transform,Universal approximation},
  NUMBER = {2},
  PAGES = {233--268},
  TITLE = {Neural network with unbounded activation functions is universal approximator},
  URLDATE = {2023-11-04},
  VOLUME = {43},
}

@INPROCEEDINGS{dugas_incorporating_2000,
  
  AUTHOR = {Dugas, Charles and Bengio, Yoshua and Bélisle, François and Nadeau, Claude and Garcia, René},
  PUBLISHER = {MIT Press},
  URL = {https://papers.nips.cc/paper_files/paper/2000/hash/44968aece94f667e4095002d140b5896-Abstract.html},
  BOOKTITLE = {Advances in {Neural} {Information} {Processing} {Systems}},
  DATE = {2000},
  TITLE = {Incorporating {Second}-{Order} {Functional} {Knowledge} for {Better} {Option} {Pricing}},
  URLDATE = {2023-11-04},
  VOLUME = {13},
}

@INPROCEEDINGS{glorot_deep_2011,
  
  AUTHOR = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  LANGUAGE = {en},
  PUBLISHER = {JMLR Workshop and Conference Proceedings},
  URL = {https://proceedings.mlr.press/v15/glorot11a.html},
  BOOKTITLE = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
  DATE = {2011-06},
  NOTE = {ISSN: 1938-7228},
  PAGES = {315--323},
  TITLE = {Deep {Sparse} {Rectifier} {Neural} {Networks}},
  URLDATE = {2023-11-04},
}

@INPROCEEDINGS{maas_rectifier_2013,
  AUTHOR = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y and others},
  PUBLISHER = {Atlanta, GA},
  BOOKTITLE = {Proc. icml},
  DATE = {2013},
  NOTE = {Issue: 1},
  PAGES = {3},
  TITLE = {Rectifier nonlinearities improve neural network acoustic models},
  VOLUME = {30},
}

@MISC{misra_mish_2020,
  
  AUTHOR = {Misra, Diganta},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1908.08681},
  ANNOTATION = {Comment: Accepted to BMVC 2020},
  DATE = {2020-08},
  DOI = {10.48550/arXiv.1908.08681},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\LQRKAH7Z\\1908.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  NOTE = {arXiv:1908.08681 [cs, stat]},
  SHORTTITLE = {Mish},
  TITLE = {Mish: {A} {Self} {Regularized} {Non}-{Monotonic} {Activation} {Function}},
  URLDATE = {2023-11-04},
}

@MISC{clevert_fast_2016,
  
  AUTHOR = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1511.07289},
  ANNOTATION = {Comment: Published as a conference paper at ICLR 2016},
  DATE = {2016-02},
  DOI = {10.48550/arXiv.1511.07289},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\5R87GHZA\\1511.html:text/html},
  KEYWORDS = {Computer Science - Machine Learning},
  NOTE = {arXiv:1511.07289 [cs]},
  TITLE = {Fast and {Accurate} {Deep} {Network} {Learning} by {Exponential} {Linear} {Units} ({ELUs})},
  URLDATE = {2023-11-04},
}

@ARTICLE{lecun_deep_2015,
  
  AUTHOR = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  LANGUAGE = {en},
  URL = {https://www.nature.com/articles/nature14539},
  DATE = {2015-05},
  DOI = {10.1038/nature14539},
  ISSN = {1476-4687},
  JOURNALTITLE = {Nature},
  KEYWORDS = {Computer science,Mathematics and computing},
  NOTE = {Number: 7553 Publisher: Nature Publishing Group},
  NUMBER = {7553},
  PAGES = {436--444},
  TITLE = {Deep learning},
  URLDATE = {2023-11-05},
  VOLUME = {521},
}

@MISC{lin_network_2014,
  
  AUTHOR = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1312.4400},
  ANNOTATION = {Comment: 10 pages, 4 figures, for iclr2014},
  DATE = {2014-03},
  DOI = {10.48550/arXiv.1312.4400},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\VKXE4IUG\\1312.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  NOTE = {arXiv:1312.4400 [cs] version: 3},
  TITLE = {Network {In} {Network}},
  URLDATE = {2023-11-07},
}

@ARTICLE{qian_momentum_1999,
  
  AUTHOR = {Qian, Ning},
  URL = {https://www.sciencedirect.com/science/article/pii/S0893608098001166},
  DATE = {1999-01},
  DOI = {10.1016/S0893-6080(98)00116-6},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\EK4E96WV\\S0893608098001166.html:text/html},
  ISSN = {0893-6080},
  JOURNALTITLE = {Neural Networks},
  KEYWORDS = {Critical damping,Damped harmonic oscillator,Gradient descent learning algorithm,Learning rate,Momentum,Speed of convergence},
  NUMBER = {1},
  PAGES = {145--151},
  TITLE = {On the momentum term in gradient descent learning algorithms},
  URLDATE = {2023-11-08},
  VOLUME = {12},
}

@ARTICLE{duchi_adaptive_2011,
  
  AUTHOR = {Duchi, John and Hazan, Elad and Singer, Yoram},
  URL = {http://jmlr.org/papers/v12/duchi11a.html},
  DATE = {2011},
  ISSN = {1533-7928},
  JOURNALTITLE = {Journal of Machine Learning Research},
  NUMBER = {61},
  PAGES = {2121--2159},
  TITLE = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
  URLDATE = {2023-11-08},
  VOLUME = {12},
}

@MISC{kingma_adam_2017,
  
  AUTHOR = {Kingma, Diederik P. and Ba, Jimmy},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1412.6980},
  ANNOTATION = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
  DATE = {2017-01},
  DOI = {10.48550/arXiv.1412.6980},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\LRLDNZAE\\1412.html:text/html},
  KEYWORDS = {Computer Science - Machine Learning},
  NOTE = {arXiv:1412.6980 [cs]},
  SHORTTITLE = {Adam},
  TITLE = {Adam: {A} {Method} for {Stochastic} {Optimization}},
  URLDATE = {2023-11-08},
}

@ARTICLE{wang_comprehensive_2022,
  
  AUTHOR = {Wang, Qi and Ma, Yue and Zhao, Kun and Tian, Yingjie},
  LANGUAGE = {en},
  URL = {https://doi.org/10.1007/s40745-020-00253-5},
  DATE = {2022-04},
  DOI = {10.1007/s40745-020-00253-5},
  ISSN = {2198-5812},
  JOURNALTITLE = {Annals of Data Science},
  KEYWORDS = {Machine learning,Deep learning,Loss function,Survey},
  NUMBER = {2},
  PAGES = {187--212},
  TITLE = {A {Comprehensive} {Survey} of {Loss} {Functions} in {Machine} {Learning}},
  URLDATE = {2023-11-08},
  VOLUME = {9},
}

@INCOLLECTION{prechelt_early_2012,
  
  AUTHOR = {Prechelt, Lutz},
  EDITOR = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  LANGUAGE = {en},
  LOCATION = {Berlin, Heidelberg},
  PUBLISHER = {Springer},
  URL = {https://doi.org/10.1007/978-3-642-35289-8_5},
  BOOKTITLE = {Neural {Networks}: {Tricks} of the {Trade}: {Second} {Edition}},
  DATE = {2012},
  DOI = {10.1007/978-3-642-35289-8_5},
  ISBN = {978-3-642-35289-8},
  KEYWORDS = {Neural Information Processing System,Generalization Error,Training Algorithm,Training Time,Validation Error},
  PAGES = {53--67},
  SERIES = {Lecture {Notes} in {Computer} {Science}},
  TITLE = {Early {Stopping} — {But} {When}?},
  URLDATE = {2023-11-08},
}

@ARTICLE{cataltepe_no_1999,
  
  AUTHOR = {Cataltepe, Zehra and Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik},
  URL = {https://doi.org/10.1162/089976699300016557},
  DATE = {1999-05},
  DOI = {10.1162/089976699300016557},
  FILE = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\4RRZVJTB\\No-Free-Lunch-for-Early-Stopping.html:text/html},
  ISSN = {0899-7667},
  JOURNALTITLE = {Neural Computation},
  NUMBER = {4},
  PAGES = {995--1009},
  TITLE = {No {Free} {Lunch} for {Early} {Stopping}},
  URLDATE = {2023-11-08},
  VOLUME = {11},
}

@ARTICLE{srivastava_dropout_2014,
  
  AUTHOR = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  URL = {http://jmlr.org/papers/v15/srivastava14a.html},
  DATE = {2014},
  ISSN = {1533-7928},
  JOURNALTITLE = {Journal of Machine Learning Research},
  NUMBER = {56},
  PAGES = {1929--1958},
  SHORTTITLE = {Dropout},
  TITLE = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
  URLDATE = {2023-11-08},
  VOLUME = {15},
}

@INPROCEEDINGS{liu_dropout_2023,
  
  AUTHOR = {Liu, Zhuang and Xu, Zhiqiu and Jin, Joseph and Shen, Zhiqiang and Darrell, Trevor},
  LOCATION = {Honolulu, Hawaii, USA},
  PUBLISHER = {JMLR.org},
  BOOKTITLE = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
  DATE = {2023-07},
  PAGES = {22233--22248},
  SERIES = {{ICML}'23},
  TITLE = {Dropout reduces underfitting},
  URLDATE = {2023-11-07},
  VOLUME = {202},
}

@INPROCEEDINGS{park_analysis_2017,
  
  AUTHOR = {Park, Sungheon and Kwak, Nojun},
  EDITOR = {Lai, Shang-Hong and Lepetit, Vincent and Nishino, Ko and Sato, Yoichi},
  LANGUAGE = {en},
  LOCATION = {Cham},
  PUBLISHER = {Springer International Publishing},
  BOOKTITLE = {Computer {Vision} – {ACCV} 2016},
  DATE = {2017},
  DOI = {10.1007/978-3-319-54184-6_12},
  ISBN = {978-3-319-54184-6},
  KEYWORDS = {Classification Error,Convolutional Neural Network,Deep Neural Network,Regularization Method,Training Image},
  PAGES = {189--204},
  SERIES = {Lecture {Notes} in {Computer} {Science}},
  TITLE = {Analysis on the {Dropout} {Effect} in {Convolutional} {Neural} {Networks}},
}

@INPROCEEDINGS{krogh_simple_1991,
  
  AUTHOR = {Krogh, Anders and Hertz, John A.},
  LOCATION = {San Francisco, CA, USA},
  PUBLISHER = {Morgan Kaufmann Publishers Inc.},
  BOOKTITLE = {Proceedings of the 4th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
  DATE = {1991-12},
  ISBN = {978-1-55860-222-9},
  PAGES = {950--957},
  SERIES = {{NIPS}'91},
  TITLE = {A simple weight decay can improve generalization},
  URLDATE = {2023-11-09},
}

@ARTICLE{zou_regularization_2005,
  
  AUTHOR = {Zou, Hui and Hastie, Trevor},
  URL = {https://doi.org/10.1111/j.1467-9868.2005.00503.x},
  DATE = {2005-04},
  DOI = {10.1111/j.1467-9868.2005.00503.x},
  FILE = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\XAATZM63\\7109482.html:text/html},
  ISSN = {1369-7412},
  JOURNALTITLE = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  NUMBER = {2},
  PAGES = {301--320},
  TITLE = {Regularization and {Variable} {Selection} {Via} the {Elastic} {Net}},
  URLDATE = {2023-11-09},
  VOLUME = {67},
}

@ARTICLE{tibshirani_regression_1996,
  
  AUTHOR = {Tibshirani, Robert},
  URL = {https://www.jstor.org/stable/2346178},
  DATE = {1996},
  ISSN = {0035-9246},
  JOURNALTITLE = {Journal of the Royal Statistical Society. Series B (Methodological)},
  NOTE = {Publisher: [Royal Statistical Society, Wiley]},
  NUMBER = {1},
  PAGES = {267--288},
  TITLE = {Regression {Shrinkage} and {Selection} via the {Lasso}},
  URLDATE = {2023-11-09},
  VOLUME = {58},
}

@ARTICLE{nakamura_adaptive_2019,
  
  AUTHOR = {Nakamura, Kensuke and Hong, Byung-Woo},
  URL = {https://ieeexplore.ieee.org/document/8811458},
  DATE = {2019},
  DOI = {10.1109/ACCESS.2019.2937139},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\LP4EIFLM\\8811458.html:text/html},
  ISSN = {2169-3536},
  JOURNALTITLE = {IEEE Access},
  NOTE = {Conference Name: IEEE Access},
  PAGES = {118857--118865},
  TITLE = {Adaptive {Weight} {Decay} for {Deep} {Neural} {Networks}},
  URLDATE = {2023-11-09},
  VOLUME = {7},
}

@INPROCEEDINGS{ioffe_batch_2015,
  
  AUTHOR = {Ioffe, Sergey and Szegedy, Christian},
  LOCATION = {Lille, France},
  PUBLISHER = {JMLR.org},
  BOOKTITLE = {Proceedings of the 32nd {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 37},
  DATE = {2015-07},
  PAGES = {448--456},
  SERIES = {{ICML}'15},
  SHORTTITLE = {Batch normalization},
  TITLE = {Batch normalization: accelerating deep network training by reducing internal covariate shift},
  URLDATE = {2023-11-09},
}

@ARTICLE{ba_layer_2016,
  
  AUTHOR = {Ba, Jimmy and Kiros, J. and Hinton, Geoffrey E.},
  URL = {https://www.semanticscholar.org/paper/Layer-Normalization-Ba-Kiros/97fb4e3d45bb098e27e0071448b6152217bd35a5},
  DATE = {2016-07},
  JOURNALTITLE = {ArXiv},
  TITLE = {Layer {Normalization}},
  URLDATE = {2023-11-10},
}

@BOOK{goodfellow_deep_2016,
  AUTHOR = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  PUBLISHER = {MIT press},
  DATE = {2016},
  TITLE = {Deep learning},
}

@ARTICLE{hornik_multilayer_1989,
  
  AUTHOR = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  URL = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  DATE = {1989-01},
  DOI = {10.1016/0893-6080(89)90020-8},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\GN4C8ZXL\\0893608089900208.html:text/html},
  ISSN = {0893-6080},
  JOURNALTITLE = {Neural Networks},
  KEYWORDS = {Universal approximation,Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem},
  NUMBER = {5},
  PAGES = {359--366},
  TITLE = {Multilayer feedforward networks are universal approximators},
  URLDATE = {2023-11-10},
  VOLUME = {2},
}

@ARTICLE{cybenko_approximation_1989,
  
  AUTHOR = {Cybenko, G.},
  LANGUAGE = {en},
  URL = {https://doi.org/10.1007/BF02551274},
  DATE = {1989-12},
  DOI = {10.1007/BF02551274},
  ISSN = {1435-568X},
  JOURNALTITLE = {Mathematics of Control, Signals and Systems},
  KEYWORDS = {Neural networks,Approximation,Completeness},
  NUMBER = {4},
  PAGES = {303--314},
  TITLE = {Approximation by superpositions of a sigmoidal function},
  URLDATE = {2023-11-10},
  VOLUME = {2},
}

@ARTICLE{widrow_neural_1994,
  AUTHOR = {Widrow, Bernard and Rumelhart, David E. and Lehr, Michael A.},
  URL = {https://dl.acm.org/doi/10.1145/175247.175257},
  DATE = {1994-03},
  DOI = {10.1145/175247.175257},
  ISSN = {0001-0782},
  JOURNALTITLE = {Communications of the ACM},
  NUMBER = {3},
  PAGES = {93--105},
  SHORTTITLE = {Neural networks},
  TITLE = {Neural networks: applications in industry, business and science},
  URLDATE = {2023-11-10},
  VOLUME = {37},
}

@ARTICLE{hochreiter_long_1997,
  
  AUTHOR = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  URL = {https://doi.org/10.1162/neco.1997.9.8.1735},
  DATE = {1997-11},
  DOI = {10.1162/neco.1997.9.8.1735},
  FILE = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\S2GMPB3B\\Long-Short-Term-Memory.html:text/html},
  ISSN = {0899-7667},
  JOURNALTITLE = {Neural Computation},
  NUMBER = {8},
  PAGES = {1735--1780},
  TITLE = {Long {Short}-{Term} {Memory}},
  URLDATE = {2023-11-13},
  VOLUME = {9},
}

@INPROCEEDINGS{cho_learning_2014,
  AUTHOR = {Cho, Kyunghyun and van Merriënboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  EDITOR = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
  LOCATION = {Doha, Qatar},
  PUBLISHER = {Association for Computational Linguistics},
  URL = {https://aclanthology.org/D14-1179},
  BOOKTITLE = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
  DATE = {2014-10},
  DOI = {10.3115/v1/D14-1179},
  PAGES = {1724--1734},
  TITLE = {Learning {Phrase} {Representations} using {RNN} {Encoder}–{Decoder} for {Statistical} {Machine} {Translation}},
  URLDATE = {2023-11-13},
}

@MISC{noauthor_restricted_2018,
  
  LANGUAGE = {en-US},
  URL = {https://www.edureka.co/blog/restricted-boltzmann-machine-tutorial/},
  DATE = {2018-11},
  FILE = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\3DJJJZDM\\restricted-boltzmann-machine-tutorial.html:text/html},
  JOURNALTITLE = {Edureka},
  NOTE = {Section: Artificial Intelligence},
  TITLE = {Restricted {Boltzmann} {Machine} {Tutorial} {|} {Deep} {Learning} {Concepts}},
  URLDATE = {2023-11-13},
}

@MISC{ali_boltzmann_2019,
  
  AUTHOR = {Ali, Amir},
  LANGUAGE = {en},
  URL = {https://medium.com/machine-learning-researcher/boltzmann-machine-c2ce76d94da5},
  DATE = {2019-12},
  FILE = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\KQCGT9ML\\boltzmann-machine-c2ce76d94da5.html:text/html},
  JOURNALTITLE = {The Art of Data Scicne},
  TITLE = {Boltzmann {Machine}},
  URLDATE = {2023-11-13},
}

@MISC{noauthor_beginners_nodate,
  
  LANGUAGE = {en},
  URL = {http://wiki.pathmind.com/restricted-boltzmann-machine},
  FILE = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\WZMA5IN5\\restricted-boltzmann-machine.html:text/html},
  JOURNALTITLE = {Pathmind},
  TITLE = {A {Beginner}'s {Guide} to {Restricted} {Boltzmann} {Machines} ({RBMs})},
  URLDATE = {2023-11-14},
}

@INCOLLECTION{hinton_practical_2012,
  
  AUTHOR = {Hinton, Geoffrey E.},
  EDITOR = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  LANGUAGE = {en},
  LOCATION = {Berlin, Heidelberg},
  PUBLISHER = {Springer},
  URL = {https://doi.org/10.1007/978-3-642-35289-8_32},
  BOOKTITLE = {Neural {Networks}: {Tricks} of the {Trade}: {Second} {Edition}},
  DATE = {2012},
  DOI = {10.1007/978-3-642-35289-8_32},
  ISBN = {978-3-642-35289-8},
  KEYWORDS = {Hide Unit,Learning Rate,Reconstruction Error,Restrict Boltzmann Machine,Training Case},
  PAGES = {599--619},
  SERIES = {Lecture {Notes} in {Computer} {Science}},
  TITLE = {A {Practical} {Guide} to {Training} {Restricted} {Boltzmann} {Machines}},
  URLDATE = {2023-11-15},
}

@ARTICLE{hinton_fast_2006,
  
  AUTHOR = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  URL = {https://doi.org/10.1162/neco.2006.18.7.1527},
  DATE = {2006-07},
  DOI = {10.1162/neco.2006.18.7.1527},
  ISSN = {0899-7667},
  JOURNALTITLE = {Neural Computation},
  NUMBER = {7},
  PAGES = {1527--1554},
  TITLE = {A {Fast} {Learning} {Algorithm} for {Deep} {Belief} {Nets}},
  URLDATE = {2023-11-16},
  VOLUME = {18},
}

@ARTICLE{hinton_deep_2009,
  AUTHOR = {Hinton, Geoffrey E.},
  LANGUAGE = {en},
  URL = {http://www.scholarpedia.org/article/Deep_belief_networks},
  DATE = {2009-05},
  DOI = {10.4249/scholarpedia.5947},
  FILE = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\D95JU5SK\\Deep_belief_networks.html:text/html},
  ISSN = {1941-6016},
  JOURNALTITLE = {Scholarpedia},
  NUMBER = {5},
  PAGES = {5947},
  TITLE = {Deep belief networks},
  URLDATE = {2023-11-16},
  VOLUME = {4},
}

@MISC{shi_convolutional_2015,
  
  AUTHOR = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1506.04214},
  DATE = {2015-09},
  DOI = {10.48550/arXiv.1506.04214},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\EVRA46Q3\\1506.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition},
  NOTE = {arXiv:1506.04214 [cs] version: 2},
  SHORTTITLE = {Convolutional {LSTM} {Network}},
  TITLE = {Convolutional {LSTM} {Network}: {A} {Machine} {Learning} {Approach} for {Precipitation} {Nowcasting}},
  URLDATE = {2023-11-17},
}

@ARTICLE{li_comprehensive_2023,
  
  AUTHOR = {Li, Pengzhi and Pei, Yan and Li, Jianqiang},
  URL = {https://www.sciencedirect.com/science/article/pii/S1568494623001941},
  DATE = {2023-05},
  DOI = {10.1016/j.asoc.2023.110176},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\RFV4PQPS\\S1568494623001941.html:text/html},
  ISSN = {1568-4946},
  JOURNALTITLE = {Applied Soft Computing},
  KEYWORDS = {Deep learning,Autoencoder,Autoencoder application,Feature extraction,Unsupervised learning},
  PAGES = {110176},
  TITLE = {A comprehensive survey on design and application of autoencoder in deep learning},
  URLDATE = {2023-11-18},
  VOLUME = {138},
}

@ARTICLE{mohd_noor_feature_2021,
  
  AUTHOR = {Mohd Noor, Mohd Halim},
  LANGUAGE = {en},
  URL = {https://doi.org/10.1007/s00521-020-05638-4},
  DATE = {2021-09},
  DOI = {10.1007/s00521-020-05638-4},
  ISSN = {1433-3058},
  JOURNALTITLE = {Neural Computing and Applications},
  KEYWORDS = {Deep learning,Activity recognition,Denoising autoencoder,Feature learning},
  NUMBER = {17},
  PAGES = {10909--10922},
  TITLE = {Feature learning using convolutional denoising autoencoder for activity recognition},
  URLDATE = {2023-11-18},
  VOLUME = {33},
}

@ARTICLE{romero_quantum_2017,
  
  AUTHOR = {Romero, Jonathan and Olson, Jonathan P. and Aspuru-Guzik, Alan},
  LANGUAGE = {en},
  URL = {https://dx.doi.org/10.1088/2058-9565/aa8072},
  DATE = {2017-08},
  DOI = {10.1088/2058-9565/aa8072},
  ISSN = {2058-9565},
  JOURNALTITLE = {Quantum Science and Technology},
  NOTE = {Publisher: IOP Publishing},
  NUMBER = {4},
  PAGES = {045001},
  TITLE = {Quantum autoencoders for efficient compression of quantum data},
  URLDATE = {2023-11-18},
  VOLUME = {2},
}

@ARTICLE{li_guided_2020,
  
  AUTHOR = {Li, Xuan and Zhang, Tao and Zhao, Xin and Yi, Zhengming},
  LANGUAGE = {en},
  URL = {https://doi.org/10.1007/s10489-020-01813-1},
  DATE = {2020-12},
  DOI = {10.1007/s10489-020-01813-1},
  ISSN = {1573-7497},
  JOURNALTITLE = {Applied Intelligence},
  KEYWORDS = {Autoencoder structure,Dimensionality reduction,Feature retrieval,Principal component analysis},
  NUMBER = {12},
  PAGES = {4557--4567},
  TITLE = {Guided autoencoder for dimensionality reduction of pedestrian features},
  URLDATE = {2023-11-18},
  VOLUME = {50},
}

@ARTICLE{ng_sparse_2011,
  AUTHOR = {Ng, Andrew and others},
  DATE = {2011},
  JOURNALTITLE = {CS294A Lecture notes},
  NUMBER = {2011},
  PAGES = {1--19},
  TITLE = {Sparse autoencoder},
  VOLUME = {72},
}

@INPROCEEDINGS{rifai_higher_2011,
  
  AUTHOR = {Rifai, Salah and Mesnil, Grégoire and Vincent, Pascal and Muller, Xavier and Bengio, Yoshua and Dauphin, Yann and Glorot, Xavier},
  EDITOR = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  LANGUAGE = {en},
  LOCATION = {Berlin, Heidelberg},
  PUBLISHER = {Springer},
  BOOKTITLE = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
  DATE = {2011},
  DOI = {10.1007/978-3-642-23783-6_41},
  ISBN = {978-3-642-23783-6},
  KEYWORDS = {deep learning,manifold,Unsupervised feature learning},
  PAGES = {645--660},
  SERIES = {Lecture {Notes} in {Computer} {Science}},
  TITLE = {Higher {Order} {Contractive} {Auto}-{Encoder}},
}

@ARTICLE{ranjan_hyperface_2019,
  
  AUTHOR = {Ranjan, Rajeev and Patel, Vishal M. and Chellappa, Rama},
  URL = {https://ieeexplore.ieee.org/document/8170321?denied=},
  DATE = {2019-01},
  DOI = {10.1109/TPAMI.2017.2781233},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\3TDW3UET\\8170321.html:text/html},
  ISSN = {1939-3539},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NOTE = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NUMBER = {1},
  PAGES = {121--135},
  SHORTTITLE = {{HyperFace}},
  TITLE = {{HyperFace}: {A} {Deep} {Multi}-{Task} {Learning} {Framework} for {Face} {Detection}, {Landmark} {Localization}, {Pose} {Estimation}, and {Gender} {Recognition}},
  URLDATE = {2023-11-18},
  VOLUME = {41},
}

@MISC{noauthor_extracting_nodate,
  URL = {https://dl.acm.org/doi/10.1145/1390156.1390294},
  TITLE = {Extracting and composing robust features with denoising autoencoders {|} {Proceedings} of the 25th international conference on {Machine} learning},
  URLDATE = {2023-11-18},
}

@INPROCEEDINGS{chen_marginalized_2012,
  
  AUTHOR = {Chen, Minmin and Xu, Zhixiang and Weinberger, Kilian Q. and Sha, Fei},
  LOCATION = {Madison, WI, USA},
  PUBLISHER = {Omnipress},
  BOOKTITLE = {Proceedings of the 29th {International} {Coference} on {International} {Conference} on {Machine} {Learning}},
  DATE = {2012-06},
  ISBN = {978-1-4503-1285-1},
  PAGES = {1627--1634},
  SERIES = {{ICML}'12},
  TITLE = {Marginalized denoising autoencoders for domain adaptation},
  URLDATE = {2023-11-17},
}

@INPROCEEDINGS{vincent_extracting_2008,
  
  AUTHOR = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  LOCATION = {New York, NY, USA},
  PUBLISHER = {Association for Computing Machinery},
  URL = {https://doi.org/10.1145/1390156.1390294},
  BOOKTITLE = {Proceedings of the 25th international conference on {Machine} learning},
  DATE = {2008-07},
  DOI = {10.1145/1390156.1390294},
  ISBN = {978-1-60558-205-4},
  PAGES = {1096--1103},
  SERIES = {{ICML} '08},
  TITLE = {Extracting and composing robust features with denoising autoencoders},
  URLDATE = {2023-11-17},
}

@ARTICLE{kingma_auto-encoding_2013,
  AUTHOR = {Kingma, Diederik P and Welling, Max},
  DATE = {2013},
  JOURNALTITLE = {arXiv preprint arXiv:1312.6114},
  TITLE = {Auto-encoding variational bayes},
}

@ARTICLE{goodfellow_generative_2014,
  AUTHOR = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  DATE = {2014},
  JOURNALTITLE = {Advances in neural information processing systems},
  TITLE = {Generative adversarial nets},
  VOLUME = {27},
}

@INPROCEEDINGS{odena_conditional_2017,
  AUTHOR = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
  PUBLISHER = {PMLR},
  BOOKTITLE = {International conference on machine learning},
  DATE = {2017},
  PAGES = {2642--2651},
  TITLE = {Conditional image synthesis with auxiliary classifier gans},
}

@MISC{weng_gan_2019,
  
  AUTHOR = {Weng, Lilian},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1904.08994},
  ANNOTATION = {Comment: 12 pages, 9 figures},
  DATE = {2019-04},
  DOI = {10.48550/arXiv.1904.08994},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\YV4ABHBN\\1904.html:text/html},
  KEYWORDS = {Computer Science - Machine Learning,Statistics - Machine Learning},
  NOTE = {arXiv:1904.08994 [cs, stat]},
  TITLE = {From {GAN} to {WGAN}},
  URLDATE = {2023-11-20},
}

@MISC{chen_infogan_2016,
  
  AUTHOR = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1606.03657},
  DATE = {2016-06},
  DOI = {10.48550/arXiv.1606.03657},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\HBSHLDQC\\1606.html:text/html},
  KEYWORDS = {Computer Science - Machine Learning,Statistics - Machine Learning},
  NOTE = {arXiv:1606.03657 [cs, stat] version: 1},
  SHORTTITLE = {{InfoGAN}},
  TITLE = {{InfoGAN}: {Interpretable} {Representation} {Learning} by {Information} {Maximizing} {Generative} {Adversarial} {Nets}},
  URLDATE = {2023-11-20},
}

@MISC{karras_style-based_2019,
  
  AUTHOR = {Karras, Tero and Laine, Samuli and Aila, Timo},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1812.04948},
  ANNOTATION = {Comment: CVPR 2019 final version},
  DATE = {2019-03},
  DOI = {10.48550/arXiv.1812.04948},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\4YNBR2LE\\1812.html:text/html},
  KEYWORDS = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  NOTE = {arXiv:1812.04948 [cs, stat] version: 3},
  TITLE = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
  URLDATE = {2023-11-20},
}

@ARTICLE{iglesias_survey_2023,
  
  AUTHOR = {Iglesias, Guillermo and Talavera, Edgar and Díaz-Álvarez, Alberto},
  URL = {https://www.sciencedirect.com/science/article/pii/S1574013723000205},
  DATE = {2023-05},
  DOI = {10.1016/j.cosrev.2023.100553},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\9PJHIDYX\\S1574013723000205.html:text/html},
  ISSN = {1574-0137},
  JOURNALTITLE = {Computer Science Review},
  KEYWORDS = {Machine learning,Deep learning,Artificial intelligence,Generative Adversarial Network},
  PAGES = {100553},
  SHORTTITLE = {A survey on {GANs} for computer vision},
  TITLE = {A survey on {GANs} for computer vision: {Recent} research, analysis and taxonomy},
  URLDATE = {2023-11-20},
  VOLUME = {48},
}

@MISC{karras_progressive_2018,
  
  AUTHOR = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1710.10196},
  ANNOTATION = {Comment: Final ICLR 2018 version},
  DATE = {2018-02},
  DOI = {10.48550/arXiv.1710.10196},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\78RBH7K2\\1710.html:text/html},
  KEYWORDS = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  NOTE = {arXiv:1710.10196 [cs, stat] version: 3},
  TITLE = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
  URLDATE = {2023-11-20},
}

@MISC{gatys_neural_2015,
  
  AUTHOR = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1508.06576},
  DATE = {2015-09},
  DOI = {10.48550/arXiv.1508.06576},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\MRM5XWQT\\1508.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  NOTE = {arXiv:1508.06576 [cs, q-bio]},
  TITLE = {A {Neural} {Algorithm} of {Artistic} {Style}},
  URLDATE = {2023-11-20},
}

@INPROCEEDINGS{krizhevsky_imagenet_2012,
  
  AUTHOR = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  PUBLISHER = {Curran Associates, Inc.},
  URL = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  BOOKTITLE = {Advances in {Neural} {Information} {Processing} {Systems}},
  DATE = {2012},
  TITLE = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
  URLDATE = {2023-11-21},
  VOLUME = {25},
}

@MISC{simonyan_very_2015,
  
  AUTHOR = {Simonyan, Karen and Zisserman, Andrew},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1409.1556},
  DATE = {2015-04},
  DOI = {10.48550/arXiv.1409.1556},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\CY4BVGGH\\1409.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition},
  NOTE = {arXiv:1409.1556 [cs] version: 6},
  TITLE = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
  URLDATE = {2023-11-21},
}

@MISC{szegedy_going_2014,
  
  AUTHOR = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1409.4842},
  DATE = {2014-09},
  DOI = {10.48550/arXiv.1409.4842},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\ZSMBYVZC\\1409.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition},
  NOTE = {arXiv:1409.4842 [cs] version: 1},
  TITLE = {Going {Deeper} with {Convolutions}},
  URLDATE = {2023-11-21},
}

@MISC{he_deep_2015,
  
  AUTHOR = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1512.03385},
  ANNOTATION = {Comment: Tech report},
  DATE = {2015-12},
  DOI = {10.48550/arXiv.1512.03385},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\26T9WLY9\\1512.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition},
  NOTE = {arXiv:1512.03385 [cs]},
  TITLE = {Deep {Residual} {Learning} for {Image} {Recognition}},
  URLDATE = {2023-11-21},
}

@MISC{srivastava_highway_2015,
  
  AUTHOR = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1505.00387},
  ANNOTATION = {Comment: 6 pages, 2 figures. Presented at ICML 2015 Deep Learning workshop. Full paper is at arXiv:1507.06228},
  DATE = {2015-11},
  DOI = {10.48550/arXiv.1505.00387},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\RWLS2LZ7\\1505.html:text/html},
  KEYWORDS = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,68T01,G.1.6,I.2.6},
  NOTE = {arXiv:1505.00387 [cs]},
  TITLE = {Highway {Networks}},
  URLDATE = {2023-11-22},
}

@INPROCEEDINGS{huang_densely_2017,
  AUTHOR = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  BOOKTITLE = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  DATE = {2017},
  PAGES = {4700--4708},
  TITLE = {Densely connected convolutional networks},
}

@MISC{xie_aggregated_2017,
  
  AUTHOR = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1611.05431},
  ANNOTATION = {Comment: Accepted to CVPR 2017. Code and models: https://github.com/facebookresearch/ResNeXt},
  DATE = {2017-04},
  DOI = {10.48550/arXiv.1611.05431},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\9RNVSI2D\\1611.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition},
  NOTE = {arXiv:1611.05431 [cs]},
  TITLE = {Aggregated {Residual} {Transformations} for {Deep} {Neural} {Networks}},
  URLDATE = {2023-11-23},
}

@MISC{zagoruyko_wide_2017,
  
  AUTHOR = {Zagoruyko, Sergey and Komodakis, Nikos},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1605.07146},
  DATE = {2017-06},
  DOI = {10.48550/arXiv.1605.07146},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\V4CVM9KT\\1605.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  NOTE = {arXiv:1605.07146 [cs] version: 4},
  TITLE = {Wide {Residual} {Networks}},
  URLDATE = {2023-11-24},
}

@MISC{hu_squeeze-and-excitation_2019,
  
  AUTHOR = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1709.01507},
  ANNOTATION = {Comment: journal version of the CVPR 2018 paper, accepted by TPAMI},
  DATE = {2019-05},
  DOI = {10.48550/arXiv.1709.01507},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\58Z6YVF4\\1709.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition},
  NOTE = {arXiv:1709.01507 [cs] version: 4},
  TITLE = {Squeeze-and-{Excitation} {Networks}},
  URLDATE = {2023-11-26},
}

@MISC{gao_global_2018,
  
  AUTHOR = {Gao, Zilin and Xie, Jiangtao and Wang, Qilong and Li, Peihua},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1811.12006},
  DATE = {2018-11},
  DOI = {10.48550/arXiv.1811.12006},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\SVXM6FSC\\1811.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition},
  NOTE = {arXiv:1811.12006 [cs] version: 2},
  TITLE = {Global {Second}-order {Pooling} {Convolutional} {Networks}},
  URLDATE = {2023-11-26},
}

@MISC{wang_eca-net_2020,
  
  AUTHOR = {Wang, Qilong and Wu, Banggu and Zhu, Pengfei and Li, Peihua and Zuo, Wangmeng and Hu, Qinghua},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1910.03151},
  ANNOTATION = {Comment: Accepted to CVPR 2020; Project Page: https://github.com/BangguWu/ECANet},
  DATE = {2020-04},
  DOI = {10.48550/arXiv.1910.03151},
  FILE = {arXiv Fulltext PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\A9B3969E\\Wang et al. - 2020 - ECA-Net Efficient Channel Attention for Deep Conv.pdf:application/pdf;arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\WSWPVCNP\\1910.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition},
  NOTE = {arXiv:1910.03151 [cs]},
  SHORTTITLE = {{ECA}-{Net}},
  TITLE = {{ECA}-{Net}: {Efficient} {Channel} {Attention} for {Deep} {Convolutional} {Neural} {Networks}},
  URLDATE = {2023-11-26},
}

@MISC{oktay_attention_2018,
  
  AUTHOR = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1804.03999},
  ANNOTATION = {Comment: Accepted to published in MIDL'18 (Revised Version) / OpenReview link: https://openreview.net/forum?id=Skft7cijM},
  DATE = {2018-05},
  DOI = {10.48550/arXiv.1804.03999},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\IIZP2K43\\1804.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition},
  NOTE = {arXiv:1804.03999 [cs]},
  SHORTTITLE = {Attention {U}-{Net}},
  TITLE = {Attention {U}-{Net}: {Learning} {Where} to {Look} for the {Pancreas}},
  URLDATE = {2023-11-28},
}

@MISC{liu_tam_2021,
  
  AUTHOR = {Liu, Zhaoyang and Wang, Limin and Wu, Wayne and Qian, Chen and Lu, Tong},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/2005.06803},
  ANNOTATION = {Comment: ICCV 2021 camera-ready version. Code is available at https://github.com/liu-zhy/temporal-adaptive-module},
  DATE = {2021-08},
  DOI = {10.48550/arXiv.2005.06803},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\MMX4GPBL\\2005.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition},
  NOTE = {arXiv:2005.06803 [cs]},
  SHORTTITLE = {{TAM}},
  TITLE = {{TAM}: {Temporal} {Adaptive} {Module} for {Video} {Recognition}},
  URLDATE = {2023-12-04},
}

@MISC{vaswani_attention_2023,
  
  AUTHOR = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1706.03762},
  ANNOTATION = {Comment: 15 pages, 5 figures},
  DATE = {2023-08},
  DOI = {10.48550/arXiv.1706.03762},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\I3B97WAB\\1706.html:text/html},
  KEYWORDS = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  NOTE = {arXiv:1706.03762 [cs]},
  TITLE = {Attention {Is} {All} {You} {Need}},
  URLDATE = {2023-12-04},
}

@MISC{dosovitskiy_image_2021,
  
  AUTHOR = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/2010.11929},
  ANNOTATION = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
  DATE = {2021-06},
  DOI = {10.48550/arXiv.2010.11929},
  FILE = {arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\8YDFJMQG\\2010.html:text/html},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  NOTE = {arXiv:2010.11929 [cs]},
  SHORTTITLE = {An {Image} is {Worth} 16x16 {Words}},
  TITLE = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
  URLDATE = {2023-12-04},
}

@ARTICLE{guo_beyond_2023,
  
  AUTHOR = {Guo, Meng-Hao and Liu, Zheng-Ning and Mu, Tai-Jiang and Hu, Shi-Min},
  URL = {https://ieeexplore.ieee.org/document/9912362},
  DATE = {2023-05},
  DOI = {10.1109/TPAMI.2022.3211006},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\WHC3AL69\\9912362.html:text/html},
  ISSN = {1939-3539},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NOTE = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NUMBER = {5},
  PAGES = {5436--5447},
  SHORTTITLE = {Beyond {Self}-{Attention}},
  TITLE = {Beyond {Self}-{Attention}: {External} {Attention} {Using} {Two} {Linear} {Layers} for {Visual} {Tasks}},
  URLDATE = {2023-12-04},
  VOLUME = {45},
}

@MISC{zeiler_visualizing_2013,
  
  AUTHOR = {Zeiler, Matthew D. and Fergus, Rob},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1311.2901},
  DATE = {2013-11},
  DOI = {10.48550/arXiv.1311.2901},
  FILE = {arXiv Fulltext PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\6JITKKYG\\Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf;arXiv.org Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\M6MVEUNH\\1311.html:text/html},
  KEYWORDS = {Computer Science - Computer Vision and Pattern Recognition},
  NOTE = {arXiv:1311.2901 [cs] version: 3},
  TITLE = {Visualizing and {Understanding} {Convolutional} {Networks}},
  URLDATE = {2023-12-14},
}

@ARTICLE{lecun_gradient-based_1998,
  
  AUTHOR = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  URL = {https://ieeexplore.ieee.org/document/726791},
  DATE = {1998-11},
  DOI = {10.1109/5.726791},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\6XTKPW5X\\726791.html:text/html},
  ISSN = {1558-2256},
  JOURNALTITLE = {Proceedings of the IEEE},
  NOTE = {Conference Name: Proceedings of the IEEE},
  NUMBER = {11},
  PAGES = {2278--2324},
  TITLE = {Gradient-based learning applied to document recognition},
  URLDATE = {2023-12-14},
  VOLUME = {86},
}

@INPROCEEDINGS{simard_best_2003,
  AUTHOR = {Simard, P.Y. and Steinkraus, D. and Platt, J.C.},
  URL = {https://ieeexplore.ieee.org/document/1227801},
  BOOKTITLE = {Seventh {International} {Conference} on {Document} {Analysis} and {Recognition}, 2003. {Proceedings}.},
  DATE = {2003-08},
  DOI = {10.1109/ICDAR.2003.1227801},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\29S8FTQA\\1227801.html:text/html},
  PAGES = {958--963},
  TITLE = {Best practices for convolutional neural networks applied to visual document analysis},
  URLDATE = {2023-12-14},
}

@ARTICLE{matsugu_subject_2003,
  
  AUTHOR = {Matsugu, M. and Mori, K. and Mitari, Y. and Kaneda, Y.},
  LANGUAGE = {English},
  ANNOTATION = {Cited By :486},
  DATE = {2003},
  DOI = {10.1016/S0893-6080(03)00115-1},
  FILE = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\5BT24LFI\\display.html:text/html},
  ISSN = {0893-6080},
  JOURNALTITLE = {Neural Networks},
  KEYWORDS = {Convolutional neural network,Face detection,Facial expression},
  NUMBER = {5-6},
  PAGES = {555--559},
  TITLE = {Subject independent facial expression recognition with robust face detection using a convolutional neural network},
  VOLUME = {16},
}

@INPROCEEDINGS{zhao_well-classified_2022,
  AUTHOR = {Zhao, Guangxiang and Yang, Wenkai and Ren, Xuancheng and Li, Lei and Wu, Yunfang and Sun, Xu},
  BOOKTITLE = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
  DATE = {2022},
  NOTE = {Issue: 8},
  PAGES = {9180--9189},
  TITLE = {Well-classified examples are underestimated in classification with deep neural networks},
  VOLUME = {36},
}

@INPROCEEDINGS{huang_asymmetric_2023,
  AUTHOR = {Huang, Yusheng and Qi, Jiexing and Wang, Xinbing and Lin, Zhouhan},
  PUBLISHER = {IEEE},
  BOOKTITLE = {{ICASSP} 2023-2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  DATE = {2023},
  PAGES = {1--5},
  TITLE = {Asymmetric {Polynomial} {Loss} for {Multi}-{Label} {Classification}},
}

@INPROCEEDINGS{park_robust_2023,
  AUTHOR = {Park, Wongi and Park, Inhyuk and Kim, Sungeun and Ryu, Jongbin},
  LANGUAGE = {en},
  URL = {https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Park_Robust_Asymmetric_Loss_for_Multi-Label_Long-Tailed_Learning_ICCVW_2023_paper.html},
  DATE = {2023},
  FILE = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\RSCAP7RT\\Park et al. - 2023 - Robust Asymmetric Loss for Multi-Label Long-Tailed.pdf:application/pdf},
  PAGES = {2711--2720},
  TITLE = {Robust {Asymmetric} {Loss} for {Multi}-{Label} {Long}-{Tailed} {Learning}},
  URLDATE = {2023-12-23},
}

@ARTICLE{nanni_building_2023,
  
  AUTHOR = {Nanni, Loris and Loreggia, Andrea and Barcellona, Leonardo and Ghidoni, Stefano},
  URL = {https://ieeexplore.ieee.org/document/10309107},
  DATE = {2023},
  DOI = {10.1109/ACCESS.2023.3330442},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\XFYTZBM7\\10309107.html:text/html},
  ISSN = {2169-3536},
  JOURNALTITLE = {IEEE Access},
  NOTE = {Conference Name: IEEE Access},
  PAGES = {124962--124974},
  SHORTTITLE = {Building {Ensemble} of {Deep} {Networks}},
  TITLE = {Building {Ensemble} of {Deep} {Networks}: {Convolutional} {Networks} and {Transformers}},
  URLDATE = {2023-12-24},
  VOLUME = {11},
}

@INPROCEEDINGS{wortsman_model_2022,
  AUTHOR = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  PUBLISHER = {PMLR},
  BOOKTITLE = {International {Conference} on {Machine} {Learning}},
  DATE = {2022},
  PAGES = {23965--23998},
  TITLE = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
}

@ARTICLE{loh_multi-symmetry_2023,
  AUTHOR = {Loh, Charlotte and Han, Seungwook and Sudalairaj, Shivchander and Dangovski, Rumen and Xu, Kai and Wenzel, Florian and Soljacic, Marin and Srivastava, Akash},
  DATE = {2023},
  JOURNALTITLE = {arXiv preprint arXiv:2303.02484},
  TITLE = {Multi-{Symmetry} {Ensembles}: {Improving} {Diversity} and {Generalization} via {Opposing} {Symmetries}},
}

@INPROCEEDINGS{wu_cvt_2021,
  
  AUTHOR = {Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  URL = {https://ieeexplore.ieee.org/document/9710031?denied=},
  BOOKTITLE = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
  DATE = {2021-10},
  DOI = {10.1109/ICCV48922.2021.00009},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\G2TQMEA6\\9710031.html:text/html;Submitted Version:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\LX9ZIMZM\\Wu et al. - 2021 - CvT Introducing Convolutions to Vision Transforme.pdf:application/pdf},
  NOTE = {ISSN: 2380-7504},
  PAGES = {22--31},
  SHORTTITLE = {{CvT}},
  TITLE = {{CvT}: {Introducing} {Convolutions} to {Vision} {Transformers}},
  URLDATE = {2023-12-24},
}

@ARTICLE{xiao_early_2021,
  AUTHOR = {Xiao, Tete and Singh, Mannat and Mintun, Eric and Darrell, Trevor and Dollár, Piotr and Girshick, Ross},
  DATE = {2021},
  JOURNALTITLE = {Advances in neural information processing systems},
  PAGES = {30392--30400},
  TITLE = {Early convolutions help transformers see better},
  VOLUME = {34},
}

@ARTICLE{ma_convolutional_2024,
  
  AUTHOR = {Ma, Yujun and Wang, Ruili and Zong, Ming and Ji, Wanting and Wang, Yi and Ye, Baoliu},
  URL = {https://www.sciencedirect.com/science/article/pii/S0925231223011505},
  DATE = {2024-02},
  DOI = {10.1016/j.neucom.2023.127027},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\3WYFH6FD\\S0925231223011505.html:text/html},
  ISSN = {0925-2312},
  JOURNALTITLE = {Neurocomputing},
  KEYWORDS = {3D convolutions,Fine-grained action recognition,Spatial-temporal features,Transformer},
  PAGES = {127027},
  TITLE = {Convolutional transformer network for fine-grained action recognition},
  URLDATE = {2023-12-24},
  VOLUME = {569},
}

@INPROCEEDINGS{tu_maxvit_2022,
  
  AUTHOR = {Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},
  EDITOR = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  LANGUAGE = {en},
  LOCATION = {Cham},
  PUBLISHER = {Springer Nature Switzerland},
  BOOKTITLE = {Computer {Vision} – {ECCV} 2022},
  DATE = {2022},
  DOI = {10.1007/978-3-031-20053-3_27},
  FILE = {Submitted Version:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\DPPP6Z96\\Tu et al. - 2022 - MaxViT Multi-axis Vision Transformer.pdf:application/pdf},
  ISBN = {978-3-031-20053-3},
  KEYWORDS = {Transformer,Image classification,Multi-axis attention},
  PAGES = {459--479},
  SERIES = {Lecture {Notes} in {Computer} {Science}},
  SHORTTITLE = {{MaxViT}},
  TITLE = {{MaxViT}: {Multi}-axis {Vision} {Transformer}},
}

@ARTICLE{peng_conformer_2023,
  
  AUTHOR = {Peng, Zhiliang and Guo, Zonghao and Huang, Wei and Wang, Yaowei and Xie, Lingxi and Jiao, Jianbin and Tian, Qi and Ye, Qixiang},
  URL = {https://ieeexplore.ieee.org/document/10040235},
  DATE = {2023-08},
  DOI = {10.1109/TPAMI.2023.3243048},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\RGB5PFI2\\10040235.html:text/html},
  ISSN = {1939-3539},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NOTE = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NUMBER = {8},
  PAGES = {9454--9468},
  SHORTTITLE = {Conformer},
  TITLE = {Conformer: {Local} {Features} {Coupling} {Global} {Representations} for {Recognition} and {Detection}},
  URLDATE = {2023-12-24},
  VOLUME = {45},
}

@INPROCEEDINGS{girshick_rich_2014,
  AUTHOR = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  URL = {https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html},
  DATE = {2014},
  PAGES = {580--587},
  TITLE = {Rich {Feature} {Hierarchies} for {Accurate} {Object} {Detection} and {Semantic} {Segmentation}},
  URLDATE = {2023-12-27},
}

@INPROCEEDINGS{girshick_fast_2015,
  
  AUTHOR = {Girshick, Ross},
  URL = {https://ieeexplore.ieee.org/document/7410526},
  BOOKTITLE = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
  DATE = {2015-12},
  DOI = {10.1109/ICCV.2015.169},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\2IUNTIEZ\\7410526.html:text/html},
  NOTE = {ISSN: 2380-7504},
  PAGES = {1440--1448},
  TITLE = {Fast {R}-{CNN}},
  URLDATE = {2023-12-27},
}

@INPROCEEDINGS{ren_faster_2015,
  
  AUTHOR = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  PUBLISHER = {Curran Associates, Inc.},
  URL = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html},
  BOOKTITLE = {Advances in {Neural} {Information} {Processing} {Systems}},
  DATE = {2015},
  SHORTTITLE = {Faster {R}-{CNN}},
  TITLE = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
  URLDATE = {2023-12-27},
  VOLUME = {28},
}

@ARTICLE{lin_focal_2020,
  
  AUTHOR = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  URL = {https://ieeexplore.ieee.org/document/8417976},
  DATE = {2020-02},
  DOI = {10.1109/TPAMI.2018.2858826},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\32QIC22G\\8417976.html:text/html},
  ISSN = {1939-3539},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NOTE = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NUMBER = {2},
  PAGES = {318--327},
  TITLE = {Focal {Loss} for {Dense} {Object} {Detection}},
  URLDATE = {2023-12-27},
  VOLUME = {42},
}

@INPROCEEDINGS{redmon_you_2016,
  AUTHOR = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  URL = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html},
  DATE = {2016},
  PAGES = {779--788},
  SHORTTITLE = {You {Only} {Look} {Once}},
  TITLE = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
  URLDATE = {2023-12-27},
}

@INPROCEEDINGS{liu_ssd_2016,
  
  AUTHOR = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  EDITOR = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  LANGUAGE = {en},
  LOCATION = {Cham},
  PUBLISHER = {Springer International Publishing},
  BOOKTITLE = {Computer {Vision} – {ECCV} 2016},
  DATE = {2016},
  DOI = {10.1007/978-3-319-46448-0_2},
  ISBN = {978-3-319-46448-0},
  KEYWORDS = {Convolutional neural network,Real-time object detection},
  PAGES = {21--37},
  SERIES = {Lecture {Notes} in {Computer} {Science}},
  SHORTTITLE = {{SSD}},
  TITLE = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
}

@ARTICLE{bochkovskiy_yolov4_2020,
  AUTHOR = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  DATE = {2020},
  JOURNALTITLE = {arXiv preprint arXiv:2004.10934},
  TITLE = {Yolov4: {Optimal} speed and accuracy of object detection},
}

@ARTICLE{redmon_yolov3_2018,
  AUTHOR = {Redmon, Joseph and Farhadi, Ali},
  DATE = {2018},
  JOURNALTITLE = {arXiv preprint arXiv:1804.02767},
  TITLE = {Yolov3: {An} incremental improvement},
}

@INPROCEEDINGS{lin_feature_2017,
  AUTHOR = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  BOOKTITLE = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  DATE = {2017},
  PAGES = {2117--2125},
  TITLE = {Feature pyramid networks for object detection},
}

@INPROCEEDINGS{tan_efficientdet_2020,
  AUTHOR = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V},
  BOOKTITLE = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
  DATE = {2020},
  PAGES = {10781--10790},
  TITLE = {Efficientdet: {Scalable} and efficient object detection},
}

@INPROCEEDINGS{tan_efficientnet_2019,
  AUTHOR = {Tan, Mingxing and Le, Quoc},
  PUBLISHER = {PMLR},
  BOOKTITLE = {International conference on machine learning},
  DATE = {2019},
  PAGES = {6105--6114},
  TITLE = {{EfficientNet}: {Rethinking} model scaling for convolutional neural networks},
}

@INPROCEEDINGS{liu_adaptive_2019,
  AUTHOR = {Liu, Songtao and Huang, Di and Wang, Yunhong},
  BOOKTITLE = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
  DATE = {2019},
  PAGES = {6459--6468},
  TITLE = {Adaptive {NMS}: {Refining} pedestrian detection in a crowd},
}

@INPROCEEDINGS{bodla_soft-nmsimproving_2017,
  AUTHOR = {Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S},
  BOOKTITLE = {Proceedings of the {IEEE} international conference on computer vision},
  DATE = {2017},
  PAGES = {5561--5569},
  TITLE = {Soft-{NMS}–improving object detection with one line of code},
}

@INPROCEEDINGS{carion_end_end_2020,
  
  AUTHOR = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  EDITOR = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  LANGUAGE = {en},
  LOCATION = {Cham},
  PUBLISHER = {Springer International Publishing},
  BOOKTITLE = {Computer {Vision} – {ECCV} 2020},
  DATE = {2020},
  DOI = {10.1007/978-3-030-58452-8\_13},
  ISBN = {978-3-030-58452-8},
  PAGES = {213--229},
  SERIES = {Lecture {Notes} in {Computer} {Science}},
  TITLE = {End-to-{End} {Object} {Detection} with {Transformers}},
}

@ARTICLE{zhu_deformable_2020,
  AUTHOR = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  DATE = {2020},
  JOURNALTITLE = {arXiv preprint arXiv:2010.04159},
  TITLE = {Deformable {DETR}: {Deformable} transformers for end-to-end object detection},
}

@INPROCEEDINGS{dai_dynamic_2021,
  AUTHOR = {Dai, Xiyang and Chen, Yinpeng and Yang, Jianwei and Zhang, Pengchuan and Yuan, Lu and Zhang, Lei},
  LANGUAGE = {en},
  URL = {https://openaccess.thecvf.com/content/ICCV2021/html/Dai\_Dynamic\_DETR\_End-to-End\_Object\_Detection\_With\_Dynamic\_Attention\_ICCV\_2021\_paper.html},
  DATE = {2021},
  FILE = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\UMV5QRK3\\Dai et al. - 2021 - Dynamic DETR End-to-End Object Detection With Dyn.pdf:application/pdf},
  PAGES = {2988--2997},
  SHORTTITLE = {Dynamic {DETR}},
  TITLE = {Dynamic {DETR}: {End}-to-{End} {Object} {Detection} {With} {Dynamic} {Attention}},
  URLDATE = {2024-01-08},
}

@ARTICLE{huang_teach-detr_2023,
  
  AUTHOR = {Huang, Linjiang and Lu, Kaixin and Song, Guanglu and Wang, Liang and Liu, Si and Liu, Yu and Li, Hongsheng},
  URL = {https://ieeexplore.ieee.org/document/10264211},
  DATE = {2023-12},
  DOI = {10.1109/TPAMI.2023.3319387},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\9QQM7AJK\\10264211.html:text/html;Submitted Version:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\67B2PTE5\\Huang et al. - 2023 - Teach-DETR Better Training DETR With Teachers.pdf:application/pdf},
  ISSN = {1939-3539},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NOTE = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NUMBER = {12},
  PAGES = {15759--15771},
  SHORTTITLE = {Teach-{DETR}},
  TITLE = {Teach-{DETR}: {Better} {Training} {DETR} {With} {Teachers}},
  URLDATE = {2024-01-11},
  VOLUME = {45},
}

@INPROCEEDINGS{long_fully_2015,
  AUTHOR = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  BOOKTITLE = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  DATE = {2015},
  PAGES = {3431--3440},
  TITLE = {Fully convolutional networks for semantic segmentation},
}

@INPROCEEDINGS{noh_learning_2015,
  AUTHOR = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
  BOOKTITLE = {Proceedings of the {IEEE} international conference on computer vision},
  DATE = {2015},
  PAGES = {1520--1528},
  TITLE = {Learning deconvolution network for semantic segmentation},
}

@ARTICLE{badrinarayanan_segnet_2017,
  AUTHOR = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  DATE = {2017},
  JOURNALTITLE = {IEEE transactions on pattern analysis and machine intelligence},
  NOTE = {Publisher: IEEE},
  NUMBER = {12},
  PAGES = {2481--2495},
  TITLE = {Segnet: {A} deep convolutional encoder-decoder architecture for image segmentation},
  VOLUME = {39},
}

@INPROCEEDINGS{chaurasia_linknet_2017,
  AUTHOR = {Chaurasia, Abhishek and Culurciello, Eugenio},
  PUBLISHER = {IEEE},
  BOOKTITLE = {2017 {IEEE} visual communications and image processing ({VCIP})},
  DATE = {2017},
  PAGES = {1--4},
  TITLE = {Linknet: {Exploiting} encoder representations for efficient semantic segmentation},
}

@INPROCEEDINGS{he_mask_2017,
  AUTHOR = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  BOOKTITLE = {Proceedings of the {IEEE} international conference on computer vision},
  DATE = {2017},
  PAGES = {2961--2969},
  TITLE = {Mask {R}-{CNN}},
}

@INPROCEEDINGS{liu_path_2018,
  AUTHOR = {Liu, Shu and Qi, Lu and Qin, Haifang and Shi, Jianping and Jia, Jiaya},
  BOOKTITLE = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  DATE = {2018},
  PAGES = {8759--8768},
  TITLE = {Path aggregation network for instance segmentation},
}

@INPROCEEDINGS{chen_masklab_2018,
  AUTHOR = {Chen, Liang-Chieh and Hermans, Alexander and Papandreou, George and Schroff, Florian and Wang, Peng and Adam, Hartwig},
  BOOKTITLE = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  DATE = {2018},
  PAGES = {4013--4022},
  TITLE = {{MaskLab}: {Instance} segmentation by refining object detection with semantic and direction features},
}

@ARTICLE{liu_multi-stage_2023,
  
  AUTHOR = {Liu, Qing and Dong, Yongsheng and Li, Xuelong},
  URL = {https://www.sciencedirect.com/science/article/pii/S0925231223002254},
  DATE = {2023-05},
  DOI = {10.1016/j.neucom.2023.03.006},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\A2Z7DGU5\\S0925231223002254.html:text/html},
  ISSN = {0925-2312},
  JOURNALTITLE = {Neurocomputing},
  KEYWORDS = {Semantic segmentation,Attention mechanism,Context information,Convolutional neural network (CNN),Spatial information},
  PAGES = {53--63},
  TITLE = {Multi-stage context refinement network for semantic segmentation},
  URLDATE = {2024-01-15},
  VOLUME = {535},
}

@ARTICLE{liu_covariance_2022,
  
  AUTHOR = {Liu, Yazhou and Chen, Yuliang and Lasang, Pongsak and Sun, Qunsen},
  URL = {https://ieeexplore.ieee.org/document/9206128},
  DATE = {2022-04},
  DOI = {10.1109/TPAMI.2020.3026069},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\8EMUI5NB\\9206128.html:text/html},
  ISSN = {1939-3539},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NOTE = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
  NUMBER = {4},
  PAGES = {1805--1818},
  TITLE = {Covariance {Attention} for {Semantic} {Segmentation}},
  URLDATE = {2024-01-15},
  VOLUME = {44},
}

@INPROCEEDINGS{shi_transformer_2023,
  AUTHOR = {Shi, Hengcan and Hayat, Munawar and Cai, Jianfei},
  BOOKTITLE = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  DATE = {2023},
  PAGES = {3051--3060},
  TITLE = {Transformer scale gate for semantic segmentation},
}

@INPROCEEDINGS{strudel_segmenter_2021,
  AUTHOR = {Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  BOOKTITLE = {Proceedings of the {IEEE}/{CVF} international conference on computer vision},
  DATE = {2021},
  PAGES = {7262--7272},
  TITLE = {Segmenter: {Transformer} for semantic segmentation},
}

@INPROCEEDINGS{hatamizadeh_global_2023,
  AUTHOR = {Hatamizadeh, Ali and Yin, Hongxu and Heinrich, Greg and Kautz, Jan and Molchanov, Pavlo},
  PUBLISHER = {PMLR},
  BOOKTITLE = {International {Conference} on {Machine} {Learning}},
  DATE = {2023},
  PAGES = {12633--12646},
  TITLE = {Global context vision transformers},
}

@ARTICLE{radford_unsupervised_2015,
  AUTHOR = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  DATE = {2015},
  JOURNALTITLE = {arXiv preprint arXiv:1511.06434},
  TITLE = {Unsupervised representation learning with deep convolutional generative adversarial networks},
}

@INPROCEEDINGS{zhang_stackgan_2017,
  AUTHOR = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N},
  BOOKTITLE = {Proceedings of the {IEEE} international conference on computer vision},
  DATE = {2017},
  PAGES = {5907--5915},
  TITLE = {{StackGAN}: {Text} to photo-realistic image synthesis with stacked generative adversarial networks},
}

@ARTICLE{zhang_stackgan_2018,
  AUTHOR = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N},
  DATE = {2018},
  JOURNALTITLE = {IEEE transactions on pattern analysis and machine intelligence},
  NOTE = {Publisher: IEEE},
  NUMBER = {8},
  PAGES = {1947--1962},
  TITLE = {{StackGAN}++: {Realistic} image synthesis with stacked generative adversarial networks},
  VOLUME = {41},
}

@INPROCEEDINGS{zhang_photographic_2018,
  AUTHOR = {Zhang, Zizhao and Xie, Yuanpu and Yang, Lin},
  BOOKTITLE = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  DATE = {2018},
  PAGES = {6199--6208},
  TITLE = {Photographic text-to-image synthesis with a hierarchically-nested adversarial network},
}

@INPROCEEDINGS{zhu_dm-gan_2019,
  AUTHOR = {Zhu, Minfeng and Pan, Pingbo and Chen, Wei and Yang, Yi},
  BOOKTITLE = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
  DATE = {2019},
  PAGES = {5802--5810},
  TITLE = {{DM}-{GAN}: {Dynamic} memory generative adversarial networks for text-to-image synthesis},
}

@INPROCEEDINGS{sun_resfpa-gan_2019,
  AUTHOR = {Sun, Jingcong and Zhou, Yimin and Zhang, Bin},
  PUBLISHER = {IEEE},
  BOOKTITLE = {2019 {IEEE} {International} {Conference} on {Advanced} {Robotics} and its {Social} {Impacts} ({ARSO})},
  DATE = {2019},
  PAGES = {317--322},
  TITLE = {{ResFPA}-{GAN}: {Text}-to-image synthesis with generative adversarial network based on residual block feature pyramid attention},
}

@ARTICLE{cai_dualattn-gan_2019,
  
  AUTHOR = {Cai, Yali and Wang, Xiaoru and Yu, Zhihong and Li, Fu and Xu, Peirong and Li, Yueli and Li, Lixian},
  URL = {https://ieeexplore.ieee.org/document/8930532?denied=},
  DATE = {2019},
  DOI = {10.1109/ACCESS.2019.2958864},
  FILE = {Full Text:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\TJU7EGUD\\Cai et al. - 2019 - Dualattn-GAN Text to Image Synthesis With Dual At.pdf:application/pdf;IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\QZATPBDC\\8930532.html:text/html},
  ISSN = {2169-3536},
  JOURNALTITLE = {IEEE Access},
  KEYWORDS = {Semantics,Generative adversarial network,Generative adversarial networks,Image synthesis,Training,Visualization,Gallium nitride,inverted residual structure,spectral normalization,Task analysis,textual attention,visual attention},
  NOTE = {Conference Name: IEEE Access},
  PAGES = {183706--183716},
  SHORTTITLE = {Dualattn-{GAN}},
  TITLE = {Dualattn-{GAN}: {Text} to {Image} {Synthesis} {With} {Dual} {Attentional} {Generative} {Adversarial} {Network}},
  URLDATE = {2024-03-03},
  VOLUME = {7},
}

@INPROCEEDINGS{xu_attngan_2018,
  AUTHOR = {Xu, Tao and Zhang, Pengchuan and Huang, Qiuyuan and Zhang, Han and Gan, Zhe and Huang, Xiaolei and He, Xiaodong},
  BOOKTITLE = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
  DATE = {2018},
  PAGES = {1316--1324},
  TITLE = {{AttnGAN}: {Fine}-grained text to image generation with attentional generative adversarial networks},
}

@ARTICLE{jiang_-gan_2024,
  
  AUTHOR = {Jiang, Bin and Zeng, Weiyuan and Yang, Chao and Wang, Renjun and Zhang, Bolin},
  LANGUAGE = {en},
  URL = {https://doi.org/10.1007/s11042-023-16377-8},
  DATE = {2024-03},
  DOI = {10.1007/s11042-023-16377-8},
  ISSN = {1573-7721},
  JOURNALTITLE = {Multimedia Tools and Applications},
  KEYWORDS = {Attention mechanism,Generative adversarial network,Cross-modal,Text-to-image synthesis},
  NUMBER = {8},
  PAGES = {23839--23852},
  SHORTTITLE = {{DE}-{GAN}},
  TITLE = {{DE}-{GAN}: {Text}-to-image synthesis with dual and efficient fusion model},
  URLDATE = {2024-03-03},
  VOLUME = {83},
}

@ARTICLE{yang_dmf-gan_2024,
  
  AUTHOR = {Yang, Bing and Xiang, Xueqin and Kong, Wangzeng and Zhang, Jianhai and Peng, Yong},
  URL = {https://ieeexplore.ieee.org/document/10413630},
  DATE = {2024},
  DOI = {10.1109/TMM.2024.3358086},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\IFY76L8V\\10413630.html:text/html},
  ISSN = {1941-0077},
  JOURNALTITLE = {IEEE Transactions on Multimedia},
  KEYWORDS = {Semantics,Generative adversarial networks,Deep multimodal fusion,Fuses,generative adversarial network,Generators,Image synthesis,text-to-image (T2I) synthesis,Training,Visualization},
  NOTE = {Conference Name: IEEE Transactions on Multimedia},
  PAGES = {1--13},
  SHORTTITLE = {{DMF}-{GAN}},
  TITLE = {{DMF}-{GAN}: {Deep} {Multimodal} {Fusion} {Generative} {Adversarial} {Networks} for {Text}-to-{Image} {Synthesis}},
  URLDATE = {2024-03-03},
}

@INPROCEEDINGS{tao_df-gan_2022,
  AUTHOR = {Tao, Ming and Tang, Hao and Wu, Fei and Jing, Xiao-Yuan and Bao, Bing-Kun and Xu, Changsheng},
  BOOKTITLE = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  DATE = {2022},
  PAGES = {16515--16525},
  TITLE = {{DF}-{GAN}: {A} simple and effective baseline for text-to-image synthesis},
}

@ARTICLE{yu_latent_2008,
  
  AUTHOR = {Yu, Bo and Xu, Zong-ben and Li, Cheng-hua},
  URL = {https://www.sciencedirect.com/science/article/pii/S0950705108000993},
  DATE = {2008-12},
  DOI = {10.1016/j.knosys.2008.03.045},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\G2LPKNRX\\S0950705108000993.html:text/html},
  ISSN = {0950-7051},
  JOURNALTITLE = {Knowledge-Based Systems},
  KEYWORDS = {Neural network,Latent semantic analysis,Text categorization},
  NUMBER = {8},
  PAGES = {900--904},
  TITLE = {Latent semantic analysis for text categorization using neural network},
  URLDATE = {2024-01-15},
  VOLUME = {21},
}

@ARTICLE{calvo_intelligent_2000,
  
  AUTHOR = {Calvo, Rafael A. and Ceccatto, H. A.},
  LANGUAGE = {en},
  URL = {https://content.iospress.com/articles/intelligent-data-analysis/ida00028},
  DATE = {2000-01},
  DOI = {10.3233/IDA-2000-4503},
  ISSN = {1088-467X},
  JOURNALTITLE = {Intelligent Data Analysis},
  NOTE = {Publisher: IOS Press},
  NUMBER = {5},
  PAGES = {411--420},
  TITLE = {Intelligent document classification},
  URLDATE = {2024-01-15},
  VOLUME = {4},
}

@INPROCEEDINGS{arevian_recurrent_2007,
  
  AUTHOR = {Arevian, Garen},
  URL = {https://ieeexplore.ieee.org/abstract/document/4427112},
  BOOKTITLE = {{IEEE}/{WIC}/{ACM} {International} {Conference} on {Web} {Intelligence} ({WI}'07)},
  DATE = {2007-11},
  DOI = {10.1109/WI.2007.126},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\DK6JTB5Z\\4427112.html:text/html},
  PAGES = {326--329},
  TITLE = {Recurrent {Neural} {Networks} for {Robust} {Real}-{World} {Text} {Classification}},
  URLDATE = {2024-01-15},
}

@INPROCEEDINGS{huang_optimization_2020,
  
  AUTHOR = {Huang, Jingyu and Feng, Yunfei},
  LOCATION = {New York, NY, USA},
  PUBLISHER = {Association for Computing Machinery},
  URL = {https://dl.acm.org/doi/10.1145/3373509.3373573},
  BOOKTITLE = {Proceedings of the 2019 8th {International} {Conference} on {Computing} and {Pattern} {Recognition}},
  DATE = {2020-03},
  DOI = {10.1145/3373509.3373573},
  FILE = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\RN36MANB\\Huang and Feng - 2020 - Optimization of Recurrent Neural Networks on Natur.pdf:application/pdf},
  ISBN = {978-1-4503-7657-0},
  KEYWORDS = {Natural Language Processing,Neural Network,Recurrent,Text Classification},
  PAGES = {39--45},
  SERIES = {{ICCPR} '19},
  TITLE = {Optimization of {Recurrent} {Neural} {Networks} on {Natural} {Language} {Processing}},
  URLDATE = {2024-01-15},
}

@INPROCEEDINGS{wang_convolutional_2019,
  
  AUTHOR = {Wang, Ruishuang and Li, Zhao and Cao, Jian and Chen, Tong and Wang, Lei},
  URL = {https://ieeexplore.ieee.org/document/8852406},
  BOOKTITLE = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
  DATE = {2019-07},
  DOI = {10.1109/IJCNN.2019.8852406},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\IED88T7D\\8852406.html:text/html},
  NOTE = {ISSN: 2161-4407},
  PAGES = {1--6},
  TITLE = {Convolutional {Recurrent} {Neural} {Networks} for {Text} {Classification}},
  URLDATE = {2024-01-16},
}

@ARTICLE{liu_bidirectional_2019,
  
  AUTHOR = {Liu, Gang and Guo, Jiabao},
  URL = {https://www.sciencedirect.com/science/article/pii/S0925231219301067},
  DATE = {2019-04},
  DOI = {10.1016/j.neucom.2019.01.078},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\WQYI8YKV\\S0925231219301067.html:text/html},
  ISSN = {0925-2312},
  JOURNALTITLE = {Neurocomputing},
  KEYWORDS = {Attention mechanism,Long short-term memory,Natural language processing,Text classification},
  PAGES = {325--338},
  TITLE = {Bidirectional {LSTM} with attention mechanism and convolutional layer for text classification},
  URLDATE = {2024-01-18},
  VOLUME = {337},
}

@ARTICLE{li_bidirectional_2020,
  
  AUTHOR = {Li, Weijiang and Qi, Fang and Tang, Ming and Yu, Zhengtao},
  URL = {https://www.sciencedirect.com/science/article/pii/S0925231220300254},
  DATE = {2020-04},
  DOI = {10.1016/j.neucom.2020.01.006},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\EC5DWVCQ\\S0925231220300254.html:text/html},
  ISSN = {0925-2312},
  JOURNALTITLE = {Neurocomputing},
  KEYWORDS = {Bidirectional long short-term memory,Multi-channel features,Self-attention mechanism,Sentiment classification},
  PAGES = {63--77},
  TITLE = {Bidirectional {LSTM} with self-attention mechanism and multi-channel features for sentiment classification},
  URLDATE = {2024-01-18},
  VOLUME = {387},
}

@ARTICLE{lin_structured_2017,
  AUTHOR = {Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  DATE = {2017},
  JOURNALTITLE = {arXiv preprint arXiv:1703.03130},
  TITLE = {A structured self-attentive sentence embedding},
}

@ARTICLE{devlin_bert_2018,
  AUTHOR = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  DATE = {2018},
  JOURNALTITLE = {arXiv preprint arXiv:1810.04805},
  TITLE = {{BERT}: {Pre}-training of deep bidirectional transformers for language understanding},
}

@INPROCEEDINGS{sun_how_2019,
  AUTHOR = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  PUBLISHER = {Springer},
  BOOKTITLE = {Chinese {Computational} {Linguistics}: 18th {China} {National} {Conference}, {CCL} 2019, {Kunming}, {China}, {October} 18–20, 2019, {Proceedings} 18},
  DATE = {2019},
  PAGES = {194--206},
  TITLE = {How to fine-tune bert for text classification?},
}

@ARTICLE{liu_roberta_2019,
  AUTHOR = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  DATE = {2019},
  JOURNALTITLE = {arXiv preprint arXiv:1907.11692},
  TITLE = {{RoBERTa}: {A} robustly optimized bert pretraining approach},
}

@ARTICLE{he_deberta_2020,
  AUTHOR = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  DATE = {2020},
  JOURNALTITLE = {arXiv preprint arXiv:2006.03654},
  TITLE = {{DeBERTa}: {Decoding}-enhanced bert with disentangled attention},
}

@ARTICLE{lan_albert_2019,
  AUTHOR = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  DATE = {2019},
  JOURNALTITLE = {arXiv preprint arXiv:1909.11942},
  TITLE = {{ALBERT}: {A} lite bert for self-supervised learning of language representations},
}

@ARTICLE{rodrawangpai_improving_2022,
  
  AUTHOR = {Rodrawangpai, Ben and Daungjaiboon, Witawat},
  URL = {https://www.sciencedirect.com/science/article/pii/S2666827022000792},
  DATE = {2022-12},
  DOI = {10.1016/j.mlwa.2022.100403},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\ANRINJAA\\S2666827022000792.html:text/html},
  ISSN = {2666-8270},
  JOURNALTITLE = {Machine Learning with Applications},
  KEYWORDS = {Machine learning,Natural language processing,Computational modeling,Pattern classification,Semantics,Text analysis,Transformers},
  PAGES = {100403},
  TITLE = {Improving text classification with transformers and layer normalization},
  URLDATE = {2024-01-18},
  VOLUME = {10},
}

@ARTICLE{yan_r-transformer_bilstm_2023,
  
  AUTHOR = {Yan, Yaoyao and Liu, Fang’ai and Zhuang, Xuqiang and Ju, Jie},
  LANGUAGE = {en},
  URL = {https://doi.org/10.1007/s11063-022-10938-y},
  DATE = {2023-04},
  DOI = {10.1007/s11063-022-10938-y},
  ISSN = {1573-773X},
  JOURNALTITLE = {Neural Processing Letters},
  KEYWORDS = {Multi-label text classification,BiLSTM,Label embedding,R-Transformer,Self-attention},
  NUMBER = {2},
  PAGES = {1293--1316},
  TITLE = {An {R}-{Transformer}\_BiLSTM {Model} {Based} on {Attention} for {Multi}-label {Text} {Classification}},
  URLDATE = {2024-01-18},
  VOLUME = {55},
}

@ARTICLE{hao_sentiment_2023,
  
  AUTHOR = {Hao, Shule and Zhang, Peng and Liu, Sen and Wang, Yuhang},
  LANGUAGE = {en},
  URL = {https://doi.org/10.1007/s00521-023-08226-4},
  DATE = {2023-12},
  DOI = {10.1007/s00521-023-08226-4},
  ISSN = {1433-3058},
  JOURNALTITLE = {Neural Computing and Applications},
  KEYWORDS = {Neural network,BERT model,Document text analysis,Support vector machine (SVM) classifier,Text sentiment recognition},
  NUMBER = {35},
  PAGES = {24621--24632},
  TITLE = {Sentiment recognition and analysis method of official document text based on {BERT}–{SVM} model},
  URLDATE = {2024-01-18},
  VOLUME = {35},
}

@ARTICLE{murfi_bert-based_2024,
  
  AUTHOR = {Murfi, Hendri and {Syamsyuriani} and Gowandi, Theresia and Ardaneswari, Gianinna and Nurrohmah, Siti},
  URL = {https://www.sciencedirect.com/science/article/pii/S1568494623011304},
  DATE = {2024-01},
  DOI = {10.1016/j.asoc.2023.111112},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\BAHJD59U\\S1568494623011304.html:text/html;Submitted Version:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\BMQ4Y65V\\Murfi et al. - 2024 - BERT-based combination of convolutional and recurr.pdf:application/pdf},
  ISSN = {1568-4946},
  JOURNALTITLE = {Applied Soft Computing},
  KEYWORDS = {Deep learning,BERT,Hybrid deep learning,Sentiment analysis,Text representation},
  PAGES = {111112},
  TITLE = {{BERT}-based combination of convolutional and recurrent neural network for indonesian sentiment analysis},
  URLDATE = {2024-02-09},
  VOLUME = {151},
}

@ARTICLE{wang_joint_2018,
  AUTHOR = {Wang, Guoyin and Li, Chunyuan and Wang, Wenlin and Zhang, Yizhe and Shen, Dinghan and Zhang, Xinyuan and Henao, Ricardo and Carin, Lawrence},
  DATE = {2018},
  JOURNALTITLE = {arXiv preprint arXiv:1805.04174},
  TITLE = {Joint embedding of words and labels for text classification},
}

@ARTICLE{zhu_bert-based_2023,
  
  AUTHOR = {Zhu, Xinhua and Zhu, Yuxiang and Zhang, Lanfang and Chen, Yishan},
  LANGUAGE = {en},
  URL = {https://doi.org/10.1007/s10489-022-03702-1},
  DATE = {2023-02},
  DOI = {10.1007/s10489-022-03702-1},
  ISSN = {1573-7497},
  JOURNALTITLE = {Applied Intelligence},
  KEYWORDS = {Aspect-aware enhancement,Aspect-target sentiment classification,BERT language model,Multi-head attention,Multi-semantic learning},
  NUMBER = {4},
  PAGES = {4609--4623},
  TITLE = {A {BERT}-based multi-semantic learning model with aspect-aware enhancement for aspect polarity classification},
  URLDATE = {2024-01-18},
  VOLUME = {53},
}

@ARTICLE{sutskever_sequence_2014,
  AUTHOR = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  DATE = {2014},
  JOURNALTITLE = {Advances in neural information processing systems},
  TITLE = {Sequence to sequence learning with neural networks},
  VOLUME = {27},
}

@ARTICLE{wu_googles_2016,
  AUTHOR = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  DATE = {2016},
  JOURNALTITLE = {arXiv preprint arXiv:1609.08144},
  TITLE = {Google's neural machine translation system: {Bridging} the gap between human and machine translation},
}

@ARTICLE{bahdanau_neural_2014,
  AUTHOR = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  DATE = {2014},
  JOURNALTITLE = {arXiv preprint arXiv:1409.0473},
  TITLE = {Neural machine translation by jointly learning to align and translate},
}

@ARTICLE{luong_effective_2015,
  AUTHOR = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
  DATE = {2015},
  JOURNALTITLE = {arXiv preprint arXiv:1508.04025},
  TITLE = {Effective approaches to attention-based neural machine translation},
}

@ARTICLE{stahlberg_neural_2020,
  AUTHOR = {Stahlberg, Felix},
  DATE = {2020},
  JOURNALTITLE = {Journal of Artificial Intelligence Research},
  PAGES = {343--418},
  TITLE = {Neural machine translation: {A} review},
  VOLUME = {69},
}

@ARTICLE{lupo_encoding_2023,
  AUTHOR = {Lupo, Lorenzo and Dinarelli, Marco and Besacier, Laurent},
  DATE = {2023},
  JOURNALTITLE = {arXiv preprint arXiv:2302.06459},
  TITLE = {Encoding {Sentence} {Position} in {Context}-{Aware} {Neural} {Machine} {Translation} with {Concatenation}},
}

@ARTICLE{rippeth_improving_2023,
  AUTHOR = {Rippeth, Elijah and Carpuat, Marine and Duh, Kevin and Post, Matt},
  DATE = {2023},
  JOURNALTITLE = {arXiv preprint arXiv:2311.15507},
  TITLE = {Improving {Word} {Sense} {Disambiguation} in {Neural} {Machine} {Translation} with {Salient} {Document} {Context}},
}

@ARTICLE{wu_study_2022,
  AUTHOR = {Wu, Xueqing and Xia, Yingce and Zhu, Jinhua and Wu, Lijun and Xie, Shufang and Qin, Tao},
  DATE = {2022},
  JOURNALTITLE = {Machine Learning},
  NOTE = {Publisher: Springer},
  NUMBER = {3},
  PAGES = {917--935},
  TITLE = {A study of {BERT} for context-aware neural machine translation},
  VOLUME = {111},
}

@INPROCEEDINGS{kim_towards_2023,
  AUTHOR = {Kim, Dohee and Baek, Yujin and Yang, Soyoung and Choo, Jaegul},
  BOOKTITLE = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
  DATE = {2023},
  PAGES = {7384--7392},
  TITLE = {Towards {Formality}-{Aware} {Neural} {Machine} {Translation} by {Leveraging} {Context} {Information}},
}

@INPROCEEDINGS{gezmu_transformers_2022,
  AUTHOR = {Gezmu, Andargachew Mekonnen and Nürnberger, Andreas},
  BOOKTITLE = {{ICAART} (1)},
  DATE = {2022},
  PAGES = {459--466},
  TITLE = {Transformers for {Low}-resource {Neural} {Machine} {Translation}.},
}

@ARTICLE{li_towards_2024,
  AUTHOR = {Li, Bin and Weng, Yixuan and Xia, Fei and Deng, Hanjun},
  DATE = {2024},
  JOURNALTITLE = {Computer Speech \& Language},
  NOTE = {Publisher: Elsevier},
  PAGES = {101566},
  TITLE = {Towards better {Chinese}-centric neural machine translation for low-resource languages},
  VOLUME = {84},
}

@ARTICLE{meetei_cues_2023,
  AUTHOR = {Meetei, Loitongbam Sanayai and Singh, Alok and Singh, Thoudam Doren and Bandyopadhyay, Sivaji},
  DATE = {2023},
  JOURNALTITLE = {Natural Language Processing Journal},
  NOTE = {Publisher: Elsevier},
  PAGES = {100016},
  TITLE = {Do cues in a video help in handling rare words in a machine translation system under a low-resource setting?},
  VOLUME = {3},
}

@INPROCEEDINGS{araabi_optimizing_2020,
  
  AUTHOR = {Araabi, Ali and Monz, Christof},
  EDITOR = {Scott, Donia and Bel, Nuria and Zong, Chengqing},
  LOCATION = {Barcelona, Spain (Online)},
  PUBLISHER = {International Committee on Computational Linguistics},
  URL = {https://aclanthology.org/2020.coling-main.304},
  BOOKTITLE = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
  DATE = {2020-12},
  DOI = {10.18653/v1/2020.coling-main.304},
  PAGES = {3429--3435},
  TITLE = {Optimizing {Transformer} for {Low}-{Resource} {Neural} {Machine} {Translation}},
}

@article{faheem_improving_2024,
	title = {Improving neural machine translation for low resource languages through non-parallel corpora: a case study of {Egyptian} dialect to modern standard {Arabic} translation},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	shorttitle = {Improving neural machine translation for low resource languages through non-parallel corpora},
	url = {https://www.nature.com/articles/s41598-023-51090-4},
	doi = {10.1038/s41598-023-51090-4},
	abstract = {Machine translation for low-resource languages poses significant challenges, primarily due to the limited availability of data. In recent years, unsupervised learning has emerged as a promising approach to overcome this issue by aiming to learn translations between languages without depending on parallel data. A wide range of methods have been proposed in the literature to address this complex problem. This paper presents an in-depth investigation of semi-supervised neural machine translation specifically focusing on translating Arabic dialects, particularly Egyptian, to Modern Standard Arabic. The study employs two distinct datasets: one parallel dataset containing aligned sentences in both dialects, and a monolingual dataset where the source dialect is not directly connected to the target language in the training data. Three different translation systems are explored in this study. The first is an attention-based sequence-to-sequence model that benefits from the shared vocabulary between the Egyptian dialect and Modern Arabic to learn word embeddings. The second is an unsupervised transformer model that depends solely on monolingual data, without any parallel data. The third system starts with the parallel dataset for an initial supervised learning phase and then incorporates the monolingual data during the training process.},
	language = {en},
	number = {1},
	urldate = {2025-03-07},
	journal = {Scientific Reports},
	author = {Faheem, Mohamed Atta and Wassif, Khaled Tawfik and Bayomi, Hanaa and Abdou, Sherif Mahdy},
	month = jan,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing, Scientific data, Information technology},
	pages = {2265},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\FJ9WR47C\\Faheem et al. - 2024 - Improving neural machine translation for low resou.pdf:application/pdf},
}

### Machine translation end

### Text generation begin

@ARTICLE{yin_neural_2015,
  AUTHOR = {Yin, Jun and Jiang, Xin and Lu, Zhengdong and Shang, Lifeng and Li, Hang and Li, Xiaoming},
  DATE = {2015},
  JOURNALTITLE = {arXiv preprint arXiv:1512.01337},
  TITLE = {Neural generative question answering},
}

@ARTICLE{nie_attention-based_2017,
  AUTHOR = {Nie, Yuan-ping and Han, Yi and Huang, Jiu-ming and Jiao, Bo and Li, Ai-ping},
  DATE = {2017},
  JOURNALTITLE = {Frontiers of Information Technology \& Electronic Engineering},
  NOTE = {Publisher: Springer},
  NUMBER = {4},
  PAGES = {535--544},
  TITLE = {Attention-based encoder-decoder model for answer selection in question answering},
  VOLUME = {18},
}

@ARTICLE{li_deep_2016,
  AUTHOR = {Li, Jiwei and Monroe, Will and Ritter, Alan and Galley, Michel and Gao, Jianfeng and Jurafsky, Dan},
  DATE = {2016},
  JOURNALTITLE = {arXiv preprint arXiv:1606.01541},
  TITLE = {Deep reinforcement learning for dialogue generation},
}

@ARTICLE{wu_building_2020,
  AUTHOR = {Wu, Jinmeng and Mu, Tingting and Thiyagalingam, Jeyarajan and Goulermas, John Y},
  DATE = {2020},
  JOURNALTITLE = {Neurocomputing},
  NOTE = {Publisher: Elsevier},
  PAGES = {93--107},
  TITLE = {Building interactive sentence-aware representation based on generative language model for community question answering},
  VOLUME = {389},
}

@ARTICLE{li_incremental_2019,
  AUTHOR = {Li, Zekang and Niu, Cheng and Meng, Fandong and Feng, Yang and Li, Qian and Zhou, Jie},
  DATE = {2019},
  JOURNALTITLE = {arXiv preprint arXiv:1907.08854},
  TITLE = {Incremental transformer with deliberation decoder for document grounded conversations},
}

@INPROCEEDINGS{alrowili_biom-transformers_2021,
  AUTHOR = {Alrowili, Sultan and Vijay-Shanker, K},
  BOOKTITLE = {Proceedings of the 20th workshop on biomedical language processing},
  DATE = {2021},
  PAGES = {221--227},
  TITLE = {{BioM}-transformers: building large biomedical language models with {BERT}, {ALBERT} and {ELECTRA}},
}

@INPROCEEDINGS{alrowili_exploring_2022,
  AUTHOR = {Alrowili, Sultan and Vijay-Shanker, K},
  BOOKTITLE = {{CLEF} ({Working} {Notes})},
  DATE = {2022},
  PAGES = {222--234},
  TITLE = {Exploring {Biomedical} {Question} {Answering} with {BioM}-{Transformers} {At} {BioASQ10B} challenge: {Findings} and {Techniques}.},
}

@article{li_feature-aware_2023,
	title = {Feature-aware conditional {GAN} for category text generation},
	volume = {547},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223004757},
	doi = {10.1016/j.neucom.2023.126352},
	abstract = {Category text generation receives considerable attentions since it is beneficial for various natural language processing tasks. Recently, the generative adversarial network (GAN) has attained promising performance in text generation, attributed to its adversarial training process. However, there are several issues in text GANs, including discreteness, training instability, mode collapse, lack of diversity and controllability etc. To address these issues, this paper proposes a novel GAN framework, the feature-aware conditional GAN (FA-GAN), for controllable category text generation. In FA-GAN, the generator has a sequence-to-sequence structure for improving sentence diversity, which consists of three encoders including a special feature-aware encoder and a category-aware encoder, and one relational-memory-core-based decoder with the Gumbel SoftMax activation function. The discriminator has an additional category classification head. To generate sentences with specified categories, the multi-class classification loss is supplemented in the adversarial training. Comprehensive experiments have been conducted, and the results show that FA-GAN consistently outperforms 10 state-of-the-art text generation approaches on 6 text classification datasets. The case study demonstrates that the synthetic sentences generated by FA-GAN can match the required categories and are aware of the features of conditioned sentences, with good readability, fluency, and text authenticity.},
	urldate = {2025-03-08},
	journal = {Neurocomputing},
	author = {Li, Xinze and Mao, Kezhi and Lin, Fanfan and Feng, Zijian},
	month = aug,
	year = {2023},
	keywords = {Adversarial text generation, Category text generation, Conditional generative adversarial net, Generative model, Sequence-to-sequence, Text classification},
	pages = {126352},
	file = {Submitted Version:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\QEQRRHC3\\Li et al. - 2023 - Feature-aware conditional GAN for category text ge.pdf:application/pdf},
}

@article{kwon_class_2024,
	title = {Class conditioned text generation with style attention mechanism for embracing diversity},
	volume = {163},
	issn = {1568-4946},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494624006677},
	doi = {10.1016/j.asoc.2024.111893},
	abstract = {In the field of artificial intelligence and natural language processing (NLP), natural language generation (NLG) has significantly advanced. Its primary aim is to automatically generate text in a manner resembling human language. Traditional text generation has mainly focused on binary style transfers, limiting the scope to simple transformations between positive and negative tones or between modern and ancient styles. However, accommodating style diversity in real scenarios presents greater complexity and demand. Existing methods usually fail to capture the richness of diverse styles, hindering their utility in practical applications. To address these limitations, we propose a multi-class conditioned text generation model. We overcome previous constraints by utilizing a transformer-based decoder equipped with adversarial networks and style-attention mechanisms to model various styles in multi-class text. According to our experimental results, the proposed model achieved better performance compared to the alternatives on multi-class text generation tasks in terms of diversity while it preserves fluency. We expect that our study will help researchers not only train their models but also build simulated multi-class text datasets for further research.},
	urldate = {2025-03-07},
	journal = {Applied Soft Computing},
	author = {Kwon, Naae and Yoo, Yuenkyung and Lee, Byunghan},
	month = sep,
	year = {2024},
	keywords = {Multi-class, Natural language generation, Non-parallel, Style attention, Text style},
	pages = {111893},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\P6QNIZ4Y\\S1568494624006677.html:text/html},
}

@ARTICLE{xie_pre-trained_2022,
  AUTHOR = {Xie, Qianqian and Bishop, Jennifer Amy and Tiwari, Prayag and Ananiadou, Sophia},
  DATE = {2022},
  JOURNALTITLE = {Knowledge-Based Systems},
  NOTE = {Publisher: Elsevier},
  PAGES = {109460},
  TITLE = {Pre-trained language models with domain knowledge for biomedical extractive summarization},
  VOLUME = {252},
}

@article{hajipoor_gptgan_2025,
	title = {{GPTGAN}: {Utilizing} the {GPT} language model and {GAN} to enhance adversarial text generation},
	volume = {617},
	issn = {0925-2312},
	shorttitle = {{GPTGAN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231224016369},
	doi = {10.1016/j.neucom.2024.128865},
	abstract = {Training generative models that can generate high-quality and diverse text remains a significant challenge in the field of natural language generation (NLG). Recently, the emergence of large language models (LLMs) like GPT has enabled the generation of text with remarkable quality and diversity. However, building these models from scratch is both time-consuming and resource-intensive, making their comprehensive training practically unfeasible. Nonetheless, LLMs utility extends to addressing issues in other models. For instance, generative adversarial models often grapple with the well-known problem of mode collapse during training, leading to a trade-off between text quality and diversity. This means that these models tend to favor quality over diversity. In this study, we introduce a novel approach designed to enhance adversarial text generation by striking a balance between the quality and diversity of generated text, leveraging the capabilities of the GPT language model and other LLMs. To achieve this, we propose an enhanced generator that is guided by the GPT model. Essentially, the GPT model functions as a mentor to the generator, influencing its outputs. To achieve this guidance, we employ discriminators of varying scales on both real data and the texts generated by GPT. Experimental results underscore a substantial enhancement in the quality and diversity of outcomes across two benchmark datasets. Also the results demonstrate the generator’s ability to assimilate the output domain of the GPT language model. Furthermore, the proposed model exhibits superior performance in human evaluations when compared to other existing adversarial methods.},
	urldate = {2025-03-07},
	journal = {Neurocomputing},
	author = {Hajipoor, Omid and Nickabadi, Ahmad and Homayounpour, Mohammad Mehdi},
	month = feb,
	year = {2025},
	keywords = {Generative adversarial networks, Automatic text generation, Generative pretrained transformer (GPT), GPTGAN, Large language model (LLM)},
	pages = {128865},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\QB9FCBTL\\S0925231224016369.html:text/html},
}

### Text generation end

@ARTICLE{ma_t-bertsum_2021,
  AUTHOR = {Ma, Tinghuai and Pan, Qian and Rong, Huan and Qian, Yurong and Tian, Yuan and Al-Nabhan, Najla},
  DATE = {2021},
  JOURNALTITLE = {IEEE Transactions on Computational Social Systems},
  NOTE = {Publisher: IEEE},
  NUMBER = {3},
  PAGES = {879--890},
  TITLE = {T-{BERTSum}: {Topic}-aware text summarization based on bert},
  VOLUME = {9},
}

@ARTICLE{munappy_data_2022,
  
  AUTHOR = {Munappy, Aiswarya Raj and Bosch, Jan and Olsson, Helena Holmström and Arpteg, Anders and Brinne, Björn},
  URL = {https://www.sciencedirect.com/science/article/pii/S0164121222000905},
  DATE = {2022-09},
  DOI = {10.1016/j.jss.2022.111359},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\XVFQN25X\\S0164121222000905.html:text/html},
  ISSN = {0164-1212},
  JOURNALTITLE = {Journal of Systems and Software},
  KEYWORDS = {Deep learning,Challenges,Data management,Production quality DL models,Solutions,Validation},
  PAGES = {111359},
  SHORTTITLE = {Data management for production quality deep learning models},
  TITLE = {Data management for production quality deep learning models: {Challenges} and solutions},
  URLDATE = {2024-02-16},
  VOLUME = {191},
}

@ARTICLE{luca_impact_2022,
  
  AUTHOR = {Luca, Andreea Roxana and Ursuleanu, Tudor Florin and Gheorghe, Liliana and Grigorovici, Roxana and Iancu, Stefan and Hlusneac, Maria and Grigorovici, Alexandru},
  URL = {https://www.sciencedirect.com/science/article/pii/S2352914822000612},
  DATE = {2022-01},
  DOI = {10.1016/j.imu.2022.100911},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\UVJRK3ID\\S2352914822000612.html:text/html},
  ISSN = {2352-9148},
  JOURNALTITLE = {Informatics in Medicine Unlocked},
  KEYWORDS = {Data types,Deep learning models,Labels,Medical image analysis},
  PAGES = {100911},
  TITLE = {Impact of quality, type and volume of data used by deep learning models in the analysis of medical images},
  URLDATE = {2024-02-16},
  VOLUME = {29},
}

@ARTICLE{mumuni_data_2022,
  
  AUTHOR = {Mumuni, Alhassan and Mumuni, Fuseini},
  URL = {https://www.sciencedirect.com/science/article/pii/S2590005622000911},
  DATE = {2022-12},
  DOI = {10.1016/j.array.2022.100258},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\85AWL23Z\\S2590005622000911.html:text/html},
  ISSN = {2590-0056},
  JOURNALTITLE = {Array},
  KEYWORDS = {Machine learning,Computer vision,Generative adversarial network,Meta-learning,Review of data augmentation,Synthetic data},
  PAGES = {100258},
  SHORTTITLE = {Data augmentation},
  TITLE = {Data augmentation: {A} comprehensive survey of modern approaches},
  URLDATE = {2024-02-16},
  VOLUME = {16},
}

@ARTICLE{zhuang_comprehensive_2020,
  AUTHOR = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  DATE = {2020},
  JOURNALTITLE = {Proceedings of the IEEE},
  NOTE = {Publisher: IEEE},
  NUMBER = {1},
  PAGES = {43--76},
  TITLE = {A comprehensive survey on transfer learning},
  VOLUME = {109},
}

@ARTICLE{murtaza_synthetic_2023,
  
  AUTHOR = {Murtaza, Hajra and Ahmed, Musharif and Khan, Naurin Farooq and Murtaza, Ghulam and Zafar, Saad and Bano, Ambreen},
  URL = {https://www.sciencedirect.com/science/article/pii/S1574013723000138},
  DATE = {2023-05},
  DOI = {10.1016/j.cosrev.2023.100546},
  FILE = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\T4UPZK28\\S1574013723000138.html:text/html},
  ISSN = {1574-0137},
  JOURNALTITLE = {Computer Science Review},
  KEYWORDS = {Synthetic data,Data privacy,Electronic health records,Generative adversarial networks,Health informatics,Medical informatics,Privacy preserving data publishing},
  PAGES = {100546},
  SHORTTITLE = {Synthetic data generation},
  TITLE = {Synthetic data generation: {State} of the art in health care domain},
  URLDATE = {2024-02-18},
  VOLUME = {48},
}

@ARTICLE{hu_survey_2023,
  
  AUTHOR = {Hu, Chaochen and Sun, Zihan and Li, Chao and Zhang, Yong and Xing, Chunxiao},
  LANGUAGE = {en},
  URL = {https://www.mdpi.com/1424-8220/23/15/6976},
  DATE = {2023-01},
  DOI = {10.3390/s23156976},
  FILE = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\RP7BL62Z\\Hu et al. - 2023 - Survey of Time Series Data Generation in IoT.pdf:application/pdf},
  ISSN = {1424-8220},
  JOURNALTITLE = {Sensors},
  KEYWORDS = {categorization,data generation,IoT,time series},
  NOTE = {Number: 15 Publisher: Multidisciplinary Digital Publishing Institute},
  NUMBER = {15},
  PAGES = {6976},
  TITLE = {Survey of {Time} {Series} {Data} {Generation} in {IoT}},
  URLDATE = {2024-02-18},
  VOLUME = {23},
}

@INPROCEEDINGS{tonekaboni_what_2019,
  AUTHOR = {Tonekaboni, Sana and Joshi, Shalmali and McCradden, Melissa D and Goldenberg, Anna},
  PUBLISHER = {PMLR},
  BOOKTITLE = {Machine learning for healthcare conference},
  DATE = {2019},
  PAGES = {359--380},
  TITLE = {What clinicians want: contextualizing explainable machine learning for clinical end use},
}

@ARTICLE{tjoa_enhancing_2023,
  AUTHOR = {Tjoa, Erico and Khok, Hong Jing and Chouhan, Tushar and Guan, Cuntai},
  DATE = {2023},
  JOURNALTITLE = {Neurocomputing},
  NOTE = {Publisher: Elsevier},
  PAGES = {126825},
  TITLE = {Enhancing the confidence of deep learning classifiers via interpretable saliency maps},
  VOLUME = {562},
}

@ARTICLE{termritthikun_explainable_2023,
  AUTHOR = {Termritthikun, Chakkrit and Umer, Ayaz and Suwanwimolkul, Suwichaya and Xia, Feng and Lee, Ivan},
  DATE = {2023},
  JOURNALTITLE = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  NOTE = {Publisher: IEEE},
  TITLE = {Explainable knowledge distillation for on-device chest x-ray classification},
}

@ARTICLE{li_hybrid_2024,
  AUTHOR = {Li, Xihua and Shen, Qikun},
  DATE = {2024},
  JOURNALTITLE = {Expert Systems with Applications},
  NOTE = {Publisher: Elsevier},
  PAGES = {121844},
  TITLE = {A hybrid framework based on knowledge distillation for explainable disease diagnosis},
  VOLUME = {238},
}

@ARTICLE{fernandes_intrinsic_2023,
  AUTHOR = {Fernandes, Luís and Fernandes, João ND and Calado, Mariana and Pinto, João Ribeiro and Cerqueira, Ricardo and Cardoso, Jaime S},
  DATE = {2023},
  JOURNALTITLE = {IEEE Access},
  NOTE = {Publisher: IEEE},
  TITLE = {Intrinsic {Explainability} for {End}-to-{End} {Object} {Detection}},
}

@ARTICLE{xiong_explainable_2022,
  
  AUTHOR = {Xiong, Wei and Xiong, Zhenyu and Cui, Yaqi},
  URL = {https://ieeexplore.ieee.org/abstract/document/9741720},
  DATE = {2022},
  DOI = {10.1109/TGRS.2022.3162195},
  FILE = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\ATYZR87Q\\9741720.html:text/html},
  ISSN = {1558-0644},
  JOURNALTITLE = {IEEE Transactions on Geoscience and Remote Sensing},
  KEYWORDS = {Causal inference,explainable visual attention,Feature extraction,fine-grained ship classification,Marine vehicles,Predictive models,remote sensing,Remote sensing,Task analysis,Training,Visualization},
  NOTE = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
  PAGES = {1--14},
  TITLE = {An {Explainable} {Attention} {Network} for {Fine}-{Grained} {Ship} {Classification} {Using} {Remote}-{Sensing} {Images}},
  URLDATE = {2024-03-13},
  VOLUME = {60},
}

@ARTICLE{gicic2023intelligent,
  AUTHOR = {Gicić, Adaleta and Donko, Dženana and Subasi, Abdulhamit},
  PUBLISHER = {Wiley Online Library},
  DATE = {2023},
  JOURNALTITLE = {Concurrency and Computation: Practice and Experience},
  NUMBER = {9},
  PAGES = {e7637},
  TITLE = {Intelligent credit scoring using deep learning methods},
  VOLUME = {35},
}

@ARTICLE{freire_e-recruitment_2021,
  AUTHOR = {Freire, Mauricio Noris and de Castro, Leandro Nunes},
  DATE = {2021},
  JOURNALTITLE = {Knowledge and Information Systems},
  NOTE = {Publisher: Springer},
  PAGES = {1--20},
  TITLE = {e-{Recruitment} recommender systems: a systematic review},
  VOLUME = {63},
}

@ARTICLE{dass_detecting_2023,
  AUTHOR = {Dass, Rahul Kumar and Petersen, Nick and Omori, Marisa and Lave, Tamara Rice and Visser, Ubbo},
  DATE = {2023},
  JOURNALTITLE = {AI \& SOCIETY},
  NOTE = {Publisher: Springer},
  NUMBER = {2},
  PAGES = {897--918},
  TITLE = {Detecting racial inequalities in criminal justice: towards an equitable deep learning approach for generating and interpreting racial categories using mugshots},
  VOLUME = {38},
}

@ARTICLE{giloni_benn_2022,
  AUTHOR = {Giloni, Amit and Grolman, Edita and Hagemann, Tanja and Fromm, Ronald and Fischer, Sebastian and Elovici, Yuval and Shabtai, Asaf},
  DATE = {2022},
  JOURNALTITLE = {IEEE Transactions on Neural Networks and Learning Systems},
  NOTE = {Publisher: IEEE},
  TITLE = {{BENN}: {Bias} {Estimation} {Using} a {Deep} {Neural} {Network}},
}

@INPROCEEDINGS{schaaf_towards_2021,
  AUTHOR = {Schaaf, Nina and de Mitri, Omar and Kim, Hang Beom and Windberger, Alexander and Huber, Marco F},
  PUBLISHER = {Springer},
  BOOKTITLE = {Artificial {Neural} {Networks} and {Machine} {Learning}–{ICANN} 2021: 30th {International} {Conference} on {Artificial} {Neural} {Networks}, {Bratislava}, {Slovakia}, {September} 14–17, 2021, {Proceedings}, {Part} {III} 30},
  DATE = {2021},
  PAGES = {433--445},
  TITLE = {Towards measuring bias in image classification},
}

@INPROCEEDINGS{iosifidis_fairness-enhancing_2019,
  AUTHOR = {Iosifidis, Vasileios and Tran, Thi Ngoc Han and Ntoutsi, Eirini},
  PUBLISHER = {Springer},
  BOOKTITLE = {Database and {Expert} {Systems} {Applications}: 30th {International} {Conference}, {DEXA} 2019, {Linz}, {Austria}, {August} 26–29, 2019, {Proceedings}, {Part} {I} 30},
  DATE = {2019},
  PAGES = {261--276},
  TITLE = {Fairness-enhancing interventions in stream classification},
}

@ARTICLE{kehrenberg_tuning_2020,
  AUTHOR = {Kehrenberg, Thomas and Chen, Zexun and Quadrianto, Novi},
  DATE = {2020},
  JOURNALTITLE = {Frontiers in artificial intelligence},
  NOTE = {Publisher: Frontiers Media SA},
  PAGES = {33},
  TITLE = {Tuning fairness by balancing target labels},
  VOLUME = {3},
}

@ARTICLE{jain_increasing_2023,
  AUTHOR = {Jain, Bhanu and Huber, Manfred and Elmasri, Ramez},
  LANGUAGE = {en},
  URL = {https://journals.flvc.org/FLAIRS/article/view/133311},
  DATE = {2023-05},
  DOI = {10.32473/flairs.36.133311},
  FILE = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\GB5IVQBH\\Jain et al. - 2023 - Increasing Fairness in Predictions Using Bias Pari.pdf:application/pdf},
  ISSN = {2334-0762},
  JOURNALTITLE = {The International FLAIRS Conference Proceedings},
  KEYWORDS = {Bias Parity Score,Deep Learning,Fairness in ML,Fairness Regularization},
  TITLE = {Increasing {Fairness} in {Predictions} {Using} {Bias} {Parity} {Score} {Based} {Loss} {Function} {Regularization}},
  URLDATE = {2024-03-19},
  VOLUME = {36},
}

@ARTICLE{yang_algorithmic_2023,
  AUTHOR = {Yang, Jenny and Soltan, Andrew AS and Eyre, David W and Clifton, David A},
  DATE = {2023},
  JOURNALTITLE = {Nature Machine Intelligence},
  NOTE = {Publisher: Nature Publishing Group UK London},
  NUMBER = {8},
  PAGES = {884--894},
  TITLE = {Algorithmic fairness and bias mitigation for clinical machine learning with deep reinforcement learning},
  VOLUME = {5},
}

@INPROCEEDINGS{Tan_2020_CVPR,
  AUTHOR = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  DATE = {2020-06},
  TITLE = {EfficientDet: Scalable and Efficient Object Detection},
}

@ARTICLE{Otter,
  AUTHOR = {Otter, Daniel W. and Medina, Julian R. and Kalita, Jugal K.},
  DATE = {2021},
  DOI = {10.1109/TNNLS.2020.2979670},
  JOURNALTITLE = {IEEE Transactions on Neural Networks and Learning Systems},
  NUMBER = {2},
  PAGES = {604--624},
  TITLE = {A Survey of the Usages of Deep Learning for Natural Language Processing},
  VOLUME = {32},
}

@ARTICLE{esteva2019guide,
  AUTHOR = {Esteva, Andre and Robicquet, Alexandre and Ramsundar, Bharath and Kuleshov, Volodymyr and DePristo, Mark and Chou, Katherine and Cui, Claire and Corrado, Greg and Thrun, Sebastian and Dean, Jeff},
  PUBLISHER = {Nature Publishing Group US New York},
  DATE = {2019},
  JOURNALTITLE = {Nature medicine},
  NUMBER = {1},
  PAGES = {24--29},
  TITLE = {A guide to deep learning in healthcare},
  VOLUME = {25},
}

@ARTICLE{soori2023artificial,
  AUTHOR = {Soori, Mohsen and Arezoo, Behrooz and Dastres, Roza},
  PUBLISHER = {Elsevier},
  DATE = {2023},
  JOURNALTITLE = {Cognitive Robotics},
  TITLE = {Artificial intelligence, machine learning and deep learning in advanced robotics, A review},
}

@ARTICLE{hernandez2019systematic,
  AUTHOR = {Hernández-Blanco, Antonio and Herrera-Flores, Boris and Tomás, David and Navarro-Colorado, Borja and others},
  PUBLISHER = {Hindawi},
  DATE = {2019},
  JOURNALTITLE = {Complexity},
  TITLE = {A systematic review of deep learning approaches to educational data mining},
  VOLUME = {2019},
}

@ARTICLE{he2020developing,
  AUTHOR = {He, Yang and Nazir, Shah and Nie, Baisheng and Khan, Sulaiman and Zhang, Jianhui},
  PUBLISHER = {Hindawi Limited},
  DATE = {2020},
  JOURNALTITLE = {Complexity},
  PAGES = {1--6},
  TITLE = {Developing an efficient deep learning-based trusted model for pervasive computing using an LSTM-based classification model},
  VOLUME = {2020},
}

@ARTICLE{weiser1991computer,
  AUTHOR = {Weiser, Mark},
  PUBLISHER = {JSTOR},
  DATE = {1991},
  JOURNALTITLE = {Scientific american},
  NUMBER = {3},
  PAGES = {94--105},
  TITLE = {The Computer for the 21 st Century},
  VOLUME = {265},
}

@ARTICLE{ige2022survey,
  AUTHOR = {Ige, Ayokunle Olalekan and Noor, Mohd Halim Mohd},
  PUBLISHER = {Elsevier},
  DATE = {2022},
  JOURNALTITLE = {Applied Soft Computing},
  PAGES = {109363},
  TITLE = {A survey on unsupervised learning for wearable sensor-based activity recognition},
}

@INPROCEEDINGS{ragab2020random,
  AUTHOR = {Ragab, Mohammed G and Abdulkadir, Said Jadid and Aziz, Norshakirah},
  ORGANIZATION = {IEEE},
  BOOKTITLE = {2020 International Conference on Computational Intelligence (ICCI)},
  DATE = {2020},
  PAGES = {86--91},
  TITLE = {Random search one dimensional CNN for human activity recognition},
}

@ARTICLE{zhang2019comprehensive,
  AUTHOR = {Zhang, Hong-Bo and Zhang, Yi-Xiang and Zhong, Bineng and Lei, Qing and Yang, Lijie and Du, Ji-Xiang and Chen, Duan-Sheng},
  PUBLISHER = {MDPI},
  DATE = {2019},
  JOURNALTITLE = {Sensors},
  NUMBER = {5},
  PAGES = {1005},
  TITLE = {A comprehensive survey of vision-based human action recognition methods},
  VOLUME = {19},
}

@ARTICLE{dang2020sensor,
  AUTHOR = {Dang, L Minh and Min, Kyungbok and Wang, Hanxiang and Piran, Md Jalil and Lee, Cheol Hee and Moon, Hyeonjoon},
  PUBLISHER = {Elsevier},
  DATE = {2020},
  JOURNALTITLE = {Pattern Recognition},
  PAGES = {107561},
  TITLE = {Sensor-based and vision-based human activity recognition: A comprehensive survey},
  VOLUME = {108},
}

@ARTICLE{singh2017stock,
  AUTHOR = {Singh, Ritika and Srivastava, Shashi},
  PUBLISHER = {Springer},
  DATE = {2017},
  JOURNALTITLE = {Multimedia Tools and Applications},
  PAGES = {18569--18584},
  TITLE = {Stock prediction using deep learning},
  VOLUME = {76},
}

@ARTICLE{zhang2021hoba,
  AUTHOR = {Zhang, Xinwei and Han, Yaoci and Xu, Wei and Wang, Qili},
  PUBLISHER = {Elsevier},
  DATE = {2021},
  JOURNALTITLE = {Information Sciences},
  PAGES = {302--316},
  TITLE = {HOBA: A novel feature engineering methodology for credit card fraud detection with a deep learning architecture},
  VOLUME = {557},
}

@INPROCEEDINGS{lei2020deep,
  AUTHOR = {Lei, Ying and Peng, Qinke and Shen, Yiqing},
  BOOKTITLE = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
  DATE = {2020},
  PAGES = {51--57},
  TITLE = {Deep learning for algorithmic trading: enhancing MACD strategy},
}

@ARTICLE{gao2021danhar,
  AUTHOR = {Gao, Wenbin and Zhang, Lei and Teng, Qi and He, Jun and Wu, Hao},
  PUBLISHER = {Elsevier},
  DATE = {2021},
  JOURNALTITLE = {Applied Soft Computing},
  PAGES = {107728},
  TITLE = {DanHAR: Dual attention network for multimodal human activity recognition using wearable sensors},
  VOLUME = {111},
}

@ARTICLE{gupta2021deep,
  AUTHOR = {Gupta, Saurabh},
  PUBLISHER = {Elsevier},
  DATE = {2021},
  JOURNALTITLE = {International Journal of Information Management Data Insights},
  NUMBER = {2},
  PAGES = {100046},
  TITLE = {Deep learning based human activity recognition (HAR) using wearable sensor data},
  VOLUME = {1},
}

@ARTICLE{erdacs2021human,
  AUTHOR = {Erdas, Cagatay Berke and Guney, Selda},
  PUBLISHER = {Springer},
  DATE = {2021},
  JOURNALTITLE = {Neural Processing Letters},
  PAGES = {1795--1809},
  TITLE = {Human activity recognition by using different deep learning approaches for wearable sensors},
  VOLUME = {53},
}

@INPROCEEDINGS{banjarey2022human,
  AUTHOR = {Banjarey, Khushboo and Sahu, Satya Prakash and Dewangan, Deepak Kumar},
  ORGANIZATION = {Springer},
  BOOKTITLE = {Sentimental Analysis and Deep Learning: Proceedings of ICSADL 2021},
  DATE = {2022},
  PAGES = {691--702},
  TITLE = {Human activity recognition using 1D convolutional neural network},
}

@INPROCEEDINGS{shuvo2020hybrid,
  AUTHOR = {Shuvo, Md Maruf Hossain and Ahmed, Nafis and Nouduri, Koundinya and Palaniappan, Kannappan},
  ORGANIZATION = {IEEE},
  BOOKTITLE = {2020 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)},
  DATE = {2020},
  PAGES = {1--5},
  TITLE = {A hybrid approach for human activity recognition with support vector machine and 1D convolutional neural network},
}

@ARTICLE{han2022human,
  AUTHOR = {Han, Chaolei and Zhang, Lei and Tang, Yin and Huang, Wenbo and Min, Fuhong and He, Jun},
  PUBLISHER = {Elsevier},
  DATE = {2022},
  JOURNALTITLE = {Expert Systems with Applications},
  PAGES = {116764},
  TITLE = {Human activity recognition using wearable sensors by heterogeneous convolutional neural networks},
  VOLUME = {198},
}

@ARTICLE{ige2023wsense,
  AUTHOR = {Ige, Ayokunle Olalekan and Noor, Mohd Halim Mohd},
  DATE = {2023},
  JOURNALTITLE = {arXiv preprint arXiv:2303.17845},
  TITLE = {WSense: A Robust Feature Learning Module for Lightweight Human Activity Recognition},
}



### HAR begin

@article{baraka_deep_2024,
	title = {Deep similarity segmentation model for sensor-based activity recognition},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-024-18933-2},
	doi = {10.1007/s11042-024-18933-2},
	abstract = {Signal segmentation is a critical stage in activity recognition. Most existing studies adopted the fixed-size sliding window method for this stage. However, the fixed-size sliding window may not produce the most effective segmentation method since human activities have variable length durations, particularly transitional activities. In this paper, we propose a novel deep similarity segmentation model that overcomes not only the limitations of the fixed sliding window method but also the weaknesses of threshold-based segmentation methods. Specifically, a novel deep learning model is designed to distinguish between transitional and basic activity by treating the segmentation task as a binary classification task. The proposed model accepts multiple sequence windows and extracts the local features automatically for each window using convolutional neural networks. The temporal features of windows are extracted by measuring the similarity and differentiation between the local features of adjacent windows. The local features are combined with the temporal features and passed to deep fully connected layers to distinguish the transitional activity from the basic activity windows. The evaluation relies on two public datasets, SBHARPT and FORTH-TRACE. According to the experimental findings, the proposed approach can distinguish between basic and transitional activities with an accuracy of 98.51\% and 98.41\%, respectively. Additionally, our method outperformed the fixed sliding window for activity recognition by 2.93\% and 2.24\% for both datasets, respectively, achieving an accuracy of 93.35\% and 84.96\%. These results are significant and outperform the precision of cutting-edge models.},
	language = {en},
	urldate = {2025-03-04},
	journal = {Multimedia Tools and Applications},
	author = {Baraka, AbdulRahman and Mohd Noor, Mohd Halim},
	month = may,
	year = {2024},
	keywords = {Deep learning, Signal segmentation, Transitional activity},
}

@article{baraka_similarity_2023,
	title = {Similarity {Segmentation} {Approach} for {Sensor}-{Based} {Activity} {Recognition}},
	volume = {23},
	issn = {1558-1748},
	url = {https://ieeexplore.ieee.org/document/10188611},
	doi = {10.1109/JSEN.2023.3295778},
	abstract = {The fixed sliding window is the commonly used technique for signal segmentation in human activity recognition (HAR). However, the fixed sliding window may not produce optimal segmentation because human activities have varying durations, especially for transitional activities (TAs). This is because a large window size may contain activity signals belonging to different activities, and a small window size may split the activity signal into multiple windows. Furthermore, the fixed sliding window does not consider the relationship between adjacent windows, which may affect the performance of the HAR model. In this study, we propose a similarity segmentation approach (SSA) that exploits the temporal structure of the activity signal within the window segmentation process. Specifically, the proposed approach segments each window into subwindows and extracts the inner features by measuring the similarity between them. The inner features are used to measure the dissimilarity between the adjacent windows. The proposed approach is able to distinguish between transitional and nontransitional windows, which achieves more effective segmentation and classification processes. Two public datasets are used for the evaluation. The experimental results show that the proposed approach can distinguish TAs from basic activities (BAs) at 97.65\% accuracy, which enhanced the accuracy of TAs recognition compared to the fixed sliding window by 33.41\%. Also, our approach achieved accuracy for activity recognition of 92.71\% and 86.65\% for both datasets, respectively, which exceeds the fixed sliding window by 2.29\% and 3.93\% for both datasets, respectively. These results are significant and exceed the accuracy of the state-of-the-art models.},
	number = {17},
	urldate = {2025-03-04},
	journal = {IEEE Sensors Journal},
	author = {Baraka, AbdulRahman M. A. and Mohd Noor, Mohd Halim},
	month = sep,
	year = {2023},
	note = {Conference Name: IEEE Sensors Journal},
	keywords = {Classification algorithms, Data models, Feature extraction, Human activity recognition, Human activity recognition (HAR), Motion segmentation, Sensors, signal segmentation, Task analysis, transitional activity (TA)},
	pages = {19704--19716},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\52DSG79L\\10188611.html:text/html},
}

@article{luwe_wearable_2022,
	title = {Wearable {Sensor}-{Based} {Human} {Activity} {Recognition} with {Hybrid} {Deep} {Learning} {Model}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-9709},
	url = {https://www.mdpi.com/2227-9709/9/3/56},
	doi = {10.3390/informatics9030056},
	abstract = {It is undeniable that mobile devices have become an inseparable part of human’s daily routines due to the persistent growth of high-quality sensor devices, powerful computational resources and massive storage capacity nowadays. Similarly, the fast development of Internet of Things technology has motivated people into the research and wide applications of sensors, such as the human activity recognition system. This results in substantial existing works that have utilized wearable sensors to identify human activities with a variety of techniques. In this paper, a hybrid deep learning model that amalgamates a one-dimensional Convolutional Neural Network with a bidirectional long short-term memory (1D-CNN-BiLSTM) model is proposed for wearable sensor-based human activity recognition. The one-dimensional Convolutional Neural Network transforms the prominent information in the sensor time series data into high level representative features. Thereafter, the bidirectional long short-term memory encodes the long-range dependencies in the features by gating mechanisms. The performance evaluation reveals that the proposed 1D-CNN-BiLSTM outshines the existing methods with a recognition rate of 95.48\% on the UCI-HAR dataset, 94.17\% on the Motion Sense dataset and 100\% on the Single Accelerometer dataset.},
	language = {en},
	number = {3},
	urldate = {2025-02-27},
	journal = {Informatics},
	author = {Luwe, Yee Jia and Lee, Chin Poo and Lim, Kian Ming},
	month = sep,
	year = {2022},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {convolutional neural network, human activity recognition, long short-term memory, wearable sensor},
	pages = {56},
}

@article{shi_novel_2023,
	title = {Novel {Deep} {Learning} {Network} for {Gait} {Recognition} {Using} {Multimodal} {Inertial} {Sensors}},
	volume = {23},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/23/2/849},
	doi = {10.3390/s23020849},
	abstract = {Some recent studies use a convolutional neural network (CNN) or long short-term memory (LSTM) to extract gait features, but the methods based on the CNN and LSTM have a high loss rate of time-series and spatial information, respectively. Since gait has obvious time-series characteristics, while CNN only collects waveform characteristics, and only uses CNN for gait recognition, this leads to a certain lack of time-series characteristics. LSTM can collect time-series characteristics, but LSTM results in performance degradation when processing long sequences. However, using CNN can compress the length of feature vectors. In this paper, a sequential convolution LSTM network for gait recognition using multimodal wearable inertial sensors is proposed, which is called SConvLSTM. Based on 1D-CNN and a bidirectional LSTM network, the method can automatically extract features from the raw acceleration and gyroscope signals without a manual feature design. 1D-CNN is first used to extract the high-dimensional features of the inertial sensor signals. While retaining the time-series features of the data, the dimension of the features is expanded, and the length of the feature vectors is compressed. Then, the bidirectional LSTM network is used to extract the time-series features of the data. The proposed method uses fixed-length data frames as the input and does not require gait cycle detection, which avoids the impact of cycle detection errors on the recognition accuracy. We performed experiments on three public benchmark datasets: UCI-HAR, HuGaDB, and WISDM. The results show that SConvLSTM performs better than most of those reporting the best performance methods, at present, on the three datasets.},
	language = {en},
	number = {2},
	urldate = {2025-02-27},
	journal = {Sensors},
	author = {Shi, Ling-Feng and Liu, Zhong-Ye and Zhou, Ke-Jun and Shi, Yifan and Jing, Xiao},
	month = jan,
	year = {2023},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, bidirectional LSTM, convolutional neural network (CNN), gait recognition},
	pages = {849},
}

@article{dua_inception_2023,
	title = {Inception inspired {CNN}-{GRU} hybrid network for human activity recognition},
	volume = {82},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-021-11885-x},
	doi = {10.1007/s11042-021-11885-x},
	abstract = {Human Activity Recognition (HAR) involves the recognition of human activities using sensor data. Most of the techniques for HAR involve hand-crafted features and hence demand a good amount of human intervention. Moreover, the activity data obtained from sensors are highly imbalanced and hence demand a robust classifier design. In this paper, a novel classifier “ICGNet” is proposed for HAR, which is a hybrid of Convolutional Neural Network (CNN) and Gated Recurrent Unit (GRU). The CNN block used in the proposed network derives its inspiration from the famous Inception module. It uses multiple-sized convolutional filters simultaneously over the input and thus can capture the information in the data at multiple scales. These multi-sized filters introduced at the same level in the convolution network helps to compute more abstract features for local patches of data. It also makes use of 1 × 1 convolution to pool the input across channel dimension, and the intuition behind it is that it helps the model extract the valuable information hidden across the channels. The proposed ICGNet leverages the strengths of CNN and GRU and hence can capture local features and long-term dependencies in the multivariate time series data. It is an end-to-end model for HAR that can process raw data captured from wearable sensors without using any manual feature engineering. Integrating the adaptive user interfaces, the proposed HAR system can be applied to Human-Computer Interaction (HCI) fields such as interactive games, robot learning, health monitoring, and pattern-based surveillance. The overall accuracies achieved on two benchmark datasets viz. MHEALTH and PAMAP2 are 99.25\% and 97.64\%, respectively. The results indicate that the proposed network outperformed the similar architectures proposed for HAR in the literature.},
	language = {en},
	number = {4},
	urldate = {2025-02-27},
	journal = {Multimedia Tools and Applications},
	author = {Dua, Nidhi and Singh, Shiva Nand and Semwal, Vijay Bhaskar and Challa, Sravan Kumar},
	month = feb,
	year = {2023},
	keywords = {Convolutional neural network, Pattern recognition, Gated recurrent unit, HAR, Human-computer interaction, Inception, Wearable sensors},
	pages = {5369--5403},
}

@article{imran_smart-wearable_2024,
	title = {Smart-{Wearable} {Sensors} and {CNN}-{BiGRU} {Model}: {A} {Powerful} {Combination} for {Human} {Activity} {Recognition}},
	volume = {24},
	issn = {1558-1748},
	shorttitle = {Smart-{Wearable} {Sensors} and {CNN}-{BiGRU} {Model}},
	url = {https://ieeexplore.ieee.org/document/10348511},
	doi = {10.1109/JSEN.2023.3338264},
	abstract = {Human activity recognition (HAR) is a key component of ambient-assisted living and one of the most active areas of research in the Internet of Things (IoT). The use of wearable and embedded sensors in HAR overcomes the limitations of conventional approaches relying on machine vision and environmental sensors. We offer a novel, lightweight convolutional neural network–bidirectional gated recurrent unit (CNN-BiGRU) model that classifies human activities using the inertial sensor data collected with body-mounted smart-watches and smartphones. Unlike the traditional approaches, the presented model is trained on the magnitude of the 3-D acceleration ( {\textbackslash}widehat {\textbackslash}text mag\_a ), which significantly minimizes the input space 1-D. The deep learner has been validated using two different publicly available datasets from the wireless sensor data mining (WISDM) lab and different evaluation parameters, such as recall/sensitivity, precision, accuracy, and F1-scores, are computed. A comparison with the existing studies reveals that our proposed learner surpasses the existing methodologies. Using magnitude of 3-D acceleration ( {\textbackslash}widehat {\textbackslash}text mag\_a{\textasciicircum}w , 1-D input signal), we have achieved 97.29\% accuracy for all six activities of WISDM 2011 dataset and 98.81\% accuracy for 3-D acceleration ( a\_x{\textasciicircum}w,a\_y{\textasciicircum}w , and a\_z{\textasciicircum}w ). The precision, recall, and F1-score remained at 97\% for the 1-D case and 99\% for the 3-D case. When evaluated on the data of all 18 smartwatch-based activities in the WISDM 2019 dataset, we have achieved 97.5\% accuracy with the magnitude of 3-D acceleration ( {\textbackslash}widehat {\textbackslash}text mag\_a{\textasciicircum}w ) and 98.4\% accuracy for 6-D acceleration and angular velocities ( a\_x{\textasciicircum}w,a\_y{\textasciicircum}w,a\_z{\textasciicircum}w,ømega \_x{\textasciicircum}w,ømega \_y{\textasciicircum}w , and ømega \_z{\textasciicircum}w ). The precision, recall, and F1-score remained at 98\% in both cases.},
	number = {2},
	urldate = {2025-02-27},
	journal = {IEEE Sensors Journal},
	author = {Imran, Hamza Ali and Riaz, Qaiser and Hussain, Mehdi and Tahir, Hasan and Arshad, Razi},
	month = jan,
	year = {2024},
	note = {Conference Name: IEEE Sensors Journal},
	keywords = {Training, Convolutional neural networks, Wearable sensors, Deep models for activity recognition, gait analysis, Human activity recognition, human activity recognition (HAR), inertial sensors, Inertial sensors, Sensors, signal processing, Three-dimensional displays, wearable sensors},
	pages = {1963--1974},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\ELR9YB6H\\10348511.html:text/html},
}

@article{nafea_sensor-based_2021,
	title = {Sensor-{Based} {Human} {Activity} {Recognition} with {Spatio}-{Temporal} {Deep} {Learning}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/6/2141},
	doi = {10.3390/s21062141},
	abstract = {Human activity recognition (HAR) remains a challenging yet crucial problem to address in computer vision. HAR is primarily intended to be used with other technologies, such as the Internet of Things, to assist in healthcare and eldercare. With the development of deep learning, automatic high-level feature extraction has become a possibility and has been used to optimize HAR performance. Furthermore, deep-learning techniques have been applied in various fields for sensor-based HAR. This study introduces a new methodology using convolution neural networks (CNN) with varying kernel dimensions along with bi-directional long short-term memory (BiLSTM) to capture features at various resolutions. The novelty of this research lies in the effective selection of the optimal video representation and in the effective extraction of spatial and temporal features from sensor data using traditional CNN and BiLSTM. Wireless sensor data mining (WISDM) and UCI datasets are used for this proposed methodology in which data are collected through diverse methods, including accelerometers, sensors, and gyroscopes. The results indicate that the proposed scheme is efficient in improving HAR. It was thus found that unlike other available methods, the proposed method improved accuracy, attaining a higher score in the WISDM dataset compared to the UCI dataset (98.53\% vs. 97.05\%).},
	language = {en},
	number = {6},
	urldate = {2025-03-04},
	journal = {Sensors},
	author = {Nafea, Ohoud and Abdul, Wadood and Muhammad, Ghulam and Alsulaiman, Mansour},
	month = jan,
	year = {2021},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Bi-directional LSTM, convolution neural networks, deep learning, human activity recognition, local spatio-temporal features},
	pages = {2141},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\JRHBSXNH\\Nafea et al. - 2021 - Sensor-Based Human Activity Recognition with Spati.pdf:application/pdf},
}

@article{khan_attention_2021,
	title = {Attention induced multi-head convolutional neural network for human activity recognition},
	volume = {110},
	issn = {1568-4946},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494621005925},
	doi = {10.1016/j.asoc.2021.107671},
	abstract = {Deep neural networks, including convolutional neural networks (CNNs), have been widely adopted for human activity recognition in recent years. They have attained significant performance improvement over traditional techniques due to their strong feature representation capabilities. Some of the challenges faced by the HAR community is the non-availability of a substantial amount of labeled training samples, and the higher computational cost and system resources requirements of deep learning architectures as opposed to shallow learning algorithms. To address these challenges, we propose an attention-based multi-head model for human activity recognition (HAR). This framework contains three lightweight convolutional heads, with each head designed using one-dimensional CNN to extract features from sensory data. The lightweight multi-head model is induced with attention to strengthen the representation ability of CNN, allowing for automatic selection of salient features and suppress unimportant ones. We conducted ablation studies and experiments on two publicly available benchmark datasets: WISDM and UCI HAR, to evaluate our model. The experimental outcome demonstrates the effectiveness of the proposed framework in activity recognition and achieves better accuracy while ensuring computational efficiency.},
	urldate = {2025-02-27},
	journal = {Applied Soft Computing},
	author = {Khan, Zanobya N. and Ahmad, Jamil},
	month = oct,
	year = {2021},
	keywords = {Convolutional neural network, Attention mechanism, Human activity recognition, Inertial sensors, Squeeze-and-excitation module},
	pages = {107671},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\6RJBEJEE\\S1568494621005925.html:text/html},
}

@inproceedings{deep_hybrid_2019,
	title = {Hybrid {Model} {Featuring} {CNN} and {LSTM} {Architecture} for {Human} {Activity} {Recognition} on {Smartphone} {Sensor} {Data}},
	url = {https://ieeexplore.ieee.org/document/9029136},
	doi = {10.1109/PDCAT46702.2019.00055},
	abstract = {The traditional methods of recognizing human activities involve typical machine learning (ML) algorithms which uses heuristic engineered features. Human activities are dynamic in nature and are encoded with a sequence of actions. ML methods are able to perform activity recognition tasks but may not exploit the temporal correlations of the input data. Therefore, in this paper, we proposed and showed the effectiveness of employing a new combination of deep learning (DL) methods for human activity recognition (HAR). DL methods are capable of extracting discriminative features automatically from the raw sensor data. Specifically, in this paper, we proposed a hybrid architecture which features a combination of Convolutional neural networks (CNN) and Long short-term Memory (LSTM) networks for HAR task. The model is tested on UCI HAR dataset which is a benchmark dataset and comprises of accelerometer and gyroscope data obtained from a smartphone. Our experimental results showed that our proposed method outperformed the recent results which used pure LSTM and bidirectional LSTM networks on the same dataset.},
	urldate = {2025-03-04},
	booktitle = {2019 20th {International} {Conference} on {Parallel} and {Distributed} {Computing}, {Applications} and {Technologies} ({PDCAT})},
	author = {Deep, Samundra and Zheng, Xi},
	month = dec,
	year = {2019},
	note = {ISSN: 2640-6721},
	keywords = {Activity recognition, CNN, Computational modeling, Computer architecture, Data models, deep learning, Feature extraction, Hidden Markov models, Hybrid model, LSTM, Machine learning, neural network},
	pages = {259--264},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\W7E2NLAZ\\9029136.html:text/html},
}

@article{ige_deep_2023,
	title = {A deep local-temporal architecture with attention for lightweight human activity recognition},
	volume = {149},
	issn = {1568-4946},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494623009729},
	doi = {10.1016/j.asoc.2023.110954},
	abstract = {Human Activity Recognition (HAR) is an essential area of pervasive computing deployed in numerous fields. In order to seamlessly capture human activities, various inertial sensors embedded in wearable devices have been used to generate enormous amounts of signals, which are multidimensional time series of state changes. Therefore, the signals must be divided into windows for feature extraction. Deep learning (DL) methods have recently been used to automatically extract local and temporal features from signals obtained using wearable sensors. Likewise, multiple input deep learning architectures have been proposed to improve the quality of learned features in wearable sensor HAR. However, these architectures are often designed to extract local and temporal features on a single pipeline, which affects feature representation quality. Also, such models are always parameter-heavy due to the number of weights involved in the architecture. Since resources (CPU, battery, and memory) of end devices are limited, it is crucial to propose lightweight deep architectures for easy deployment of activity recognition models on end devices. To contribute, this paper presents a new deep parallel architecture named DLT, based on pipeline concatenation. Each pipeline consists of two sub-pipelines, where the first sub-pipeline learns local features in the current window using 1D-CNN, and the second sub-pipeline learns temporal features using Bi-LSTM and LSTMs before concatenating the feature maps and integrating channel attention. By doing this, the proposed DLT model fully harnessed the capabilities of CNN and RNN equally in capturing more discriminative features from wearable sensor signals while increasing responsiveness to essential features. Also, the size of the model is reduced by adding a lightweight module to the top of the architecture, thereby ensuring the proposed DLT architecture is lightweight. Experiments on two publicly available datasets showed that the proposed architecture achieved an accuracy of 98.52\% on PAMAP2 and 97.90\% on WISDM datasets, outperforming existing models with few model parameters.},
	urldate = {2025-03-04},
	journal = {Applied Soft Computing},
	author = {Ige, Ayokunle Olalekan and Mohd Noor, Mohd Halim},
	month = dec,
	year = {2023},
	keywords = {Deep learning, Lightweight, Local features, Temporal features, Wearable sensors},
	pages = {110954},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\6XWWMKET\\S1568494623009729.html:text/html},
}

@article{gao_danhar_2021,
	title = {{DanHAR}: {Dual} {Attention} {Network} for multimodal human activity recognition using wearable sensors},
	volume = {111},
	issn = {1568-4946},
	shorttitle = {{DanHAR}},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494621006499},
	doi = {10.1016/j.asoc.2021.107728},
	abstract = {In the paper, we present a new dual attention method called DanHAR, which blends channel and temporal attention on residual networks to improve feature representation ability for sensor-based HAR task. Specially, the channel attention plays a key role in deciding what to focus, i.e., sensor modalities, while the temporal attention can focus on the target activity from a long sensor sequence to tell where to focus. Extensive experiments are conducted on four public HAR datasets, as well as weakly labeled HAR dataset. The results show that dual attention mechanism is of central importance for many activity recognition tasks. We obtain 2.02\%, 4.20\%, 1.95\%, 5.22\% and 5.00\% relative improvement over regular ConvNets respectively on WISDM dataset, UNIMIB SHAR dataset, PAMAP2 dataset, OPPORTUNITY dataset, as well as weakly labeled HAR dataset. The DanHAR is able to surpass other state-of-the-art algorithms at negligible computational overhead. Visualizing analysis is conducted to show that the proposed attention can capture the spatial–temporal dependencies of multimodal sensing data, which amplifies the more important sensor modalities and timesteps during classification. The results are in good agreement with normal human intuition.},
	urldate = {2025-03-08},
	journal = {Applied Soft Computing},
	author = {Gao, Wenbin and Zhang, Lei and Teng, Qi and He, Jun and Wu, Hao},
	month = nov,
	year = {2021},
	keywords = {Channel attention, Convolutional neural networks, Human activity recognition, Multimodal sensors, Residual network},
	pages = {107728},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\PZGLFWIW\\S1568494621006499.html:text/html;Submitted Version:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\7ZV6AA28\\Gao et al. - 2021 - DanHAR Dual Attention Network for multimodal huma.pdf:application/pdf},
}

@article{agac_resource-efficient_2024,
	title = {Resource-efficient, sensor-based human activity recognition with lightweight deep models boosted with attention},
	volume = {117},
	issn = {0045-7906},
	url = {https://www.sciencedirect.com/science/article/pii/S0045790624002027},
	doi = {10.1016/j.compeleceng.2024.109274},
	abstract = {With their automatic feature extraction capabilities, deep learning models have become more widespread in sensor-based human activity recognition, particularly on larger datasets. However, their direct use on mobile and wearable devices is challenging due to the extensive resource requirements. Concurrently, attention-based models are emerging to improve recognition performance by dynamically emphasizing relevant parts of features and disregarding the irrelevant ones, particularly in the computer vision domain. This study introduces a novel application of attention mechanisms to smaller deep architectures, investigating whether smaller models can achieve comparable recognition performance to larger models in sensor-based human activity recognition systems while keeping resource usage at lower levels. For this purpose, we integrate the convolutional block attention module into a hybrid model, deep convolutional and long short-term memory network. Experiments are conducted using five public datasets in three model sizes: lightweight, moderate and original. The results show that applying attention to the lightweight model enables achieving similar recognition performances to the moderate-size model, and the lightweight model requires approximately 2–13 times fewer parameters and 3.5 times fewer flops. We also conduct experiments with sensor data at lower sampling rates and from fewer sensors attached to different body parts. The results show that attention improves recognition performance under lower sampling rates, as well as under higher sampling rates when model sizes are smaller, and mitigates the impact of missing data from one or more body parts, making the model more suitable for real-world sensor-based applications.},
	urldate = {2025-03-08},
	journal = {Computers and Electrical Engineering},
	author = {Agac, Sumeyye and Incel, Ozlem Durmaz},
	month = jul,
	year = {2024},
	keywords = {Attention mechanism, Convolutional neural networks, Human activity recognition, Hybrid deep models, Motion sensors, Resource consumption},
	pages = {109274},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\ENG34H4C\\S0045790624002027.html:text/html},
}

@article{tang_triple_2022,
	title = {Triple {Cross}-{Domain} {Attention} on {Human} {Activity} {Recognition} {Using} {Wearable} {Sensors}},
	volume = {6},
	issn = {2471-285X},
	url = {https://ieeexplore.ieee.org/document/9669910},
	doi = {10.1109/TETCI.2021.3136642},
	abstract = {Efficiently identifying activities of daily living (ADL) provides very important contextual information that is able to improve the effectiveness of various sports tracking and healthcare applications. Recently, attention mechanism that selectively focuses on time series signals has been widely adopted in sensor based human activity recognition (HAR), which can enhance interesting target activity and ignore irrelevant background activity. Several attention mechanisms have been investigated, which achieve remarkable performance in HAR scenario. Despite their success, prior these attention methods ignore the cross-interaction between different dimensions. In the paper, in order to avoid above shortcoming, we present a triplet cross-dimension attention for sensor-based activity recognition task, where three attention branches are built to capture the cross-interaction between sensor dimension, temporal dimension and channel dimension. The effectiveness of triplet attention method is validated through extensive experiments on four public HAR dataset namely UCI-HAR, PAMAP2, WISDM and UNIMIB-SHAR as well as the weakly labeled HAR dataset. Extensive experiments show consistent improvements in classification performance with various backbone models such as plain CNN and ResNet, demonstrating a good generality ability of the triplet attention. Visualization analysis is provided to support our conclusion, and actual implementation is evaluated on a Raspberry Pi platform.},
	number = {5},
	urldate = {2025-03-09},
	journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
	author = {Tang, Yin and Zhang, Lei and Teng, Qi and Min, Fuhong and Song, Aiguo},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Emerging Topics in Computational Intelligence},
	keywords = {Activity recognition, attention, Computer vision, convolutional neural networks, Feature extraction, Shape, Standards, Task analysis, Tensors, Training, weakly supervised learning, wearable sensors},
	pages = {1167--1176},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\PPMRKSSP\\9669910.html:text/html},
}

@misc{misra_rotate_2020,
	title = {Rotate to {Attend}: {Convolutional} {Triplet} {Attention} {Module}},
	shorttitle = {Rotate to {Attend}},
	url = {http://arxiv.org/abs/2010.03045},
	doi = {10.48550/arXiv.2010.03045},
	abstract = {Benefiting from the capability of building inter-dependencies among channels or spatial locations, attention mechanisms have been extensively studied and broadly used in a variety of computer vision tasks recently. In this paper, we investigate light-weight but effective attention mechanisms and present triplet attention, a novel method for computing attention weights by capturing cross-dimension interaction using a three-branch structure. For an input tensor, triplet attention builds inter-dimensional dependencies by the rotation operation followed by residual transformations and encodes inter-channel and spatial information with negligible computational overhead. Our method is simple as well as efficient and can be easily plugged into classic backbone networks as an add-on module. We demonstrate the effectiveness of our method on various challenging tasks including image classification on ImageNet-1k and object detection on MSCOCO and PASCAL VOC datasets. Furthermore, we provide extensive in-sight into the performance of triplet attention by visually inspecting the GradCAM and GradCAM++ results. The empirical evaluation of our method supports our intuition on the importance of capturing dependencies across dimensions when computing attention weights. Code for this paper can be publicly accessed at https://github.com/LandskapeAI/triplet-attention},
	urldate = {2025-03-09},
	publisher = {arXiv},
	author = {Misra, Diganta and Nalamada, Trikay and Arasanipalai, Ajay Uppili and Hou, Qibin},
	month = nov,
	year = {2020},
	note = {arXiv:2010.03045 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\A6UYPQA5\\Misra et al. - 2020 - Rotate to Attend Convolutional Triplet Attention .pdf:application/pdf},
}

@article{chen_transformer_2022,
	title = {Transformer {With} {Bidirectional} {GRU} for {Nonintrusive}, {Sensor}-{Based} {Activity} {Recognition} in a {Multiresident} {Environment}},
	volume = {9},
	issn = {2327-4662},
	url = {https://ieeexplore.ieee.org/document/9826881},
	doi = {10.1109/JIOT.2022.3190307},
	abstract = {Several techniques for human activity recognition (HAR) in a smart indoor environment have been developed and improved along with the rapid advancement of sensor technologies. However, recognizing multiple people’s activities is still challenging due to the complexity of their activities, such as parallel and collaborative activities. To address these challenges, we propose a transformer with a bidirectional gated recurrent unit (GRU) deep learning (DL) method, called TRANS-BiGRU, to efficiently learn and recognize different types of activities performed by multiple residents. We compare the proposed model with the state-of-the-art models and various DL models, such as Ensemble2LSTM (Ens2-LSTM), bidirectional GRUs (Bi-GRU), and traditional machine learning (ML) models, such as support vector machine (SVM). Our experimental results based on the center for advanced studies in adaptive system and ARAS public data sets show that our model significantly outperforms the existing models for complex activity recognition of multiple residents.},
	number = {23},
	urldate = {2025-02-27},
	journal = {IEEE Internet of Things Journal},
	author = {Chen, Dong and Yongchareon, Sira and Lai, Edmund M.-K. and Yu, Jian and Sheng, Quan Z. and Li, Yafeng},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Deep learning, Activity recognition, Transformers, Hidden Markov models, Smart homes, human activity recognition (HAR), Bidirectional GRU (Bi-GRU), Collaboration, Motion detection, multiresident activity recognition, transformer architecture},
	pages = {23716--23727},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\WKKPTMQP\\9826881.html:text/html},
}

@article{sun_efficient_2024,
	title = {Efficient human activity recognition: {A} deep convolutional transformer-based contrastive self-supervised approach using wearable sensors},
	volume = {135},
	issn = {0952-1976},
	shorttitle = {Efficient human activity recognition},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197624008637},
	doi = {10.1016/j.engappai.2024.108705},
	abstract = {Artificial intelligence has advanced the applications of sensor-based human motion capture and recognition technology in various engineering fields, such as human–robot collaboration and health monitoring. Deep learning methods can achieve satisfactory recognition results when provided with sufficient labeled data. However, labeling a large dataset is expensive and time-consuming in practical applications. To address this issue, this paper proposes a deep convolutional transformer-based contrastive self-supervised (DCTCSS) model under the bootstrap your own latent (BYOL) framework. The DCTCSS model aims to achieve reliable activity recognition using only a small amount of labeled data. Firstly, a deep convolutional transformer (DCT) model is proposed as the backbone of DCTCSS model, to learn high-level feature representations from unlabeled data in pre-training period. Subsequently, a simple linear classifier is trained with supervised fine-tuning using a limited amount of labeled data to recognize activities. In addition, this paper experimentally formulates a random data augmentation strategy to increase the diversity of input data. The performance of the DCTCSS model is evaluated and compared with several state-of-the-art algorithms on three datasets widely used in daily life, medical monitoring, and intelligent manufacturing. Experimental results show that the DCTCSS model achieves mean F1 scores of 95.64\%, 88.39\%, and 98.40\% on the UCI-HAR, Skoda, and Mhealth datasets, respectively, using only 10\% of the labeled data. These results outperform both supervised and unsupervised baseline models. Consequently, the DCTCSS model demonstrates its effectiveness in reducing the dependence on large amounts of labeled data while still achieving competitive activity recognition performance.},
	urldate = {2025-03-04},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Sun, Yujie and Xu, Xiaolong and Tian, Xincheng and Zhou, Lelai and Li, Yibin},
	month = sep,
	year = {2024},
	keywords = {Contrastive self-supervised learning, Deep convolutional transformer, Human activity recognition, Wearable sensors},
	pages = {108705},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\BQZIDDEF\\S0952197624008637.html:text/html},
}

@inproceedings{lattanzi_are_2025,
	address = {New York, NY, USA},
	series = {{ICAAI} '24},
	title = {Are {Transformers} a {Useful} {Tool} for {Tiny} devices in {Human} {Activity} {Recognition}?},
	isbn = {9798400718014},
	url = {https://dl.acm.org/doi/10.1145/3704137.3704171},
	doi = {10.1145/3704137.3704171},
	abstract = {Human Activity Recognition using tiny wearable devices presents unique challenges due to limited computational resources and battery life. Transformers have recently emerged as the most effective tools for time-series classification. This is due to their capacity to capture long-range dependencies and complex temporal dynamics, combined with relatively lower computational complexity, which allows them to surpass the performance of more traditional systems such as convolutional or recurrent neural networks. In fact, transformers do not process data sequentially. Instead, they leverage an attention mechanism that allows them to weigh the importance of different parts of the input data differently. This study experimentally investigates the applicability of transformer models on tiny devices by porting them on a low-power ESP32 device. Comprehensive evaluations are conducted on several benchmark datasets, demonstrating that transformer-based models are not viable solutions for tiny devices compared to convolutional and recurrent neural networks, with respect to which it achieves up to 14\% lower accuracy depending on the dataset used.},
	urldate = {2025-03-03},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Advances} in {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Lattanzi, Emanuele and Calisti, Lorenzo and Contoli, Chiara},
	month = mar,
	year = {2025},
	pages = {339--344},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\MPR7RQWW\\Lattanzi et al. - 2025 - Are Transformers a Useful Tool for Tiny devices in.pdf:application/pdf},
}

@article{chan_unified_2021,
	title = {A unified generative model using generative adversarial network for activity recognition},
	volume = {12},
	issn = {1868-5145},
	url = {https://doi.org/10.1007/s12652-020-02548-0},
	doi = {10.1007/s12652-020-02548-0},
	abstract = {The recent advancement of deep learning methods has seen a significant increase in recognition accuracy in many important applications such as human activity recognition. However, deep learning methods require a vast amount of sensor data to automatically extract the most salient features for activity classification. Therefore, in this paper, a unified generative model is proposed to generate verisimilar data of different activities for activity recognition. The proposed generative model not only able to generate data that have a similar pattern, but also data with diverse characteristics. This allows for data augmentation in activity classification to improve the overall recognition accuracy. Three similarity measures are proposed to assess the quality of the synthetic data in addition to two visual evaluation methods. The proposed generative model was evaluated on a public dataset. The training data was prepared by systematically varying the combination of original and synthetic data. Results have shown that classification using the hybrid training data achieved a comparable recognition accuracy with the classification using the original training data. The performance of the classifiers maintained at the recognition accuracy of 85\%.},
	language = {en},
	number = {7},
	urldate = {2025-03-04},
	journal = {Journal of Ambient Intelligence and Humanized Computing},
	author = {Chan, Mang Hong and Noor, Mohd Halim Mohd},
	month = jul,
	year = {2021},
	keywords = {Activity recognition, Artificial Intelligence, Data augmentation, Data generation, Generative adversarial network},
	pages = {8119--8128},
}

@article{jimale_fully_2022,
	title = {Fully {Connected} {Generative} {Adversarial} {Network} for {Human} {Activity} {Recognition}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9893100},
	doi = {10.1109/ACCESS.2022.3206952},
	abstract = {Conditional Generative Adversarial Networks (CGAN) have shown great promise in generating synthetic data for sensor-based activity recognition. However, one key issue concerning existing CGAN is the design of the network architecture that affects sample quality. This study proposes an effective CGAN architecture that synthesizes higher quality samples than state-of-the-art CGAN architectures. This is achieved by combining convolutional layers with multiple fully connected networks in the generator’s input and discriminator’s output of the CGAN. We show the effectiveness of the proposed approach using elderly data for sensor-based activity recognition. Visual evaluation, similarity measure, and usability evaluation are used to assess the quality of generated samples by the proposed approach and validate its performance in activity recognition. In comparison to the state-of-the-art CGAN, the visual evaluation and similarity measure demonstrate that the proposed models’ synthetic data more accurately represents actual data and creates more variations in each synthetic data than the state-of-the-art approach respectively. The experimental stages of the usability evaluation, on the other hand, show a performance gain of 2.5\%, 2.5\%, 3.1\%, and 4.4\% over the state-of-the-art CGAN when using synthetic samples by the proposed architecture.},
	urldate = {2025-03-04},
	journal = {IEEE Access},
	author = {Jimale, Ali Olow and Mohd Noor, Mohd Halim},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Activity recognition, Data models, deep learning, Deep learning, Feature extraction, generative adversarial network, Generative adversarial networks, Generators, Sensors, Training data},
	pages = {100257--100266},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\PNKGB85N\\9893100.html:text/html},
}

@article{lupion_data_2024,
	title = {Data {Augmentation} for {Human} {Activity} {Recognition} {With} {Generative} {Adversarial} {Networks}},
	volume = {28},
	issn = {2168-2208},
	url = {https://ieeexplore.ieee.org/document/10432776},
	doi = {10.1109/JBHI.2024.3364910},
	abstract = {Currently, Human Activity Recognition (HAR) applications need a large volume of data to be able to generalize to new users and environments. However, the availability of labeled data is usually limited and the process of recording new data is costly and time-consuming. Synthetically increasing datasets using Generative Adversarial Networks (GANs) has been proposed, outperforming cropping, time-warping, and jittering techniques on raw signals. Incorporating GAN-generated synthetic data into datasets has been demonstrated to improve the accuracy of trained models. Regardless, currently, there is no optimal GAN architecture to generate accelerometry signals, neither a proper evaluation methodology to assess signal quality or accuracy using synthetic data. This work is the first to propose conditional Wasserstein Generative Adversarial Networks (cWGANs) to generate synthetic HAR accelerometry signals. Furthermore, we calculate quality metrics from the literature and study the impact of synthetic data on a large HAR dataset involving 395 users. Results show that i) cWGAN outperforms original Conditional Generative Adversarial Networks (cGANs), being 1D convolutional layers appropriate for generating accelerometry signals, ii) the performance improvement incorporating synthetic data is more significant as the dataset size is smaller, and iii) the quantity of synthetic data required is inversely proportional to the quantity of real data.},
	number = {4},
	urldate = {2025-03-04},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Lupión, Marcos and Cruciani, Federico and Cleland, Ian and Nugent, Chris and Ortigosa, Pilar M.},
	month = apr,
	year = {2024},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	keywords = {Accelerometry, data augmentation, Data augmentation, Data models, Generative adversarial networks, generative adversarial neural networks, Generators, human activity recognition, Human activity recognition, synthetic data, Synthetic data, Training},
	pages = {2350--2361},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\TB44JHM8\\10432776.html:text/html;Submitted Version:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\VAVJ9CQQ\\Lupión et al. - 2024 - Data Augmentation for Human Activity Recognition W.pdf:application/pdf},
}

@article{mohammadzadeh_cgan-based_2025,
	title = {{cGAN}-based high dimensional {IMU} sensor data generation for enhanced human activity recognition in therapeutic activities},
	volume = {103},
	issn = {1746-8094},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809424015349},
	doi = {10.1016/j.bspc.2024.107476},
	abstract = {Human activity recognition is a core technology for applications such as rehabilitation, health monitoring, and human–computer interactions. Wearable devices, especially Inertial Measurement Unit (IMU) sensors, provide rich features of human movements at a reasonable cost, which can be leveraged in activity recognition. Developing a robust classifier for activity recognition has always been of interest to researchers. One major problem is that there is usually a deficit of training data, which makes developing deep classifiers difficult and sometimes impossible. In this work, a novel generative adversarial network called TheraGAN was developed to generate IMU signals associated with rehabilitation activities. The generated signal comprises data from a 6-channel IMU, i.e., angular velocities and linear accelerations. Also, introducing simple activities simplified the generation process for activities of varying lengths. To evaluate the generated signals, several qualitative and quantitative studies were conducted, including perceptual similarity analysis, comparing extracted features to those from real data, visual inspection, and an investigation into how the generated data affects the performance of three deep classifiers trained on the generated and real data. The results showed that the generated signals closely mimicked the real signals, and generated data significantly improved the performance of all tested networks. Among the tested networks, the LSTM classifier showed the largest improvement with a 13.27\% increase in accuracy, while the CNN classifier achieved the highest accuracy at 89.7\%, effectively tackling data scarcity. This shows the generated data’s validity and TheraGAN’s potential as a tool to build more robust classifiers in case of insufficient data.},
	urldate = {2025-03-04},
	journal = {Biomedical Signal Processing and Control},
	author = {Mohammadzadeh, Mohammad and Ghadami, Ali and Taheri, Alireza and Behzadipour, Saeed},
	month = may,
	year = {2025},
	keywords = {Data augmentation, GAN, Human activity recognition, IMU, Rehabilitation},
	pages = {107476},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\SC49QDQ3\\S1746809424015349.html:text/html;Submitted Version:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\NJ2Z8RNU\\Mohammadzadeh et al. - 2025 - cGAN-based high dimensional IMU sensor data genera.pdf:application/pdf},
}

@article{kia_human_2024,
	title = {Human activity recognition by body-worn sensor data using bi-directional generative adversarial networks and frequency analysis techniques},
	volume = {81},
	issn = {1573-0484},
	url = {https://doi.org/10.1007/s11227-024-06743-0},
	doi = {10.1007/s11227-024-06743-0},
	abstract = {Existing datasets used for human activity recognition (HAR) usually suffer from limitations in terms of size variation and distribution of activity classes, which can impair the generalizability and robustness of the trained model, especially in the case of activity classes with minority data. This paper proposes an architecture utilizing bi-directional generative adversarial networks (Bi-GANs) beside fast Fourier transform, which stacks the 1-D accelerometer signals as m frequency bins × frames × 3 orientations and produces an RGB-based features pattern. The extracted patterns allow the 2D-CNN-based Bi-GAN architecture to learn the accelerometer signals' cross-axis relationships, which served as input for training a deep learning model for activity recognition equipped with a fuzzy inference dense layer. Also, we have used Hidden Markov Models (HMMs) for post-processing the classifier's output, which integrates the window-level decision in more extended periods, obtaining a significant performance improvement. The proposed method examined and conducted on MobiAct, Up-Fall, Opportunity, and WISDM datasets with different rates of augmentation, reaching to the 99.7\%, 99.96\%, 86.8\%, and 99.12\% rate of accuracy, respectively.},
	language = {en},
	number = {1},
	urldate = {2025-03-04},
	journal = {The Journal of Supercomputing},
	author = {Kia, Zohre and Yadollahzaeh-Tabari, Meisam and Motameni, Homayun},
	month = dec,
	year = {2024},
	keywords = {BI-GAN, FFT, HMM, Human activity recognition},
	pages = {342},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\3H3BWFL9\\Kia et al. - 2024 - Human activity recognition by body-worn sensor dat.pdf:application/pdf},
}

@ARTICLE{KHAN2021107671,
  AUTHOR = {Khan, Zanobya N. and Ahmad, Jamil},
  URL = {https://www.sciencedirect.com/science/article/pii/S1568494621005925},
  DATE = {2021},
  DOI = {https://doi.org/10.1016/j.asoc.2021.107671},
  ISSN = {1568-4946},
  JOURNALTITLE = {Applied Soft Computing},
  KEYWORDS = {Human activity recognition,Convolutional neural network,Squeeze-and-excitation module,Attention mechanism,Inertial sensors},
  PAGES = {107671},
  TITLE = {Attention induced multi-head convolutional neural network for human activity recognition},
  VOLUME = {110},
}

### HAR end

@ARTICLE{Nassif2019,
  AUTHOR = {Nassif, Ali Bou and Shahin, Ismail and Attili, Imtinan and Azzeh, Mohammad and Shaalan, Khaled},
  DATE = {2019},
  DOI = {10.1109/ACCESS.2019.2896880},
  JOURNALTITLE = {IEEE Access},
  KEYWORDS = {Hidden Markov models;Speech recognition;Neural networks;Deep learning;Feature extraction;Computer architecture;Acoustics;Speech recognition;deep neural network;systematic review},
  PAGES = {19143--19165},
  TITLE = {Speech Recognition Using Deep Neural Networks: A Systematic Review},
  VOLUME = {7},
}

@ARTICLE{padmanabhan2015review,
  AUTHOR = {Padmanabhan, Jayashree and Premkumar, Melvin Jose Johnson},
  PUBLISHER = {Taylor & Francis},
  DATE = {2015},
  DOI = {10.1080/02564602.2015.1010611},
  JOURNALTITLE = {IETE Technical Review},
  NUMBER = {4},
  PAGES = {240--251},
  TITLE = {Machine Learning in Automatic Speech Recognition: A Survey},
  VOLUME = {32},
}

@ARTICLE{srivastava2022speech,
  AUTHOR = {Srivastava, RK and Pandey, Digesh},
  PUBLISHER = {Elsevier},
  DATE = {2022},
  DOI = {https://doi.org/10.1016/j.matpr.2021.10.097},
  JOURNALTITLE = {Materials Today: Proceedings},
  PAGES = {1878--1883},
  TITLE = {Speech recognition using HMM and Soft Computing},
  VOLUME = {51},
}

@INPROCEEDINGS{Tirumala,
  AUTHOR = {Tirumala, Sreenivas Sremath and Shahamiri, Seyed Reza},
  LOCATION = {New York, NY, USA},
  PUBLISHER = {Association for Computing Machinery},
  URL = {https://doi.org/10.1145/3015166.3015210},
  BOOKTITLE = {Proceedings of the 8th International Conference on Signal Processing Systems},
  DATE = {2016},
  DOI = {10.1145/3015166.3015210},
  ISBN = {9781450347907},
  PAGES = {142--147},
  TITLE = {A review on Deep Learning approaches in Speaker Identification},
}

@ARTICLE{app11083603,
  AUTHOR = {Ye, Feng and Yang, Jun},
  URL = {https://www.mdpi.com/2076-3417/11/8/3603},
  DATE = {2021},
  DOI = {10.3390/app11083603},
  ISSN = {2076-3417},
  JOURNALTITLE = {Applied Sciences},
  NUMBER = {8},
  TITLE = {A Deep Neural Network Model for Speaker Identification},
  VOLUME = {11},
}

@ARTICLE{khalil2019,
  AUTHOR = {Khalil, Ruhul Amin and Jones, Edward and Babar, Mohammad Inayatullah and Jan, Tariqullah and Zafar, Mohammad Haseeb and Alhussain, Thamer},
  DATE = {2019},
  DOI = {10.1109/ACCESS.2019.2936124},
  JOURNALTITLE = {IEEE Access},
  PAGES = {117327--117345},
  TITLE = {Speech Emotion Recognition Using Deep Learning Techniques: A Review},
  VOLUME = {7},
}

@ARTICLE{singh2021spoken,
  AUTHOR = {Singh, Gundeep and Sharma, Sahil and Kumar, Vijay and Kaur, Manjit and Baz, Mohammed and Masud, Mehedi and others},
  PUBLISHER = {Hindawi},
  DATE = {2021},
  JOURNALTITLE = {Computational Intelligence and Neuroscience},
  TITLE = {Spoken language identification using deep learning},
  VOLUME = {2021},
}

@INPROCEEDINGS{jiao2016accent,
  AUTHOR = {Jiao, Yishan and Tu, Ming and Berisha, Visar and Liss, Julie M},
  BOOKTITLE = {Interspeech},
  DATE = {2016},
  PAGES = {2388--2392},
  TITLE = {Accent Identification by Combining Deep Neural Networks and Recurrent Neural Networks Trained on Long and Short Term Features.},
}

@ARTICLE{sanchez2022age,
  AUTHOR = {Sánchez-Hevia, Héctor A and Gil-Pita, Roberto and Utrilla-Manso, Manuel and Rosa-Zurera, Manuel},
  PUBLISHER = {Springer},
  DATE = {2022},
  JOURNALTITLE = {Multimedia Tools and Applications},
  NUMBER = {3},
  PAGES = {3535--3552},
  TITLE = {Age group classification and gender recognition from speech with temporal convolutional neural networks},
  VOLUME = {81},
}

@ARTICLE{alnuaim2022speaker,
  AUTHOR = {Alnuaim, Abeer Ali and Zakariah, Mohammed and Shashidhar, Chitra and Hatamleh, Wesam Atef and Tarazi, Hussam and Shukla, Prashant Kumar and Ratna, Rajnish},
  PUBLISHER = {Hindawi Limited},
  DATE = {2022},
  JOURNALTITLE = {Wireless Communications and Mobile Computing},
  PAGES = {1--13},
  TITLE = {Speaker gender recognition based on deep neural networks and ResNet50},
  VOLUME = {2022},
}

@ARTICLE{lu2020automatic,
  AUTHOR = {Lu, Xugang and Li, Sheng and Fujimoto, Masakiyo},
  PUBLISHER = {Springer},
  DATE = {2020},
  JOURNALTITLE = {Speech-to-speech translation},
  PAGES = {21--38},
  TITLE = {Automatic speech recognition},
}

@ARTICLE{shewalkar2019performance,
  AUTHOR = {Shewalkar, Apeksha and Nyavanandi, Deepika and Ludwig, Simone A},
  DATE = {2019},
  JOURNALTITLE = {Journal of Artificial Intelligence and Soft Computing Research},
  NUMBER = {4},
  PAGES = {235--245},
  TITLE = {Performance evaluation of deep neural networks applied to speech recognition: RNN, LSTM and GRU},
  VOLUME = {9},
}

@ARTICLE{HEMA2023109492,
  AUTHOR = {Hema, C. and {Garcia Marquez}, Fausto Pedro},
  URL = {https://www.sciencedirect.com/science/article/pii/S0003682X23002906},
  DATE = {2023},
  DOI = {https://doi.org/10.1016/j.apacoust.2023.109492},
  ISSN = {0003-682X},
  JOURNALTITLE = {Applied Acoustics},
  PAGES = {109492},
  TITLE = {Emotional speech Recognition using CNN and Deep learning techniques},
  VOLUME = {211},
}

@ARTICLE{mukhamadiyev2022automatic,
  AUTHOR = {Mukhamadiyev, Abdinabi and Khujayarov, Ilyos and Djuraev, Oybek and Cho, Jinsoo},
  PUBLISHER = {MDPI},
  DATE = {2022},
  DOI = {https://doi.org/10.3390/s22103683},
  JOURNALTITLE = {Sensors},
  NUMBER = {10},
  PAGES = {3683},
  TITLE = {Automatic speech recognition method based on deep learning approaches for Uzbek language},
  VOLUME = {22},
}

@INPROCEEDINGS{prabhavalkar2017comparison,
  AUTHOR = {Prabhavalkar, Rohit and Rao, Kanishka and Sainath, Tara N and Li, Bo and Johnson, Leif and Jaitly, Navdeep},
  BOOKTITLE = {Interspeech},
  DATE = {2017},
  PAGES = {939--943},
  TITLE = {A Comparison of sequence-to-sequence models for speech recognition.},
}

@INPROCEEDINGS{szucbeamsearch2019,
  AUTHOR = {Szűcs, Gábor and Huszti, Dorottya},
  BOOKTITLE = {2019 IEEE 17th International Symposium on Intelligent Systems and Informatics (SISY)},
  DATE = {2019},
  DOI = {10.1109/SISY47553.2019.9111502},
  KEYWORDS = {deep neural network;LSTM;two-way encoder;decoder;sequence-to-sequence model;beam search;text generation},
  PAGES = {221--226},
  TITLE = {Seq2seq Deep Learning Method for Summary Generation by LSTM with Two-way Encoder and Beam Search Decoder},
}

@INPROCEEDINGS{li2018seq2seq,
  AUTHOR = {Li, Zuchao and Cai, Jiaxun and He, Shexia and Zhao, Hai},
  BOOKTITLE = {Proceedings of the 27th International Conference on Computational Linguistics},
  DATE = {2018},
  PAGES = {3203--3214},
  TITLE = {Seq2seq dependency parsing},
}

@ARTICLE{graves2012sequence,
  AUTHOR = {Graves, Alex},
  DATE = {2012},
  JOURNALTITLE = {arXiv preprint arXiv:1211.3711},
  TITLE = {Sequence transduction with recurrent neural networks},
}

@ARTICLE{chan2015listen,
  AUTHOR = {Chan, William and Jaitly, Navdeep and Le, Quoc V and Vinyals, Oriol},
  DATE = {2015},
  JOURNALTITLE = {arXiv preprint arXiv:1508.01211},
  TITLE = {Listen, attend and spell},
}

@ARTICLE{jaitly2016online,
  AUTHOR = {Jaitly, Navdeep and Le, Quoc V and Vinyals, Oriol and Sutskever, Ilya and Sussillo, David and Bengio, Samy},
  DATE = {2016},
  JOURNALTITLE = {Advances in Neural Information Processing Systems},
  TITLE = {An online sequence-to-sequence model using partial conditioning},
  VOLUME = {29},
}

@INPROCEEDINGS{raffel2017online,
  AUTHOR = {Raffel, Colin and Luong, Minh-Thang and Liu, Peter J and Weiss, Ron J and Eck, Douglas},
  ORGANIZATION = {PMLR},
  BOOKTITLE = {International conference on machine learning},
  DATE = {2017},
  PAGES = {2837--2846},
  TITLE = {Online and linear-time attention by enforcing monotonic alignments},
}

@INPROCEEDINGS{sak2017recurrent,
  AUTHOR = {Sak, Hasim and Shannon, Matt and Rao, Kanishka and Beaufays, Françoise},
  BOOKTITLE = {Interspeech},
  DATE = {2017},
  PAGES = {1298--1302},
  TITLE = {Recurrent neural aligner: An encoder-decoder neural network model for sequence to sequence mapping.},
  VOLUME = {8},
}

@INPROCEEDINGS{chiugoogle,
  AUTHOR = {Chiu, Chung-Cheng and Sainath, Tara N. and Wu, Yonghui and Prabhavalkar, Rohit and Nguyen, Patrick and Chen, Zhifeng and Kannan, Anjuli and Weiss, Ron J. and Rao, Kanishka and Gonina, Ekaterina and Jaitly, Navdeep and Li, Bo and Chorowski, Jan and Bacchiani, Michiel},
  BOOKTITLE = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  DATE = {2018},
  DOI = {10.1109/ICASSP.2018.8462105},
  KEYWORDS = {Training;Hidden Markov models;Decoding;Task analysis;Optimization;Acoustics;Neural networks},
  PAGES = {4774--4778},
  TITLE = {State-of-the-Art Speech Recognition with Sequence-to-Sequence Models},
}

@ARTICLE{LIU2021107187,
  AUTHOR = {Liu, Xinwen and Wang, Huan and Li, Zongjin and Qin, Lang},
  URL = {https://www.sciencedirect.com/science/article/pii/S0950705121004494},
  DATE = {2021},
  DOI = {https://doi.org/10.1016/j.knosys.2021.107187},
  ISSN = {0950-7051},
  JOURNALTITLE = {Knowledge-Based Systems},
  PAGES = {107187},
  TITLE = {Deep learning in ECG diagnosis: A review},
  VOLUME = {227},
}

@ARTICLE{tsao2023heart,
  AUTHOR = {Tsao, Connie W and Aday, Aaron W and Almarzooq, Zaid I and Anderson, Cheryl AM and Arora, Pankaj and Avery, Christy L and Baker-Smith, Carissa M and Beaton, Andrea Z and Boehme, Amelia K and Buxton, Alfred E and others},
  PUBLISHER = {Am Heart Assoc},
  DATE = {2023},
  JOURNALTITLE = {Circulation},
  NUMBER = {8},
  PAGES = {e93--e621},
  TITLE = {Heart disease and stroke statistics—2023 update: a report from the American Heart Association},
  VOLUME = {147},
}

@ARTICLE{LLamedo2011,
  AUTHOR = {Llamedo, Mariano and Martínez, Juan Pablo},
  DATE = {2011},
  DOI = {10.1109/TBME.2010.2068048},
  JOURNALTITLE = {IEEE Transactions on Biomedical Engineering},
  KEYWORDS = {Databases;Electrocardiography;Lead;Heart beat;Discrete wavelet transforms;Training;Feature selection;heartbeat classification;linear classifier;wavelet transform (WT)},
  NUMBER = {3},
  PAGES = {616--625},
  TITLE = {Heartbeat Classification Using Feature Selection Driven by Database Generalization Criteria},
  VOLUME = {58},
}

@ARTICLE{MATHEWS201853,
  AUTHOR = {Mathews, Sherin M. and Kambhamettu, Chandra and Barner, Kenneth E.},
  URL = {https://www.sciencedirect.com/science/article/pii/S0010482518301264},
  DATE = {2018},
  DOI = {https://doi.org/10.1016/j.compbiomed.2018.05.013},
  ISSN = {0010-4825},
  JOURNALTITLE = {Computers in Biology and Medicine},
  PAGES = {53--62},
  TITLE = {A novel application of deep learning for single-lead ECG classification},
  VOLUME = {99},
}

@ARTICLE{zhu2019,
  AUTHOR = {zhu wenliang wenliang , wenliang and Chen, Xiaohe and Wang, Yan and Wang, Lirong},
  DATE = {2019},
  DOI = {10.1109/TCBB.2018.2846611},
  JOURNALTITLE = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  KEYWORDS = {Electrocardiography;Feature extraction;Morphology;Heart rate variability;Pregnancy;Heart beat;Principal component analysis;ECG morphology;feature extraction;classification;SVM},
  NUMBER = {1},
  PAGES = {131--138},
  TITLE = {Arrhythmia Recognition and Classification Using ECG Morphology and Segment Feature Analysis},
  VOLUME = {16},
}

@ARTICLE{desai2021low,
  AUTHOR = {Desai, Madhav P and Caffarena, Gabriel and Jevtic, Ruzica and Márquez, David G and Otero, Abraham},
  PUBLISHER = {MDPI},
  DATE = {2021},
  JOURNALTITLE = {Electronics},
  NUMBER = {19},
  PAGES = {2324},
  TITLE = {A low-latency, low-power FPGA implementation of ECG signal characterization using hermite polynomials},
  VOLUME = {10},
}

@ARTICLE{crippa2015multi,
  AUTHOR = {Crippa, Paolo and Curzi, Alessandro and Falaschetti, Laura and Turchetti, Claudio and others},
  DATE = {2015},
  JOURNALTITLE = {Int. J. Simul. Syst. Sci. Technol},
  NUMBER = {1},
  PAGES = {2--1},
  TITLE = {Multi-class ECG beat classification based on a Gaussian mixture model of Karhunen-Loève transform},
  VOLUME = {16},
}

@ARTICLE{CHEN2022127,
  AUTHOR = {Chen, Chun-Yen and Lin, Yan-Ting and Lee, Shie-Jue and Tsai, Wei-Chung and Huang, Tien-Chi and Liu, Yi-Hsueh and Cheng, Mu-Chun and Dai, Chia-Yen},
  URL = {https://www.sciencedirect.com/science/article/pii/S1046202321001134},
  DATE = {2022},
  DOI = {https://doi.org/10.1016/j.ymeth.2021.04.021},
  ISSN = {1046-2023},
  JOURNALTITLE = {Methods},
  NOTE = {Machine Learning Methods for Bio-Medical Image and Signal Processing: Recent Advances},
  PAGES = {127--135},
  TITLE = {Automated ECG classification based on 1D deep learning network},
  VOLUME = {202},
}

@ARTICLE{acharya2017deep,
  AUTHOR = {Acharya, U Rajendra and Oh, Shu Lih and Hagiwara, Yuki and Tan, Jen Hong and Adam, Muhammad and Gertych, Arkadiusz and San Tan, Ru},
  PUBLISHER = {Elsevier},
  DATE = {2017},
  JOURNALTITLE = {Computers in biology and medicine},
  PAGES = {389--396},
  TITLE = {A deep convolutional neural network model to classify heartbeats},
  VOLUME = {89},
}

@ARTICLE{singh2018classification,
  AUTHOR = {Singh, Shraddha and Pandey, Saroj Kumar and Pawar, Urja and Janghel, Rekh Ram},
  PUBLISHER = {Elsevier},
  DATE = {2018},
  JOURNALTITLE = {Procedia computer science},
  PAGES = {1290--1297},
  TITLE = {Classification of ECG arrhythmia using recurrent neural networks},
  VOLUME = {132},
}

@ARTICLE{prabhakararao2020attentive,
  AUTHOR = {Prabhakararao, Eedara and Dandapat, Samarendra},
  PUBLISHER = {IEEE},
  DATE = {2020},
  JOURNALTITLE = {IEEE Signal Processing Letters},
  PAGES = {2029--2033},
  TITLE = {Attentive RNN-based network to fuse 12-lead ECG and clinical features for improved myocardial infarction diagnosis},
  VOLUME = {27},
}

@ARTICLE{wang2023single,
  AUTHOR = {Wang, Mou and Rahardja, Sylwan and Fränti, Pasi and Rahardja, Susanto},
  PUBLISHER = {Elsevier},
  DATE = {2023},
  JOURNALTITLE = {Biomedical Signal Processing and Control},
  PAGES = {104067},
  TITLE = {Single-lead ECG recordings modeling for end-to-end recognition of atrial fibrillation with dual-path RNN},
  VOLUME = {79},
}

@ARTICLE{baloglu2019classification,
  AUTHOR = {Baloglu, Ulas Baran and Talo, Muhammed and Yildirim, Ozal and San Tan, Ru and Acharya, U Rajendra},
  PUBLISHER = {Elsevier},
  DATE = {2019},
  JOURNALTITLE = {Pattern recognition letters},
  PAGES = {23--30},
  TITLE = {Classification of myocardial infarction with multi-lead ECG signals and deep CNN},
  VOLUME = {122},
}

@ARTICLE{rai2022hybrid,
  AUTHOR = {Rai, Hari Mohan and Chatterjee, Kalyan},
  PUBLISHER = {Springer},
  DATE = {2022},
  JOURNALTITLE = {Applied Intelligence},
  NUMBER = {5},
  PAGES = {5366--5384},
  TITLE = {Hybrid CNN-LSTM deep learning model and ensemble technique for automatic detection of myocardial infarction using big ECG data},
  VOLUME = {52},
}

@ARTICLE{sowmya2022contemplate,
  AUTHOR = {Sowmya, S and Jose, Deepa},
  PUBLISHER = {Elsevier},
  DATE = {2022},
  JOURNALTITLE = {Measurement: Sensors},
  PAGES = {100558},
  TITLE = {Contemplate on ECG signals and classification of arrhythmia signals using CNN-LSTM deep learning model},
  VOLUME = {24},
}

@INPROCEEDINGS{Banerjee2020,
  AUTHOR = {Banerjee, Rohan and Ghose, Avik and Muthana Mandana, Kayapanda},
  BOOKTITLE = {2020 International Joint Conference on Neural Networks (IJCNN)},
  DATE = {2020},
  DOI = {10.1109/IJCNN48605.2020.9207044},
  KEYWORDS = {Electrocardiography;Heart rate variability;Feature extraction;Convolution;Time series analysis;Arteries;Morphology;Coronary Artery Disease (CAD);ECG;CNN;LSTM;Hybrid Architecture},
  PAGES = {1--8},
  TITLE = {A Hybrid CNN-LSTM Architecture for Detection of Coronary Artery Disease from ECG},
}



### ECG begin

@article{kusuma_ecg_2022,
	title = {{ECG} signals-based automated diagnosis of congestive heart failure using {Deep} {CNN} and {LSTM} architecture},
	volume = {42},
	issn = {0208-5216},
	url = {https://www.sciencedirect.com/science/article/pii/S0208521622000067},
	doi = {10.1016/j.bbe.2022.02.003},
	abstract = {In humans, Congestive Heart Failure (CHF) refers to the chronic progressive condition that drastically influences the pumping potentiality of the heart muscle. This CHF has the possibility of increasing health expenditure, morbidity, mortality and minimized quality of life. In this context, Electrocardiogram (ECG) is considered as the simplest and a non-invasive diagnosis method that aids in detecting and demonstrating the realizable changes in CHF. However, diagnosing CHF based on manual exploration of ECG signals is frequently impacted by errors as duration and small amplitude of the signals either investigated separately or in the integration is determined to neither specific nor sensitive. At this juncture, the reliability and diagnostic objectivity of ECG signals during the CHF detection process may be enhanced through the inclusion of automated computer-aided system. In this paper, Deep CNN and LSTM Architecture (DCNN-LSTM)-based automated diagnosis system is proposed for detecting CHF using ECG signals. In specific, CNN is included for the purpose of extracting deep features and LSTM is used for attaining the objective of CHF detection using the extracted features. This proposed DCNN-LSTM is evolved with minimal pre-processing of ECG signals and does not involve any classification process or manual engineered features during diagnosis. The experimentation of the proposed DCNN-LSTM conducted using the real time ECG signals datasets confirmed an accuracy of 99.52, sensitivity of 99.31\%, specificity of 99.28\%, F-Score of 98.94\% and AUC of 99.9\%, respectively.},
	number = {1},
	urldate = {2025-03-02},
	journal = {Biocybernetics and Biomedical Engineering},
	author = {Kusuma, S. and Jothi, K. R.},
	month = jan,
	year = {2022},
	keywords = {CHF, CNN, ECG, LSTM},
	pages = {247--257},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\5YR5GGDM\\S0208521622000067.html:text/html},
}

@article{sun_arrhythmia_2024,
	title = {An {Arrhythmia} {Classification} {Model} {Based} on a {CNN}-{LSTM}-{SE} {Algorithm}},
	volume = {24},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/24/19/6306},
	doi = {10.3390/s24196306},
	abstract = {Arrhythmia is the main cause of sudden cardiac death, and ECG signal analysis is a common method for the noninvasive diagnosis of arrhythmia. In this paper, we propose an arrhythmia classification model based on the combination of a channel attention mechanism (SE module), convolutional neural network (CNN), and long short-term memory neural network (LSTM). The data of this model use the MIT-BIH arrhythmia database, and after noise reduction of raw ECG data by the EEMD denoising algorithm, a CNN-LSTM is used to learn features from the data, and the fusion channel attention mechanism is used to adjust the weight of the feature map. The CNN-LSTM-SE model is compared with the LSTM, CNN-LSTM, and LSTM-attention models, and the models are evaluated using Precision, Recall, and F1-Score. The classification performance of the tested CNN-LSTM-SE classification prediction model is better, with a classification accuracy of 98.5\%, a classification precision rate of more than 97\% for each label, a recall rate of more than 98\%, and an F1-score of more than 0.98. It meets the requirements of arrhythmia classification prediction and has a certain practical value.},
	language = {en},
	number = {19},
	urldate = {2025-03-07},
	journal = {Sensors},
	author = {Sun, Ao and Hong, Wei and Li, Juan and Mao, Jiandong},
	month = jan,
	year = {2024},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {arrhythmia, classification prediction, CNN-LSTM-SE},
	pages = {6306},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\IKXCHXWY\\Sun et al. - 2024 - An Arrhythmia Classification Model Based on a CNN-.pdf:application/pdf},
}

@article{chen_automated_2022,
	series = {Machine {Learning} {Methods} for {Bio}-{Medical} {Image} and {Signal} {Processing}: {Recent} {Advances}},
	title = {Automated {ECG} classification based on {1D} deep learning network},
	volume = {202},
	issn = {1046-2023},
	url = {https://www.sciencedirect.com/science/article/pii/S1046202321001134},
	doi = {10.1016/j.ymeth.2021.04.021},
	abstract = {The standard 12-lead electrocardiogram (ECG) records the heart’s electrical activity from electrodes on the skin, and is widely used in screening and diagnosis of the cardiac conditions due to its low price and non-invasive characteristics. Manual examination of ECGs requires professional medical skills, and is strenuous and time consuming. Recently, deep learning methodologies have been successfully applied in the analysis of medical images. In this paper, we present an automated system for the identification of normal and abnormal ECG signals. A multi-channel multi-scale deep neural network (DNN) model is proposed, which is an end-to-end structure to classify the ECG signals without any feature extraction. Convolutional layers are used to extract primary features, and long short-term memory (LSTM) and attention are incorporated to improve the performance of the DNN model. The system was developed with a 12-lead ECG dataset provided by the Kaohsiung Medical University Hospital (KMUH). Experimental results show that the proposed system can yield high recognition rates in classifying normal and abnormal ECG signals.},
	urldate = {2025-03-02},
	journal = {Methods},
	author = {Chen, Chun-Yen and Lin, Yan-Ting and Lee, Shie-Jue and Tsai, Wei-Chung and Huang, Tien-Chi and Liu, Yi-Hsueh and Cheng, Mu-Chun and Dai, Chia-Yen},
	month = jun,
	year = {2022},
	keywords = {12-Lead electrocardiogram, Cardiac abnormality, Convolutional layer, Long short-term memory, Self-constructing clustering},
	pages = {127--135},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\5DTKX6QZ\\S1046202321001134.html:text/html},
}

@article{alamatsaz_lightweight_2024,
	title = {A lightweight hybrid {CNN}-{LSTM} explainable model for {ECG}-based arrhythmia detection},
	volume = {90},
	issn = {1746-8094},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809423013174},
	doi = {10.1016/j.bspc.2023.105884},
	abstract = {Objective:
Electrocardiogram (ECG) is the most frequent and routine diagnostic tool used for monitoring heart electrical signals and evaluating its functionality. The human heart can suffer from a variety of diseases, including cardiac arrhythmias. Arrhythmia is an irregular heart rhythm that in severe cases can lead to stroke and can be diagnosed via ECG recordings. Since early detection of cardiac arrhythmias is of great importance, computerized and automated classification and identification of these abnormal heart signals have received much attention for the past decades.
Methods:
This paper introduces a light Deep Learning (DL) approach for high accuracy detection of 8 different cardiac arrhythmias and normal rhythms. To employ DL techniques, the ECG signals were preprocessed using resampling and baseline wander removal techniques. The classification was performed using an 11-layer network employing a combination of Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM).
Results:
In order to evaluate the proposed technique, ECG signals are chosen from the two physionet databases, the MIT-BIH arrhythmia database and the long-term AF database. The proposed DL framework based on the combination of CNN and LSTM showed promising results than most of the state-of-the-art methods. The proposed method reaches the mean diagnostic accuracy of 98.24\%.
Conclusion:
A trained model for arrhythmia classification using diverse ECG signals were successfully developed and tested.
Significance:
This study presents a lightweight classification technique with high diagnostic accuracy compared to other notable methods, making it a potential candidate for implementation in Holter monitor devices for arrhythmia detection. Finally, we used SHapley Additive exPlanations (SHAP), the most popular Explainable Artificial Intelligence (XAI) method to understand how our model make predictions. The results indicate that those features (ECG samples) that have contributed the most to a prediction are consonant with clinicians’ decisions. Therefore, the use of interpretable models increases the trust of clinicians in AI and thus leads to decreasing the number of misdiagnoses of cardiovascular diseases.},
	urldate = {2025-03-07},
	journal = {Biomedical Signal Processing and Control},
	author = {Alamatsaz, Negin and Tabatabaei, Leyla and Yazdchi, Mohammadreza and Payan, Hamidreza and Alamatsaz, Nima and Nasimi, Fahimeh},
	month = apr,
	year = {2024},
	keywords = {Arrhythmia, Convolutional Neural Network, Deep learning, Electrocardiogram, Long Short Term Memory, SHapley additive exPlanations},
	pages = {105884},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\S6X7EJXL\\S1746809423013174.html:text/html},
}

@article{wang_automated_2021,
	title = {Automated {ECG} classification using a non-local convolutional block attention module},
	volume = {203},
	issn = {0169-2607},
	url = {https://www.sciencedirect.com/science/article/pii/S016926072100081X},
	doi = {10.1016/j.cmpb.2021.106006},
	abstract = {Background and objective: Recent advances in deep learning have been applied to ECG detection and obtained great success. The spatial and temporal information from ECG signals is fused by combining convolutional neural networks (CNN) with recurrent neural network (RNN). However, these networks ignore the different contribution of local and global segments of a feature map extracted from the ECG and the correlation relationship between the above two segments. To address this issue, a novel convolutional neural network with non-local convolutional block attention module(NCBAM) is proposed to automatically classify ECG heartbeats. Methods: Our proposed method consists of a 33-layer CNN architecture followed by a NCBAM module. Initially, preprocessed electrocardiogram (ECG) signals are fed into the CNN architecture to extract the spatial and channel features. Further, long-range dependencies of representative features along spatial and channel axis are captured by non-local attention. Finally, the spatial, channel and temporal information of ECG are fused by a learned matrix. The learned matrix is to mine rich relationship information across the above three types of information to make up for the different contribution. Results and conclusion: The proposed method achieves an average F1 score of 0.9664 on MIT-BIH arrhythmia database, as well as AUC of 0.9314 and Fmax of 0.8507 on PTB-XL ECG database. Compared with the state-of-the-art attention mechanism based on the same public database, NCBAM achieves an obvious improvement in classifying ECG heartbeats. The results demonstrate the proposed method is reliable and efficient for ECG beat classification.},
	urldate = {2025-03-02},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Wang, Jikuo and Qiao, Xu and Liu, Changchun and Wang, Xinpei and Liu, YuanYuan and Yao, Lianke and Zhang, Huan},
	month = may,
	year = {2021},
	keywords = {Cardiac arrhythmias, Cardiovascular diseases, Convolutional neural network, Attention mechanism, Non-local convolutional block attention module, ECG},
	pages = {106006},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\9PTI7URM\\S016926072100081X.html:text/html},
}

@article{zhang_ecg-based_2020,
	title = {{ECG}-based multi-class arrhythmia detection using spatio-temporal attention-based convolutional recurrent neural network},
	volume = {106},
	issn = {0933-3657},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365719312606},
	doi = {10.1016/j.artmed.2020.101856},
	abstract = {Automatic arrhythmia detection based on electrocardiogram (ECG) is of great significance for early prevention and diagnosis of cardiac diseases. Recently, deep learning methods have been applied to arrhythmia detection and obtained great success. Among them, convolutional neural network (CNN) is an effective method for extracting features due to its local connectivity and parameter sharing. In addition, recurrent neural network (RNN) is another commonly used method, which is applied to process time-series signal. The stacking of both CNN and RNN has been proved to be more effective in multi-class arrhythmia detection. However, these networks ignored the fact that different channels and temporal segments of a feature map extracted from the 12-lead ECG signal contribute differently to cardiac arrhythmia detection, and thus, the classification performance could be greatly improved. To address this issue, spatio-temporal attention-based convolutional recurrent neural network (STA-CRNN) is proposed to focus on representative features along both spatial and temporal axes. STA-CRNN consists of CNN subnetwork, spatio-temporal attention modules and RNN subnetwork. The experiment result shows that, STA-CRNN reaches an average F1 score of 0.835 in classifying 8 types of arrhythmias and normal rhythm. Compared with the state-of-the-art methods based on the same public dataset, STA-CRNN achieves an obvious improvement on identifying most of arrhythmias. Also, it is demonstrated by visualization that the learned features through STA-CRNN are in line with clinical judgement. STA-CRNN provides a promising method for automatic arrhythmia detection, which has a potential to assist cardiologists in the diagnosis of arrhythmias.},
	urldate = {2025-03-02},
	journal = {Artificial Intelligence in Medicine},
	author = {Zhang, Jing and Liu, Aiping and Gao, Min and Chen, Xiang and Zhang, Xu and Chen, Xun},
	month = jun,
	year = {2020},
	keywords = {Arrhythmia detection, Convolution neural network, ECG, Recurrent neural network, Spatio-temporal attention module},
	pages = {101856},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\7E7AJU34\\S0933365719312606.html:text/html},
}

@article{zhu_electrocardiogram_2019,
	title = {Electrocardiogram generation with a bidirectional {LSTM}-{CNN} generative adversarial network},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-42516-z},
	doi = {10.1038/s41598-019-42516-z},
	abstract = {Heart disease is a malignant threat to human health. Electrocardiogram (ECG) tests are used to help diagnose heart disease by recording the heart’s activity. However, automated medical-aided diagnosis with computers usually requires a large volume of labeled clinical data without patients' privacy to train the model, which is an empirical problem that still needs to be solved. To address this problem, we propose a generative adversarial network (GAN), which is composed of a bidirectional long short-term memory(LSTM) and convolutional neural network(CNN), referred as BiLSTM-CNN,to generate synthetic ECG data that agree with existing clinical data so that the features of patients with heart disease can be retained. The model includes a generator and a discriminator, where the generator employs the two layers of the BiLSTM networks and the discriminator is based on convolutional neural networks. The 48 ECG records from individuals of the MIT-BIH database were used to train the model. We compared the performance of our model with two other generative models, the recurrent neural network autoencoder(RNN-AE) and the recurrent neural network variational autoencoder (RNN-VAE). The results showed that the loss function of our model converged to zero the fastest. We also evaluated the loss of the discriminator of GANs with different combinations of generator and discriminator. The results indicated that BiLSTM-CNN GAN could generate ECG data with high morphological similarity to real ECG recordings.},
	language = {en},
	number = {1},
	urldate = {2025-03-02},
	journal = {Scientific Reports},
	author = {Zhu, Fei and Ye, Fei and Fu, Yuchen and Liu, Quan and Shen, Bairong},
	month = may,
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	keywords = {Bioinformatics, Interventional cardiology, Scientific data},
	pages = {6734},
}

### Sota ECG

@article{huang_ecg_2024,
	title = {{ECG} classification based on guided attention mechanism},
	volume = {257},
	issn = {0169-2607},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260724004474},
	doi = {10.1016/j.cmpb.2024.108454},
	abstract = {Background and Objective
Integrating domain knowledge into deep learning models can improve their effectiveness and increase explainability. This study aims to enhance the classification performance of electrocardiograms (ECGs) by customizing specific guided mechanisms based on the characteristics of different cardiac abnormalities.
Methods
Two novel guided attention mechanisms, Guided Spatial Attention (GSA) and CAM-based spatial guided attention mechanism (CGAM), were introduced. Different attention guidance labels were created based on clinical knowledge for four ECG abnormality classification tasks: ST change detection, premature contraction identification, Wolf-Parkinson-White syndrome (WPW) classification, and atrial fibrillation (AF) detection. The models were trained and evaluated separately for each classification task. Model explainability was quantified using Shapley values.
Results
GSA improved the F1 score of the model by 5.74\%, 5\%, 8.96\%, and 3.91\% for ST change detection, premature contraction identification, WPW classification, and AF detection, respectively. Similarly, CGAM exhibited improvements of 3.89\%, 5.40\%, 8.21\%, and 1.80\% for the respective tasks. The combined use of GSA and CGAM resulted in even higher improvements of 6.26\%, 5.58\%, 8.85\%, and 4.03\%, respectively. Moreover, when all four tasks were conducted simultaneously, a notable overall performance boost was achieved, demonstrating the broad adaptability of the proposed model. The quantified Shapley values demonstrated the effectiveness of the guided attention mechanisms in enhancing the model's explainability.
Conclusions
The guided attention mechanisms, utilizing domain knowledge, effectively directed the model's attention, leading to improved classification performance and explainability. These findings have significant implications in facilitating accurate automated ECG classification.},
	urldate = {2025-03-03},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Huang, Yangcheng and Liu, Wenjing and Yin, Ziyi and Hu, Shuaicong and Wang, Mingjie and Cai, Wenjie},
	month = dec,
	year = {2024},
	keywords = {Deep learning, Electrocardiogram, Guided Attention mechanism},
	pages = {108454},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\QL6V4KFK\\S0169260724004474.html:text/html},
}

@article{hasani_liquid_2021,
	title = {Liquid {Time}-constant {Networks}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16936},
	doi = {10.1609/aaai.v35i9.16936},
	abstract = {We introduce a new class of time-continuous recurrent neural network models. Instead of declaring a learning system's dynamics by implicit nonlinearities, we construct networks of linear first-order dynamical systems modulated via nonlinear interlinked gates. The resulting models represent dynamical systems with varying (i.e., liquid) time-constants coupled to their hidden state, with outputs being computed by numerical differential equation solvers. These neural networks exhibit stable and bounded behavior, yield superior expressivity within the family of neural ordinary differential equations, and give rise to improved performance on time-series prediction tasks. To demonstrate these properties, we first take a theoretical approach to find bounds over their dynamics, and compute their expressive power by the trajectory length measure in a latent trajectory space. We then conduct a series of time-series prediction experiments to manifest the approximation capability of Liquid Time-Constant Networks (LTCs) compared to classical and modern RNNs.},
	language = {en},
	number = {9},
	urldate = {2025-03-03},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
	month = may,
	year = {2021},
	note = {Number: 9},
	keywords = {Time-Series/Data Streams},
	pages = {7657--7666},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\JMDQ5GWM\\Hasani et al. - 2021 - Liquid Time-constant Networks.pdf:application/pdf},
}

@misc{aghaomidi_ecg-sleepnet_2024,
	title = {{ECG}-{SleepNet}: {Deep} {Learning}-{Based} {Comprehensive} {Sleep} {Stage} {Classification} {Using} {ECG} {Signals}},
	shorttitle = {{ECG}-{SleepNet}},
	url = {http://arxiv.org/abs/2412.01929},
	doi = {10.48550/arXiv.2412.01929},
	abstract = {Accurate sleep stage classification is essential for understanding sleep disorders and improving overall health. This study proposes a novel three-stage approach for sleep stage classification using ECG signals, offering a more accessible alternative to traditional methods that often rely on complex modalities like EEG. In Stages 1 and 2, we initialize the weights of two networks, which are then integrated in Stage 3 for comprehensive classification. In the first phase, we estimate key features using Feature Imitating Networks (FINs) to achieve higher accuracy and faster convergence. The second phase focuses on identifying the N1 sleep stage through the time-frequency representation of ECG signals. Finally, the third phase integrates models from the previous stages and employs a Kolmogorov-Arnold Network (KAN) to classify five distinct sleep stages. Additionally, data augmentation techniques, particularly SMOTE, are used in enhancing classification capabilities for underrepresented stages like N1. Our results demonstrate significant improvements in the classification performance, with an overall accuracy of 80.79\% an overall kappa of 0.73. The model achieves specific accuracies of 86.70\% for Wake, 60.36\% for N1, 83.89\% for N2, 84.85\% for N3, and 87.16\% for REM. This study emphasizes the importance of weight initialization and data augmentation in optimizing sleep stage classification with ECG signals.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Aghaomidi, Poorya and Wang, Ge},
	month = dec,
	year = {2024},
	note = {arXiv:2412.01929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	file = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\8MHITBJB\\2412.html:text/html},
}

@article{yang_data_2024,
	title = {Data imbalance in cardiac health diagnostics using {CECG}-{GAN}},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-65619-8},
	doi = {10.1038/s41598-024-65619-8},
	abstract = {Heart disease is the world’s leading cause of death. Diagnostic models based on electrocardiograms (ECGs) are often limited by the scarcity of high-quality data and issues of data imbalance. To address these challenges, we propose a conditional generative adversarial network (CECG-GAN). This strategy enables the generation of samples that closely approximate the distribution of ECG data. Additionally, CECG-GAN addresses waveform jitter, slow processing speeds, and dataset imbalance issues through the integration of a transformer architecture. We evaluated this approach using two datasets: MIT-BIH and CSPC2020. The experimental results demonstrate that CECG-GAN achieves outstanding performance metrics. Notably, the percentage root mean square difference (PRD) reached 55.048, indicating a high degree of similarity between generated and actual ECG waveforms. Additionally, the Fréchet distance (FD) was approximately 1.139, the root mean square error (RMSE) registered at 0.232, and the mean absolute error (MAE) was recorded at 0.166.},
	language = {en},
	number = {1},
	urldate = {2025-03-02},
	journal = {Scientific Reports},
	author = {Yang, Yang and Lan, Tianyu and Wang, Yang and Li, Fengtian and Liu, Liyan and Huang, Xupeng and Gao, Fei and Jiang, Shuhua and Zhang, Zhijun and Chen, Xing},
	month = jun,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Computational models, Computational neuroscience, Data processing, Databases, Machine learning},
	pages = {14767},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\DW4TYCZA\\Yang et al. - 2024 - Data imbalance in cardiac health diagnostics using.pdf:application/pdf},
}

@article{msigwa_iot-driven_2024,
	title = {{IoT}-driven wearable devices enhancing healthcare: {ECG} classification with cluster-based {GAN} and meta-features},
	volume = {28},
	issn = {2542-6605},
	shorttitle = {{IoT}-driven wearable devices enhancing healthcare},
	url = {https://www.sciencedirect.com/science/article/pii/S2542660524003469},
	doi = {10.1016/j.iot.2024.101405},
	abstract = {Wearable devices in medical technology promise advancements in healthcare but face challenges like limited data use and delayed analysis, hindering their real-time effectiveness. Enabling wearable devices with edge computing maximizes their potential, allowing real-time tasks like ECG classification to be performed intelligently at the device level. We propose the Wearable IoT Edge, a computing device that empowers wearable health devices with real-time data insights and IoT capabilities, facilitated by the Wearable Interworking Proxy and compliant with oneM2M standard-based server. We demonstrate the application of a proposed Wearable IoT Edge by addressing ECG classification challenges. Our approach addresses data imbalance by integrating a Cluster-Based Generative Adversarial Network (GAN) with meta-features derived from Convolutional Neural Networks (CNNs) and Transformers to enhance ECG classification accuracy. Experimental results demonstrate a 3.18\% improvement in the F1 score for ECG classification validating the effectiveness of the approach. These findings highlight the Wearable IoT Edge’s potential to improve real-time healthcare monitoring and diagnostics.},
	urldate = {2025-03-03},
	journal = {Internet of Things},
	author = {Msigwa, Constantino and Bernard, Denis and Yun, Jaeseok},
	month = dec,
	year = {2024},
	keywords = {Bidirectional Long Short-Term Memory, Bitalino, Clustering, Convolutional Neural Network, Electrocardiogram, Electrocardiogram embedding, Feature fusion, Generative Adversarial Network, Transformer, Wearable interworking proxy, Wearable IoT Edge},
	pages = {101405},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\2H8LWSUV\\S2542660524003469.html:text/html},
}

### ECG end

@ARTICLE{Schirrmeister2017,
  AUTHOR = {Schirrmeister, Robin Tibor and Springenberg, Jost Tobias and Fiederer, Lukas Dominique Josef and Glasstetter, Martin and Eggensperger, Katharina and Tangermann, Michael and Hutter, Frank and Burgard, Wolfram and Ball, Tonio},
  URL = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.23730},
  DATE = {2017},
  DOI = {https://doi.org/10.1002/hbm.23730},
  JOURNALTITLE = {Human Brain Mapping},
  NUMBER = {11},
  PAGES = {5391--5420},
  TITLE = {Deep learning with convolutional neural networks for EEG decoding and visualization},
  VOLUME = {38},
}

@ARTICLE{gao2021complex,
  AUTHOR = {Gao, Zhongke and Dang, Weidong and Wang, Xinmin and Hong, Xiaolin and Hou, Linhua and Ma, Kai and Perc, Matjaž},
  PUBLISHER = {Springer},
  DATE = {2021},
  JOURNALTITLE = {Cognitive Neurodynamics},
  NUMBER = {3},
  PAGES = {369--388},
  TITLE = {Complex networks and deep learning for EEG signal analysis},
  VOLUME = {15},
}

@ARTICLE{Ang7802578,
  AUTHOR = {Ang, Kai Keng and Guan, Cuntai},
  DATE = {2017},
  DOI = {10.1109/TNSRE.2016.2646763},
  JOURNALTITLE = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  KEYWORDS = {Electroencephalography;Feature extraction;Brain modeling;Computational modeling;Calibration;Adaptation models;Data models;Adaptive;brain–computer interface (BCI);electroenceptography (EEG);machine learning;motor imagery (MI);operant conditioning;stroke rehabilitation},
  NUMBER = {4},
  PAGES = {392--401},
  TITLE = {EEG-Based Strategies to Detect Motor Imagery for Control and Rehabilitation},
  VOLUME = {25},
}

@ARTICLE{shen2022aberrated,
  AUTHOR = {Shen, Zhongxia and Li, Gang and Fang, Jiaqi and Zhong, Hongyang and Wang, Jie and Sun, Yu and Shen, Xinhua},
  PUBLISHER = {MDPI},
  DATE = {2022},
  JOURNALTITLE = {Sensors},
  NUMBER = {14},
  PAGES = {5420},
  TITLE = {Aberrated multidimensional EEG characteristics in patients with generalized anxiety disorder: A machine-learning based analysis framework},
  VOLUME = {22},
}

@ARTICLE{boonyaki2020,
  AUTHOR = {Boonyakitanont, Poomipat and Lek-Uthai, Apiwat and Chomtho, Krisnachai and Songsiri, Jitkomut},
  PUBLISHER = {Elsevier},
  DATE = {2020},
  JOURNALTITLE = {Biomedical Signal Processing and Control},
  PAGES = {101702},
  TITLE = {A review of feature extraction and performance evaluation in epileptic seizure detection using EEG},
  VOLUME = {57},
}

@ARTICLE{sharma2021automated,
  AUTHOR = {Sharma, Manish and Tiwari, Jainendra and Patel, Virendra and Acharya, U Rajendra},
  PUBLISHER = {MDPI},
  DATE = {2021},
  JOURNALTITLE = {Electronics},
  NUMBER = {13},
  PAGES = {1531},
  TITLE = {Automated identification of sleep disorder types using triplet half-band filter and ensemble machine learning techniques with EEG signals},
  VOLUME = {10},
}

@ARTICLE{vaquerizo2023explainable,
  AUTHOR = {Vaquerizo-Villar, Fernando and Gutiérrez-Tobal, Gonzalo C and Calvo, Eva and Álvarez, Daniel and Kheirandish-Gozal, Leila and Del Campo, Félix and Gozal, David and Hornero, Roberto},
  PUBLISHER = {Elsevier},
  DATE = {2023},
  JOURNALTITLE = {Computers in Biology and Medicine},
  PAGES = {107419},
  TITLE = {An explainable deep-learning model to stage sleep states in children and propose novel EEG-related patterns in sleep apnea},
  VOLUME = {165},
}

@ARTICLE{modir2023systematic,
  AUTHOR = {Modir, Aslan and Shamekhi, Sina and Ghaderyan, Peyvand},
  PUBLISHER = {Elsevier},
  DATE = {2023},
  JOURNALTITLE = {Measurement},
  PAGES = {113274},
  TITLE = {A systematic review and methodological analysis of EEG-based biomarkers of Alzheimer's disease},
}

@ARTICLE{altaheri2023deep,
  AUTHOR = {Altaheri, Hamdi and Muhammad, Ghulam and Alsulaiman, Mansour and Amin, Syed Umar and Altuwaijri, Ghadir Ali and Abdul, Wadood and Bencherif, Mohamed A and Faisal, Mohammed},
  PUBLISHER = {Springer},
  DATE = {2023},
  JOURNALTITLE = {Neural Computing and Applications},
  NUMBER = {20},
  PAGES = {14681--14722},
  TITLE = {Deep learning techniques for classification of electroencephalogram (EEG) motor imagery (MI) signals: A review},
  VOLUME = {35},
}

@ARTICLE{delorme2007enhanced,
  AUTHOR = {Delorme, Arnaud and Sejnowski, Terrence and Makeig, Scott},
  PUBLISHER = {Elsevier},
  DATE = {2007},
  JOURNALTITLE = {Neuroimage},
  NUMBER = {4},
  PAGES = {1443--1449},
  TITLE = {Enhanced detection of artifacts in EEG data using higher-order statistics and independent component analysis},
  VOLUME = {34},
}

@ARTICLE{jafarifarmand2019eeg,
  AUTHOR = {Jafarifarmand, Aysa and Badamchizadeh, Mohammad Ali},
  PUBLISHER = {IEEE},
  DATE = {2019},
  JOURNALTITLE = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  NUMBER = {6},
  PAGES = {1200--1208},
  TITLE = {EEG artifacts handling in a real practical brain--computer interface controlled vehicle},
  VOLUME = {27},
}

@ARTICLE{brunner2007spatial,
  AUTHOR = {Brunner, Clemens and Naeem, Muhammad and Leeb, Robert and Graimann, Bernhard and Pfurtscheller, Gert},
  PUBLISHER = {Elsevier},
  DATE = {2007},
  JOURNALTITLE = {Pattern recognition letters},
  NUMBER = {8},
  PAGES = {957--964},
  TITLE = {Spatial filtering and selection of optimized components in four class motor imagery EEG data using independent components analysis},
  VOLUME = {28},
}

@ARTICLE{zhang2019novel,
  AUTHOR = {Zhang, Ruilong and Zong, Qun and Dou, Liqian and Zhao, Xinyi},
  PUBLISHER = {IOP Publishing},
  DATE = {2019},
  JOURNALTITLE = {Journal of neural engineering},
  NUMBER = {6},
  PAGES = {066004},
  TITLE = {A novel hybrid deep learning scheme for four-class motor imagery classification},
  VOLUME = {16},
}

@INPROCEEDINGS{kumar2016deep,
  AUTHOR = {Kumar, Shiu and Sharma, Alok and Mamun, Kabir and Tsunoda, Tatsuhiko},
  ORGANIZATION = {IEEE},
  BOOKTITLE = {2016 3rd Asia-Pacific World Congress on Computer Science and Engineering (APWC on CSE)},
  DATE = {2016},
  PAGES = {34--39},
  TITLE = {A deep learning approach for motor imagery EEG signal classification},
}

@ARTICLE{tibrewal2022,
  AUTHOR = {Tibrewal, Navneet and Leeuwis, Nikki and Alimardani, Maryam},
  PUBLISHER = {Public Library of Science San Francisco, CA USA},
  DATE = {2022},
  JOURNALTITLE = {Plos one},
  NUMBER = {7},
  PAGES = {e0268880},
  TITLE = {Classification of motor imagery EEG using deep learning increases performance in inefficient BCI users},
  VOLUME = {17},
}

@ARTICLE{dai2019eeg,
  AUTHOR = {Dai, Mengxi and Zheng, Dezhi and Na, Rui and Wang, Shuai and Zhang, Shuailei},
  PUBLISHER = {MDPI},
  DATE = {2019},
  JOURNALTITLE = {Sensors},
  NUMBER = {3},
  PAGES = {551},
  TITLE = {EEG classification of motor imagery using a novel deep learning framework},
  VOLUME = {19},
}

@ARTICLE{Feng2020novel,
  AUTHOR = {Li, Feng and He, Fan and Wang, Fei and Zhang, Dengyong and Xia, Yi and Li, Xiaoyu},
  PUBLISHER = {MDPI},
  DATE = {2020},
  JOURNALTITLE = {Applied Sciences},
  NUMBER = {5},
  PAGES = {1605},
  TITLE = {A novel simplified convolutional neural network classification algorithm of motor imagery EEG signals based on deep learning},
  VOLUME = {10},
}

@INPROCEEDINGS{LiMingaiLSTM,
  AUTHOR = {Li, Mingai and Zhu, Wei and Zhang, Meng and Sun, Yanjun and Wang, Zhe},
  BOOKTITLE = {2017 IEEE International Conference on Mechatronics and Automation (ICMA)},
  DATE = {2017},
  DOI = {10.1109/ICMA.2017.8015882},
  KEYWORDS = {Wavelet packets;Electroencephalography;Recurrent neural networks;Feature extraction;Time-frequency analysis;Recognition;EEG;Recurrent Neural Network;Wavelet Packet Transform;subject-based feature},
  PAGES = {584--589},
  TITLE = {The novel recognition method with Optimal Wavelet Packet and LSTM based Recurrent Neural Network},
}


@INPROCEEDINGS{pruning8794944,
  AUTHOR = {Li, Lianqiang and Zhu, Jie and Sun, Ming-Ting},
  BOOKTITLE = {2019 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
  DATE = {2019},
  DOI = {10.1109/ICMEW.2019.00-68},
  KEYWORDS = {Network pruning,filter-level,deep learning},
  PAGES = {312--317},
  TITLE = {Deep Learning Based Method for Pruning Deep Neural Networks},
}

@INPROCEEDINGS{Yang_CVPR,
  AUTHOR = {Yang, Jiwei and Shen, Xu and Xing, Jun and Tian, Xinmei and Li, Houqiang and Deng, Bing and Huang, Jianqiang and Hua, Xian-sheng},
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  DATE = {2019-06},
  TITLE = {Quantization Networks},
}

@INPROCEEDINGS{NEURIPS2021_376c6b9f,
  AUTHOR = {Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and Alemi, Alexander A and Wilson, Andrew G},
  EDITOR = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  PUBLISHER = {Curran Associates, Inc.},
  URL = {https://proceedings.neurips.cc/paper_files/paper/2021/file/376c6b9ff3bedbbea56751a84fffc10c-Paper.pdf},
  BOOKTITLE = {Advances in Neural Information Processing Systems},
  DATE = {2021},
  PAGES = {6906--6919},
  TITLE = {Does Knowledge Distillation Really Work?},
  VOLUME = {34},
}



### EEG begin

@article{hwang_improving_2023,
	title = {Improving {Multi}-{Class} {Motor} {Imagery} {EEG} {Classification} {Using} {Overlapping} {Sliding} {Window} and {Deep} {Learning} {Model}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/12/5/1186},
	doi = {10.3390/electronics12051186},
	abstract = {Motor imagery (MI) electroencephalography (EEG) signals are widely used in BCI systems. MI tasks are performed by imagining doing a specific task and classifying MI through EEG signal processing. However, it is a challenging task to classify EEG signals accurately. In this study, we propose a LSTM-based classification framework to enhance classification accuracy of four-class MI signals. To obtain time-varying data of EEG signals, a sliding window technique is used, and an overlapping-band-based FBCSP is applied to extract the subject-specific spatial features. Experimental results on BCI competition IV dataset 2a showed an average accuracy of 97\% and kappa value of 0.95 in all subjects. It is demonstrated that the proposed method outperforms the existing algorithms for classifying the four-class MI EEG, and it also illustrates the robustness on the variability of inter-trial and inter-session of MI data. Furthermore, the extended experimental results for channel selection showed the best performance of classification accuracy when using all twenty-two channels by the proposed method, but an average kappa value of 0.93 was achieved with only seven channels.},
	language = {en},
	number = {5},
	urldate = {2025-03-03},
	journal = {Electronics},
	author = {Hwang, Jeonghee and Park, Soyoung and Chi, Jeonghee},
	month = jan,
	year = {2023},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {EEG classification, FBCSP, LSTM, multi-class motor imagery, overlapping bandpass filter, overlapping window},
	pages = {1186},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\MB2JV7DJ\\Hwang et al. - 2023 - Improving Multi-Class Motor Imagery EEG Classifica.pdf:application/pdf},
}

@inproceedings{zhao_deep_2015,
	address = {Cham},
	title = {Deep {Learning} in the {EEG} {Diagnosis} of {Alzheimer}’s {Disease}},
	isbn = {978-3-319-16628-5},
	doi = {10.1007/978-3-319-16628-5_25},
	abstract = {EEG (electroencephalogram) has a lot of advantages compared to other methods in the analysis of Alzheimer’s disease such as diagnosing Alzheimer’s disease in an early stage. Traditional EEG analysis method needs a lot of artificial works such as calculating coherence between different pair of electrodes. In our work we applied deep learning network in the analysis of EEG data of Alzheimer’s disease to fully use the advantage of the unsupervised feature learning. We studied EEG based deep learning on 15 clinically diagnosed Alzheimer’s disease patients and 15 healthy people. Each person has 16 electrodes. The time domain EEG data of each electrode is cut into 40 data units according to the data size in a period. In our work we first train the deep learning network with 25 data units on each electrode separately and then test with 15 data units to get the accuracy on each electrode. Finally we will combine the learning results on 16 electrodes and train them with SVM and get a final result. We report a 92 \% accuracy after combining 16 electrodes of each person. In order to improve the deep learning model on Alzheimer’s disease with the upcoming new data, we use incremental learning to make full use of the existing data while decrease the expenses on memory space and computing time by replacing the exising data with new data. We report a 0.5 \% improvement in accuracy with incremental learning.},
	language = {en},
	booktitle = {Computer {Vision} - {ACCV} 2014 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Zhao, Yilu and He, Lianghua},
	editor = {Jawahar, C.V. and Shan, Shiguang},
	year = {2015},
	pages = {340--353},
}

@article{xia_novel_2023,
	title = {A novel method for diagnosing {Alzheimer}'s disease using deep pyramid {CNN} based on {EEG} signals},
	volume = {9},
	issn = {2405-8440},
	url = {https://www.sciencedirect.com/science/article/pii/S2405844023020650},
	doi = {10.1016/j.heliyon.2023.e14858},
	abstract = {Background
The diagnosis of Alzheimer's disease (AD) using electroencephalography (EEG) has garnered more attention recently.
New methods
In this paper, we present a novel approach for the diagnosis of AD, in terms of classifying the resting-state EEG of AD, mild cognitive impairment (MCI), and healthy control (HC). To overcome the hurdles of limited data available and the over-fitting problem of the deep learning models, we studied overlapping sliding windows to augment the one-dimensional EEG data of 100 subjects (including 49 AD subjects, 37 MCI subjects and 14 HC subjects). After constructing the appropriate dataset, the modified DPCNN was used to classify the augmented EEG. Furthermore, the model performance was evaluated by 5 times of 5-fold cross-validation and the confusion matrix has been obtained.
Results
The average accuracy rate of the model for classifying AD, MCI, and HC is 97.10\%, and the F1 score of the three-class classification model is 97.11\%, which further proves the model's excellent performance.
Conclusions
Therefore, the DPCNN proposed in this paper can accurately classify the one-dimensional EEG of AD and is worthy of reference for the diagnosis of the disease.},
	number = {4},
	urldate = {2025-03-03},
	journal = {Heliyon},
	author = {Xia, Wei and Zhang, Ran and Zhang, Xiao and Usman, Muhammad},
	month = apr,
	year = {2023},
	keywords = {Alzheimer's disease, Deep learning, Deep pyramid convolutional neural network, Electroencephalography, Mild cognitive impairment},
	pages = {e14858},
	file = {Full Text:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\A8ABZH2P\\Xia et al. - 2023 - A novel method for diagnosing Alzheimer's disease .pdf:application/pdf;ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\NBSEZILK\\S2405844023020650.html:text/html},
}

@article{hermawan_multi_2024,
	title = {A {Multi} {Representation} {Deep} {Learning} {Approach} for {Epileptic} {Seizure} {Detection}},
	volume = {5},
	copyright = {Copyright (c) 2024 Arya Tandy Hermawan, Ilham Ari Elbaith Zaeni, Aji Prasetya Wibawa, Gunawan Gunawan, William Hartanto Hendrawan, Yosi Kristian},
	issn = {2715-5072},
	url = {https://journal.umy.ac.id/index.php/jrc/article/view/20870},
	doi = {10.18196/jrc.v5i1.20870},
	abstract = {Epileptic seizures, unpredictable in nature and potentially dangerous during activities like driving, pose significant risks to individual and public safety. Traditional diagnostic methods, which involve labour-intensive manual feature extraction from Electroencephalography (EEG) data, are being supplanted by automated deep learning frameworks. This paper introduces an automated epileptic seizure detection framework utilizing deep learning to bypass manual feature extraction. Our framework incorporates detailed pre-processing techniques: normalization via L2 normalization, filtering with an 80 Hz and 0,5 Hz Butterworth low-pass and high-pass filter, and a 50 Hz IIR Notch filter, channel selection based on standard deviation calculations and Mutual Information algorithm, and frequency domain transformation using FFT or STFT with Hann windows and 50\% hop. We evaluated on two datasets: the first comprising 4 canines and 8 patients with 2.299 ictal, 23.445 interictal, and 32.915 test data, 400-5000Hz sampling rate across 16-72 channels; the second dataset, intended for testing, 733 icatal, 4.314 interictal, and 1908 test data, each 10 minutes long, recorded at 400Hz across 16 channels. Three deep learning architectures were assessed: CNN, LSTM, and a hybrid CNN-LSTM model-stems from their demonstrated efficacy in handling the complex nature of EEG data. Each model offers unique strengths, with the CNN excelling in spatial feature extraction, LSTM in temporal dynamics, and the hybrid model combining these advantages.  The CNN model, comprising 31 layers, yielded highest accuracy, achieving 91\% on the first dataset (precision 92\%, recall 91\%, F1-score 91\%) and 82\% on the second dataset using a 30-second threshold. This threshold was chosen for its clinical relevance. The research advances epileptic seizure detection using deep learning, indicating a promising direction for future medical technology. Future work will focus on expanding dataset diversity and refining methodologies to build upon these foundational results.},
	language = {en},
	number = {1},
	urldate = {2025-03-03},
	journal = {Journal of Robotics and Control (JRC)},
	author = {Hermawan, Arya Tandy and Zaeni, Ilham Ari Elbaith and Wibawa, Aji Prasetya and Gunawan, Gunawan and Hendrawan, William Hartanto and Kristian, Yosi},
	month = jan,
	year = {2024},
	note = {Number: 1},
	keywords = {Spectrogram.},
	pages = {187--204},
}

@article{abdulwahhab_detection_2024,
	title = {Detection of epileptic seizure using {EEG} signals analysis based on deep learning techniques},
	volume = {181},
	issn = {0960-0779},
	url = {https://www.sciencedirect.com/science/article/pii/S0960077924002522},
	doi = {10.1016/j.chaos.2024.114700},
	abstract = {The brain neurons' electrical activities represented by Electroencephalogram (EEG) signals are the most common data for diagnosing Epilepsy seizure, which is considered a chronic nervous disorder that cannot be controlled medically using surgical operation or medications with more than 40 \% of Epilepsy seizure case. With the progress and development of artificial intelligence and deep learning techniques, it becomes possible to detect these seizures over the observation of the non-stationary-dynamic EEG signals, which contain important information about the mental state of patients. This paper provides a concerted deep machine learning model consisting of two simultaneous techniques detecting the activity of epileptic seizures using EEG signals. The time-frequency image of EEG waves and EEG raw waves are used as input components for the convolution neural network (CNN) and recurrent neural network (RNN) with long- and short-term memory (LSTM). Two processing signal methods have been used, Short-Time Fourier Transform (STFT) and Continuous Wavelet Transformation (CWT), have been used for generating spectrogram and scalogram images with sizes of 77 × 75 and 32 × 32, respectively. The experimental results showed a detection accuracy of 99.57 \%, 99.57 \% using CWT Scalograms, and 99.26 \%, 97.12 \% using STFT spectrograms as CNN input for the Bonn University dataset and the CHB-MIT dataset, respectively. Thus, the proposed models provide the ability to detect epileptic seizures with high success compared to previous studies.},
	urldate = {2025-03-03},
	journal = {Chaos, Solitons \& Fractals},
	author = {Abdulwahhab, Ali H. and Abdulaal, Alaa Hussein and Thary Al-Ghrairi, Assad H. and Mohammed, Ali Abdulwahhab and Valizadeh, Morteza},
	month = apr,
	year = {2024},
	keywords = {Convolutional neural network, Deep learning, EEG, Electroencephalogram, Epileptic seizure, Recurrent neural network},
	pages = {114700},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\VBJHUC4V\\S0960077924002522.html:text/html},
}

@article{pandey_subject_2022,
	title = {Subject independent emotion recognition from {EEG} using {VMD} and deep learning},
	volume = {34},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157819309991},
	doi = {10.1016/j.jksuci.2019.11.003},
	abstract = {Emotion recognition from Electroencephalography (EEG) is proved to be a good choice as it cannot be mimicked like speech signals or facial expressions. EEG signals of emotions are not unique and it varies from person to person as each one has different emotional responses to the same stimuli. Thus EEG signals are subject dependent and proved to be effective for subject dependent emotion recognition. However, subject independent emotion recognition plays an important role in situations like emotion recognition from paralyzed or burnt face, where EEG of emotions of the subjects before the incidents are not available to build the emotion recognition model. Hence there is a need to identify common EEG patterns corresponds to each emotion independent of the subjects. In this paper, a subject independent emotion recognition technique is proposed from EEG signals using Variational Mode Decomposition (VMD) as a feature extraction technique and Deep Neural Network as the classifier. The performance evaluation of the proposed method with the benchmark DEAP dataset shows that the combination of VMD and Deep Neural Network performs better compared to the state of the art techniques in subject-independent emotion recognition from EEG.},
	number = {5},
	urldate = {2025-03-03},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Pandey, Pallavi and Seeja, K. R.},
	month = may,
	year = {2022},
	keywords = {Affective computing, Deep Neural Network, Intrinsic-mode functions, Valence-Arousal model, Variational Mode Decomposition},
	pages = {1730--1738},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\2A5Q45A4\\S1319157819309991.html:text/html},
}

@article{hassouneh_development_2020,
	title = {Development of a {Real}-{Time} {Emotion} {Recognition} {System} {Using} {Facial} {Expressions} and {EEG} based on machine learning and deep neural network methods},
	volume = {20},
	issn = {2352-9148},
	url = {https://www.sciencedirect.com/science/article/pii/S235291482030201X},
	doi = {10.1016/j.imu.2020.100372},
	abstract = {Real-time emotion recognition has been an active field of research over the past several decades. This work aims to classify physically disabled people (deaf, dumb, and bedridden) and Autism children's emotional expressions based on facial landmarks and electroencephalograph (EEG) signals using a convolutional neural network (CNN) and long short-term memory (LSTM) classifiers by developing an algorithm for real-time emotion recognition using virtual markers through an optical flow algorithm that works effectively in uneven lightning and subject head rotation (up to 25°), different backgrounds, and various skin tones. Six facial emotions (happiness, sadness, anger, fear, disgust, and surprise) are collected using ten virtual markers. Fifty-five undergraduate students (35 male and 25 female) with a mean age of 22.9 years voluntarily participated in the experiment for facial emotion recognition. Nineteen undergraduate students volunteered to collect EEG signals. Initially, Haar-like features are used for facial and eye detection. Later, virtual markers are placed on defined locations on the subject's face based on a facial action coding system using the mathematical model approach, and the markers are tracked using the Lucas-Kande optical flow algorithm. The distance between the center of the subject's face and each marker position is used as a feature for facial expression classification. This distance feature is statistically validated using a one-way analysis of variance with a significance level of p {\textless} 0.01. Additionally, the fourteen signals collected from the EEG signal reader (EPOC+) channels are used as features for emotional classification using EEG signals. Finally, the features are cross-validated using fivefold cross-validation and given to the LSTM and CNN classifiers. We achieved a maximum recognition rate of 99.81\% using CNN for emotion detection using facial landmarks. However, the maximum recognition rate achieved using the LSTM classifier is 87.25\% for emotion detection using EEG signals.},
	urldate = {2025-03-03},
	journal = {Informatics in Medicine Unlocked},
	author = {Hassouneh, Aya and Mutawa, A. M. and Murugappan, M.},
	month = jan,
	year = {2020},
	keywords = {EEG emotion Detection, Face emotion recognition, LSTM, Virtual markers},
	pages = {100372},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\9NGXFL5L\\S235291482030201X.html:text/html},
}

@article{pan_multimodal_2024,
	title = {Multimodal {Emotion} {Recognition} {Based} on {Facial} {Expressions}, {Speech}, and {EEG}},
	volume = {5},
	issn = {2644-1276},
	url = {https://ieeexplore.ieee.org/document/10026861},
	doi = {10.1109/OJEMB.2023.3240280},
	abstract = {Goal: As an essential human-machine interactive task, emotion recognition has become an emerging area over the decades. Although previous attempts to classify emotions have achieved high performance, several challenges remain open: 1) How to effectively recognize emotions using different modalities remains challenging. 2) Due to the increasing amount of computing power required for deep learning, how to provide real-time detection and improve the robustness of deep neural networks is important. Method: In this paper, we propose a deep learning-based multimodal emotion recognition (MER) called Deep-Emotion, which can adaptively integrate the most discriminating features from facial expressions, speech, and electroencephalogram (EEG) to improve the performance of the MER. Specifically, the proposed Deep-Emotion framework consists of three branches, i.e., the facial branch, speech branch, and EEG branch. Correspondingly, the facial branch uses the improved GhostNet neural network proposed in this paper for feature extraction, which effectively alleviates the overfitting phenomenon in the training process and improves the classification accuracy compared with the original GhostNet network. For work on the speech branch, this paper proposes a lightweight fully convolutional neural network (LFCNN) for the efficient extraction of speech emotion features. Regarding the study of EEG branches, we proposed a tree-like LSTM (tLSTM) model capable of fusing multi-stage features for EEG emotion feature extraction. Finally, we adopted the strategy of decision-level fusion to integrate the recognition results of the above three modes, resulting in more comprehensive and accurate performance. Result and Conclusions: Extensive experiments on the CK+, EMO-DB, and MAHNOB-HCI datasets have demonstrated the advanced nature of the Deep-Emotion method proposed in this paper, as well as the feasibility and superiority of the MER approach.},
	urldate = {2025-03-03},
	journal = {IEEE Open Journal of Engineering in Medicine and Biology},
	author = {Pan, Jiahui and Fang, Weijie and Zhang, Zhihang and Chen, Bingzhi and Zhang, Zheng and Wang, Shuihua},
	year = {2024},
	note = {Conference Name: IEEE Open Journal of Engineering in Medicine and Biology},
	keywords = {Brain modeling, Convolution, Deep learning, electroencephalogram, Electroencephalography, Emotion recognition, facial expressions, Feature extraction, Multimodal emotion recognition, speech, Speech recognition},
	pages = {396--403},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\TTKFDSFU\\Pan et al. - 2024 - Multimodal Emotion Recognition Based on Facial Exp.pdf:application/pdf;IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\TTU4D3M4\\10026861.html:text/html},
}

@article{wang_multimodal_2023,
	title = {Multimodal {Emotion} {Recognition} {From} {EEG} {Signals} and {Facial} {Expressions}},
	volume = {11},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10089483},
	doi = {10.1109/ACCESS.2023.3263670},
	abstract = {Emotion recognition has attracted attention in recent years. It is widely used in healthcare, teaching, human-computer interaction, and other fields. Human emotional features are often used to recognize different emotions. Currently, there is more and more research on multimodal emotion recognition based on the fusion of multiple features. This paper proposes a deep learning model for multimodal emotion recognition based on the fusion of electroencephalogram (EEG) signals and facial expressions to achieve an excellent classification effect. First, a pre-trained convolution neural network (CNN) is used to extract the facial features from the facial expressions. Next, the attention mechanism is introduced to extract more critical facial frame features. Then, we apply CNNs to extract spatial features from original EEG signals, which use a local convolution kernel and a global convolution kernel to learn the features of left and right hemispheres channels and all EEG channels. After feature-level fusion, the fusion features of the facial expression features and EEG features are fed into the classifier for emotion recognition. This paper conducted experiments on the DEAP and MAHNOB-HCI datasets to evaluate the performance of the proposed model. The accuracy of valence dimension classification is 96.63\%, and arousal dimension classification is 97.15\% on the DEAP dataset, while 96.69\% and 96.26\% on the MAHNOB-HCI dataset. The experimental results show that the proposed model can effectively recognize emotions.},
	urldate = {2025-03-03},
	journal = {IEEE Access},
	author = {Wang, Shuai and Qu, Jingzi and Zhang, Yong and Zhang, Yidie},
	year = {2023},
	note = {Conference Name: IEEE Access},
	keywords = {attention mechanism, Brain modeling, Convolution, deep learning, Deep learning, EEG, Electroencephalography, Emotion recognition, facial expressions, Facial features, Feature extraction, Multimodal emotion recognition},
	pages = {33061--33068},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\6B4N2UUV\\Wang et al. - 2023 - Multimodal Emotion Recognition From EEG Signals an.pdf:application/pdf;IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\9WNXBH7X\\10089483.html:text/html},
}

@misc{corley_deep_2025,
	title = {Deep {EEG} {Super}-{Resolution}: {Upsampling} {EEG} {Spatial} {Resolution} with {Generative} {Adversarial} {Networks}},
	shorttitle = {Deep {EEG} {Super}-{Resolution}},
	url = {http://arxiv.org/abs/2502.08803},
	doi = {10.48550/arXiv.2502.08803},
	abstract = {Electroencephalography (EEG) activity contains a wealth of information about what is happening within the human brain. Recording more of this data has the potential to unlock endless future applications. However, the cost of EEG hardware is increasingly expensive based upon the number of EEG channels being recorded simultaneously. We combat this problem in this paper by proposing a novel deep EEG super-resolution (SR) approach based on Generative Adversarial Networks (GANs). This approach can produce high spatial resolution EEG data from low resolution samples, by generating channel-wise upsampled data to effectively interpolate numerous missing channels, thus reducing the need for expensive EEG equipment. We tested the performance using an EEG dataset from a mental imagery task. Our proposed GAN model provided 10{\textasciicircum}4 fold and 10{\textasciicircum}2 fold reduction in mean-squared error (MSE) and mean-absolute error (MAE), respectively, over the baseline bicubic interpolation method. We further validate our method by training a classifier on the original classification task, which displayed minimal loss in accuracy while using the super-resolved data. The proposed SR EEG by GAN is a promising approach to improve the spatial resolution of low density EEG headsets.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Corley, Isaac and Huang, Yufei},
	month = feb,
	year = {2025},
	note = {arXiv:2502.08803 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\VD4AMUZQ\\Corley and Huang - 2025 - Deep EEG Super-Resolution Upsampling EEG Spatial .pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\YMTSCIMB\\2502.html:text/html},
}

@article{song_eeggan-net_2024,
	title = {{EEGGAN}-{Net}: enhancing {EEG} signal classification through data augmentation},
	volume = {18},
	issn = {1662-5161},
	shorttitle = {{EEGGAN}-{Net}},
	url = {https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2024.1430086/full},
	doi = {10.3389/fnhum.2024.1430086},
	abstract = {{\textless}sec{\textgreater}{\textless}title{\textgreater}Background{\textless}/title{\textgreater}{\textless}p{\textgreater}Emerging brain-computer interface (BCI) technology holds promising potential to enhance the quality of life for individuals with disabilities. Nevertheless, the constrained accuracy of electroencephalography (EEG) signal classification poses numerous hurdles in real-world applications.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Methods{\textless}/title{\textgreater}{\textless}p{\textgreater}In response to this predicament, we introduce a novel EEG signal classification model termed EEGGAN-Net, leveraging a data augmentation framework. By incorporating Conditional Generative Adversarial Network (CGAN) data augmentation, a cropped training strategy and a Squeeze-and-Excitation (SE) attention mechanism, EEGGAN-Net adeptly assimilates crucial features from the data, consequently enhancing classification efficacy across diverse BCI tasks.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Results{\textless}/title{\textgreater}{\textless}p{\textgreater}The EEGGAN-Net model exhibits notable performance metrics on the BCI Competition IV-2a and IV-2b datasets. Specifically, it achieves a classification accuracy of 81.3\% with a kappa value of 0.751 on the IV-2a dataset, and a classification accuracy of 90.3\% with a kappa value of 0.79 on the IV-2b dataset. Remarkably, these results surpass those of four other CNN-based decoding models.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Conclusions{\textless}/title{\textgreater}{\textless}p{\textgreater}In conclusion, the amalgamation of data augmentation and attention mechanisms proves instrumental in acquiring generalized features from EEG signals, ultimately elevating the overall proficiency of EEG signal classification.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}},
	language = {English},
	urldate = {2025-03-03},
	journal = {Frontiers in Human Neuroscience},
	author = {Song, Jiuxiang and Zhai, Qiang and Wang, Chuang and Liu, Jizhong},
	month = jun,
	year = {2024},
	note = {Publisher: Frontiers},
	keywords = {Brain-Computer Interfaces, Conditional generative adversarial network, cropped training, Squeeze-and-excitation attention, Squeeze-and-Excitation Attention EEGGAN-Net: Enhancing EEG Signal Classification through Data Augmentation Brain-computer interfaces},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\BGV6XSMT\\Song et al. - 2024 - EEGGAN-Net enhancing EEG signal classification th.pdf:application/pdf},
}

@article{cai_dhct-gan_2025,
	title = {{DHCT}-{GAN}: {Improving} {EEG} {Signal} {Quality} with a {Dual}-{Branch} {Hybrid} {CNN}–{Transformer} {Network}},
	volume = {25},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {{DHCT}-{GAN}},
	url = {https://www.mdpi.com/1424-8220/25/1/231},
	doi = {10.3390/s25010231},
	abstract = {Electroencephalogram (EEG) signals are important bioelectrical signals widely used in brain activity studies, cognitive mechanism research, and the diagnosis and treatment of neurological disorders. However, EEG signals are often influenced by various physiological artifacts, which can significantly affect data analysis and diagnosis. Recently, deep learning-based EEG denoising methods have exhibited unique advantages over traditional methods. Most existing methods mainly focus on identifying the characteristics of clean EEG signals to facilitate artifact removal; however, the potential to integrate cross-disciplinary knowledge, such as insights from artifact research, remains an area that requires further exploration. In this study, we developed DHCT-GAN, a new EEG denoising model, using a dual-branch hybrid network architecture. This model independently learns features from both clean EEG signals and artifact signals, then fuses this information through an adaptive gating network to generate denoised EEG signals that accurately preserve EEG signal features while effectively removing artifacts. We evaluated DHCT-GAN’s performance through waveform analysis, power spectral density (PSD) analysis, and six performance metrics. The results demonstrate that DHCT-GAN significantly outperforms recent state-of-the-art networks in removing various artifacts. Furthermore, ablation experiments revealed that the hybrid model surpasses single-branch models in artifact removal, underscoring the crucial role of artifact knowledge constraints in improving denoising effectiveness.},
	language = {en},
	number = {1},
	urldate = {2025-03-03},
	journal = {Sensors},
	author = {Cai, Yinan and Meng, Zhao and Huang, Dian},
	month = jan,
	year = {2025},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {denoising, electroencephalogram (EEG), generative adversarial network (GAN), transformer},
	pages = {231},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\58BUM5UC\\Cai et al. - 2025 - DHCT-GAN Improving EEG Signal Quality with a Dual.pdf:application/pdf},
}

### EEG end

@ARTICLE{ozbayoglu2020deep,
  AUTHOR = {Ozbayoglu, Ahmet Murat and Gudelek, Mehmet Ugur and Sezer, Omer Berat},
  PUBLISHER = {Elsevier},
  DATE = {2020},
  JOURNALTITLE = {Applied soft computing},
  PAGES = {106384},
  TITLE = {Deep learning for financial applications: A survey},
  VOLUME = {93},
}

@ARTICLE{shen2021new,
  AUTHOR = {Shen, Feng and Zhao, Xingchao and Kou, Gang and Alsaadi, Fawaz E},
  PUBLISHER = {Elsevier},
  DATE = {2021},
  JOURNALTITLE = {Applied Soft Computing},
  PAGES = {106852},
  TITLE = {A new deep learning ensemble credit risk evaluation model with an improved synthetic minority oversampling technique},
  VOLUME = {98},
}

@ARTICLE{wang2020portfolio,
  AUTHOR = {Wang, Wuyu and Li, Weizi and Zhang, Ning and Liu, Kecheng},
  PUBLISHER = {Elsevier},
  DATE = {2020},
  JOURNALTITLE = {Expert Systems with Applications},
  PAGES = {113042},
  TITLE = {Portfolio formation with preselection using deep learning from long-term financial data},
  VOLUME = {143},
}

@ARTICLE{chen2024deep,
  AUTHOR = {Chen, Luyang and Pelger, Markus and Zhu, Jason},
  PUBLISHER = {INFORMS},
  DATE = {2024},
  JOURNALTITLE = {Management Science},
  NUMBER = {2},
  PAGES = {714--750},
  TITLE = {Deep learning in asset pricing},
  VOLUME = {70},
}

@INPROCEEDINGS{ahnouch2023model,
  AUTHOR = {Ahnouch, Mohammed and Elaachak, Lotfi and Ghadi, Abderrahim},
  ORGANIZATION = {Springer},
  BOOKTITLE = {The Proceedings of the International Conference on Smart City Applications},
  DATE = {2023},
  PAGES = {155--165},
  TITLE = {Model Risk in Financial Derivatives and The Transformative Impact of Deep Learning: A Systematic Review},
}

@ARTICLE{abedin2021deep,
  AUTHOR = {Abedin, Mohammad Zoynul and Moon, Mahmudul Hasan and Hassan, M Kabir and Hajek, Petr},
  PUBLISHER = {Springer},
  DATE = {2021},
  JOURNALTITLE = {Annals of Operations Research},
  PAGES = {1--52},
  TITLE = {Deep learning-based exchange rate prediction during the COVID-19 pandemic},
}

@ARTICLE{SUN2020101160,
  AUTHOR = {Sun, Shaolong and Wang, Shouyang and Wei, Yunjie},
  URL = {https://www.sciencedirect.com/science/article/pii/S1474034620301312},
  DATE = {2020},
  DOI = {https://doi.org/10.1016/j.aei.2020.101160},
  ISSN = {1474-0346},
  JOURNALTITLE = {Advanced Engineering Informatics},
  PAGES = {101160},
  TITLE = {A new ensemble deep learning approach for exchange rates forecasting and trading},
  VOLUME = {46},
}

@INPROCEEDINGS{eapen2019novel,
  AUTHOR = {Eapen, Jithin and Bein, Doina and Verma, Abhishek},
  ORGANIZATION = {IEEE},
  BOOKTITLE = {2019 IEEE 9th annual computing and communication workshop and conference (CCWC)},
  DATE = {2019},
  PAGES = {0264--0270},
  TITLE = {Novel deep learning model with CNN and bi-directional LSTM for improved stock market index prediction},
}

@INPROCEEDINGS{cai2018financial,
  AUTHOR = {Cai, Shubin and Feng, Xiaogang and Deng, Ziwei and Ming, Zhong and Shan, Zhiguang},
  ORGANIZATION = {Springer},
  BOOKTITLE = {Smart Computing and Communication: Third International Conference, SmartCom 2018, Tokyo, Japan, December 10--12, 2018, Proceedings 3},
  DATE = {2018},
  PAGES = {366--375},
  TITLE = {Financial news quantization and stock market forecast research based on CNN and LSTM},
}

@INPROCEEDINGS{gudelek2017deep,
  AUTHOR = {Gudelek, M Ugur and Boluk, S Arda and Ozbayoglu, A Murat},
  ORGANIZATION = {IEEE},
  BOOKTITLE = {2017 IEEE symposium series on computational intelligence (SSCI)},
  DATE = {2017},
  PAGES = {1--8},
  TITLE = {A deep learning based stock trading model with 2-D CNN trend detection},
}

@ARTICLE{nikou2019stock,
  AUTHOR = {Nikou, Mahla and Mansourfar, Gholamreza and Bagherzadeh, Jamshid},
  PUBLISHER = {Wiley Online Library},
  DATE = {2019},
  JOURNALTITLE = {Intelligent Systems in Accounting, Finance and Management},
  NUMBER = {4},
  PAGES = {164--174},
  TITLE = {Stock price prediction using DEEP learning algorithm and its comparison with machine learning algorithms},
  VOLUME = {26},
}

@ARTICLE{adversarial2018,
  AUTHOR = {Akhtar, Naveed and Mian, Ajmal},
  DATE = {2018},
  DOI = {10.1109/ACCESS.2018.2807385},
  JOURNALTITLE = {IEEE Access},
  PAGES = {14410--14430},
  TITLE = {Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey},
  VOLUME = {6},
}

@ARTICLE{szegedy2013intriguing,
  AUTHOR = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  DATE = {2013},
  JOURNALTITLE = {arXiv preprint arXiv:1312.6199},
  TITLE = {Intriguing properties of neural networks},
}

@ARTICLE{tabacof2016adversarial,
  AUTHOR = {Tabacof, Pedro and Tavares, Julia and Valle, Eduardo},
  DATE = {2016},
  JOURNALTITLE = {arXiv preprint arXiv:1612.00155},
  TITLE = {Adversarial images for variational autoencoders},
}

@ARTICLE{zhang2020adversarial,
  AUTHOR = {Zhang, Wei Emma and Sheng, Quan Z and Alhazmi, Ahoud and Li, Chenliang},
  PUBLISHER = {ACM New York, NY, USA},
  DATE = {2020},
  JOURNALTITLE = {ACM Transactions on Intelligent Systems and Technology (TIST)},
  NUMBER = {3},
  PAGES = {1--41},
  TITLE = {Adversarial attacks on deep-learning models in natural language processing: A survey},
  VOLUME = {11},
}

@INPROCEEDINGS{jiang2019black,
  AUTHOR = {Jiang, Linxi and Ma, Xingjun and Chen, Shaoxiang and Bailey, James and Jiang, Yu-Gang},
  BOOKTITLE = {Proceedings of the 27th ACM International Conference on Multimedia},
  DATE = {2019},
  PAGES = {864--872},
  TITLE = {Black-box adversarial attacks on video recognition models},
}

@ARTICLE{esmaeilpour2019robust,
  AUTHOR = {Esmaeilpour, Mohammad and Cardinal, Patrick and Koerich, Alessandro Lameiras},
  PUBLISHER = {IEEE},
  DATE = {2019},
  JOURNALTITLE = {IEEE Transactions on information forensics and security},
  PAGES = {2147--2159},
  TITLE = {A robust approach for securing audio classification against adversarial attacks},
  VOLUME = {15},
}

@ARTICLE{wang2024class,
  AUTHOR = {Wang, Xiaofeng and Zhang, Ying and Bai, Ningning and Yu, Qinhua and Wang, Qin},
  PUBLISHER = {Elsevier},
  DATE = {2024},
  JOURNALTITLE = {Expert Systems with Applications},
  PAGES = {122192},
  TITLE = {Class-imbalanced time series anomaly detection method based on cost-sensitive hybrid network},
  VOLUME = {238},
}

@ARTICLE{maldonado2023owadapt,
  AUTHOR = {Maldonado, Sebastián and Vairetti, Carla and Jara, Katherine and Carrasco, Miguel and López, Julio},
  PUBLISHER = {Elsevier},
  DATE = {2023},
  JOURNALTITLE = {Knowledge-Based Systems},
  PAGES = {111022},
  TITLE = {OWAdapt: An adaptive loss function for deep learning using OWA operators},
  VOLUME = {280},
}

@ARTICLE{zhu2024irda,
  AUTHOR = {Zhu, Weiyao and Wu, Ou and Yang, Nan},
  PUBLISHER = {Elsevier},
  DATE = {2024},
  JOURNALTITLE = {Information Sciences},
  PAGES = {120873},
  TITLE = {IRDA: Implicit Data Augmentation for Deep Imbalanced Regression},
}

@ARTICLE{belhaouari2024oversampling,
  AUTHOR = {Belhaouari, Samir Brahim and Islam, Ashhadul and Kassoul, Khelil and Al-Fuqaha, Ala and Bouzerdoum, Abdesselam},
  PUBLISHER = {Elsevier},
  DATE = {2024},
  JOURNALTITLE = {Expert Systems with Applications},
  PAGES = {124118},
  TITLE = {Oversampling techniques for imbalanced data in regression},
  VOLUME = {252},
}

@ARTICLE{ircio2023minimum,
  AUTHOR = {Ircio, Josu and Lojo, Aizea and Mori, Usue and Malinowski, Simon and Lozano, Jose A},
  PUBLISHER = {IEEE},
  DATE = {2023},
  JOURNALTITLE = {IEEE Transactions on Knowledge and Data Engineering},
  NUMBER = {10},
  PAGES = {10024--10034},
  TITLE = {Minimum recall-based loss function for imbalanced time series classification},
  VOLUME = {35},
}

@ARTICLE{moles2024exploring,
  AUTHOR = {Moles, Luis and Andres, Alain and Echegaray, Goretti and Boto, Fernando},
  PUBLISHER = {MDPI},
  DATE = {2024},
  JOURNALTITLE = {Mathematics},
  NUMBER = {12},
  PAGES = {1898},
  TITLE = {Exploring Data Augmentation and Active Learning Benefits in Imbalanced Datasets},
  VOLUME = {12},
}

@ARTICLE{zhu2024sfpl,
  AUTHOR = {Zhu, Yongbei and Wang, Shuo and Yu, He and Li, Weimin and Tian, Jie},
  PUBLISHER = {Elsevier},
  DATE = {2024},
  JOURNALTITLE = {Medical Image Analysis},
  PAGES = {103281},
  TITLE = {SFPL: Sample-specific fine-grained prototype learning for imbalanced medical image classification},
  VOLUME = {97},
}


@misc{noauthor_imagenet_nodate,
	title = {{ImageNet}},
	url = {https://www.image-net.org/},
	urldate = {2025-02-19},
	file = {ImageNet:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\FJTIWT7D\\www.image-net.org.html:text/html},
}

@misc{noauthor_cifar-10_nodate,
	title = {{CIFAR}-10 and {CIFAR}-100 datasets},
	url = {https://www.cs.toronto.edu/~kriz/cifar.html},
	urldate = {2025-02-25},
	file = {CIFAR-10 and CIFAR-100 datasets:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\ZCXQEST5\\cifar.html:text/html},
}


@article{van_houdt_review_2020,
	title = {A review on the long short-term memory model},
	volume = {53},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-020-09838-1},
	doi = {10.1007/s10462-020-09838-1},
	abstract = {Long short-term memory (LSTM) has transformed both machine learning and neurocomputing fields. According to several online sources, this model has improved Google’s speech recognition, greatly improved machine translations on Google Translate, and the answers of Amazon’s Alexa. This neural system is also employed by Facebook, reaching over 4 billion LSTM-based translations per day as of 2017. Interestingly, recurrent neural networks had shown a rather discrete performance until LSTM showed up. One reason for the success of this recurrent network lies in its ability to handle the exploding/vanishing gradient problem, which stands as a difficult issue to be circumvented when training recurrent or very deep neural networks. In this paper, we present a comprehensive review that covers LSTM’s formulation and training, relevant applications reported in the literature and code resources implementing this model for a toy example.},
	language = {en},
	number = {8},
	urldate = {2025-02-23},
	journal = {Artificial Intelligence Review},
	author = {Van Houdt, Greg and Mosquera, Carlos and Nápoles, Gonzalo},
	month = dec,
	year = {2020},
	keywords = {Artificial Intelligence, Deep learning, Long short-term memory, Recurrent neural networks, Vanishing/exploding gradient},
	pages = {5929--5955},
	file = {Submitted Version:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\3A6E5NG9\\Van Houdt et al. - 2020 - A review on the long short-term memory model.pdf:application/pdf},
}

@article{chougrad_deep_2018,
	title = {Deep {Convolutional} {Neural} {Networks} for breast cancer screening},
	volume = {157},
	issn = {0169-2607},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260717301451},
	doi = {10.1016/j.cmpb.2018.01.011},
	abstract = {Background and objective
Radiologists often have a hard time classifying mammography mass lesions which leads to unnecessary breast biopsies to remove suspicions and this ends up adding exorbitant expenses to an already burdened patient and health care system.
Methods
In this paper we developed a Computer-aided Diagnosis (CAD) system based on deep Convolutional Neural Networks (CNN) that aims to help the radiologist classify mammography mass lesions. Deep learning usually requires large datasets to train networks of a certain depth from scratch. Transfer learning is an effective method to deal with relatively small datasets as in the case of medical images, although it can be tricky as we can easily start overfitting.
Results
In this work, we explore the importance of transfer learning and we experimentally determine the best fine-tuning strategy to adopt when training a CNN model. We were able to successfully fine-tune some of the recent, most powerful CNNs and achieved better results compared to other state-of-the-art methods which classified the same public datasets. For instance we achieved 97.35\% accuracy and 0.98 AUC on the DDSM database, 95.50\% accuracy and 0.97 AUC on the INbreast database and 96.67\% accuracy and 0.96 AUC on the BCDR database. Furthermore, after pre-processing and normalizing all the extracted Regions of Interest (ROIs) from the full mammograms, we merged all the datasets to build one large set of images and used it to fine-tune our CNNs. The CNN model which achieved the best results, a 98.94\% accuracy, was used as a baseline to build the Breast Cancer Screening Framework. To evaluate the proposed CAD system and its efficiency to classify new images, we tested it on an independent database (MIAS) and got 98.23\% accuracy and 0.99 AUC.
Conclusion
The results obtained demonstrate that the proposed framework is performant and can indeed be used to predict if the mass lesions are benign or malignant.},
	urldate = {2025-02-19},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Chougrad, Hiba and Zouaki, Hamid and Alheyane, Omar},
	month = apr,
	year = {2018},
	keywords = {Deep learning, Convolutional Neural Network, Breast cancer, Breast mass lesion classification, Computer-aided Diagnosis, Transfer learning},
	pages = {19--30},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\3WYYU7MY\\S0169260717301451.html:text/html},
}

@article{zhao_novel_2020,
	title = {A novel {U}-{Net} approach to segment the cardiac chamber in magnetic resonance images with ghost artifacts},
	volume = {196},
	issn = {0169-2607},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260720314565},
	doi = {10.1016/j.cmpb.2020.105623},
	abstract = {Objective
We propose a robust technique for segmenting magnetic resonance images of post-atrial septal occlusion intervention in the cardiac chamber.
Methods
A variant of the U-Net architecture is used to perform atrial segmentation via a deep convolutional neural network, and we compare performance with the Kass snake model. It can be used to determine the surgical success of atrial septal occlusion (ASO) pre- and post- the implantation of the septal occluder, which is based on the volume restoration of the right atria (RA) and left atria (LA).
Results
The method was evaluated on a test dataset containing 550 two-dimensional image slices, outperforming conventional active contouring regarding the Dice similarity coefficient, Jaccard index, and Hausdorff distance, and achieving segmentation in the presence of ghost artifacts that occlude the atrium outline. This problem has been unsolvable using traditional machine learning algorithm pertaining to active contouring via the Kass snake algorithm. Moreover, the proposed technique is closer to manual segmentation than the snakes active contour model in mean of atrial area (M-AA), mean of atrial maximum diameter (M-AMXD), mean atrial minimum diameter (M-AMID), and mean angle of the atrial long axis (M-AALA).
Conclusion
After segmentation, we compute the volume ratio of right to left atria, obtaining a smaller ratio that indicates better restoration. Hence, the proposed technique allows to evaluate the surgical success of atrial septal occlusion and may support diagnosis regarding the accurate evaluation of atrial septal defects before and after occlusion procedures.},
	urldate = {2025-02-19},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Zhao, Ming and Wei, Yang and Lu, Yu and Wong, Kelvin K. L.},
	month = nov,
	year = {2020},
	keywords = {Deep learning, U-Net, Active contour, Atrial septal defect, Kass snake algorithm, MRI Diagnostics},
	pages = {105623},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\LYTB5VFW\\S0169260720314565.html:text/html},
}

@article{cinar_hybrid_2022,
	title = {A hybrid {DenseNet121}-{UNet} model for brain tumor segmentation from {MR} {Images}},
	volume = {76},
	issn = {1746-8094},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809422001690},
	doi = {10.1016/j.bspc.2022.103647},
	abstract = {Several techniques are used to detect brain tumors in the medical research field; however, Magnetic Resonance Imaging (MRI) is still the most effective technique used by experts. Recently, researchers have proposed different MRI techniques to detect brain tumors with the possibility of uploading and visualizing the image. In the current decade, deep learning techniques have shown promising results in every research area, especially in bioinformatics and medical image analysis. This paper aims to segment brain tumors using deep learning methods of MR images. The UNet architecture, one of the deep learning networks, is used as a hybrid model with pre-trained DenseNet121 architecture for the segmentation process. During training and testing of the model, we focus on smaller sub-regions of tumors that comprise the complex structure. The proposed model is validated on BRATS 2019 publicly available brain tumor dataset that contains high-grade and low-grade glioma tumors. The experimental results indicate that our model performs better than other state-of-the-art methods presented in this particular area. Specifically, the best Dice Similarity Coefficient (DSC) are obtained by using the proposed approach to segment whole tumor (WT), core tumor (CT), and enhancing tumor (ET).},
	urldate = {2025-02-21},
	journal = {Biomedical Signal Processing and Control},
	author = {Cinar, Necip and Ozcan, Alper and Kaya, Mehmet},
	month = jul,
	year = {2022},
	keywords = {Artificial neural network models, Brain tumor segmentation, Deep learning, DenseNet121, Image processing, Image segmentation, UNet},
	pages = {103647},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\VV3A5SZK\\S1746809422001690.html:text/html},
}

@article{dong_improved_2023,
	title = {An {Improved} {ResNet}-1d with {Channel} {Attention} for {Tool} {Wear} {Monitor} in {Smart} {Manufacturing}},
	volume = {23},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/23/3/1240},
	doi = {10.3390/s23031240},
	abstract = {Tool wear is a key factor in the machining process, which affects the tool life and quality of the machined work piece. Therefore, it is crucial to monitor and diagnose the tool condition. An improved CaAt-ResNet-1d model for multi-sensor tool wear diagnosis was proposed. The ResNet18 structure based on a one-dimensional convolutional neural network is adopted to make the basic model architecture. The one-dimensional convolutional neural network is more suitable for feature extraction of time series data. Add the channel attention mechanism of CaAt1 to the residual network block and the channel attention mechanism of CaAt5 automatically learns the features of different channels. The proposed method is validated on the PHM2010 dataset. Validation results show that CaAt-ResNet-1d can reach 89.27\% accuracy, improving by about 7\% compared to Gated-Transformer and 3\% compared to Resnet18. The experimental results demonstrate the capacity and effectiveness of the proposed method for tool wear monitor.},
	language = {en},
	number = {3},
	urldate = {2025-02-19},
	journal = {Sensors},
	author = {Dong, Liang and Wang, Chensheng and Yang, Guang and Huang, Zeyuan and Zhang, Zhiyue and Li, Cen},
	month = jan,
	year = {2023},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {ResNet, channel attention, multiple sensors, tool wear monitor},
	pages = {1240},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\TYWN5I4D\\Dong et al. - 2023 - An Improved ResNet-1d with Channel Attention for T.pdf:application/pdf},
}

@article{sun_self-attentional_2023,
	title = {A {Self}-{Attentional} {ResNet}-{LightGBM} {Model} for {IoT}-{Enabled} {Voice} {Liveness} {Detection}},
	volume = {10},
	issn = {2327-4662},
	url = {https://ieeexplore.ieee.org/document/9996089},
	doi = {10.1109/JIOT.2022.3230992},
	abstract = {Voice user interface (VUI) brings high efficiency and convenience for the applications of Internet of Things (IoT), meanwhile, it can also cause increasingly serious security issues. The word-level voice liveness detection is proved to be the promising solution to thwart spoofing attacks. However, the complex acoustic feature, diversified attacks, and different interaction distance can severely affects the improvement of detection accuracy. To alleviate this issue, we develop a novel pop noise-based word-level voice liveness detection framework. First, a new voice frame selection method is proposed for determining optimal frames, including short time Fourier transform, low-frequency average energy computation, and sequencing. Then, the acoustic features of the selected frames are calculated by the Gammatone frequency cepstral coefficient (GFCC). Finally, based on these features, a newly built joint voice detector, fusing the self-attentional residual network (ResNet), and light gradient boosting machine (LightGBM), can achieve accurate voice classification. On the popular voice spoofing attack data sets, experimental results show that our proposal significantly outperforming the baseline and the state-of-the-arts models, and it is gender dependent. Moreover, our proposal has good generalization ability for far-field replay voice attack, speech synthesis and voice conversion attacks, and partial fake voice attack. Finally, its effectiveness is verified by the ablation study.},
	number = {9},
	urldate = {2025-02-19},
	journal = {IEEE Internet of Things Journal},
	author = {Sun, Xiaochuan and Fu, Jingchang and Wei, Biao and Li, Zhigang and Li, Yingqi and Wang, Ning},
	month = may,
	year = {2023},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Feature extraction, Acoustic feature, Automobiles, Business, Detectors, frame selection, Internet of Things, light gradient boosting machine (LightGBM), Performance evaluation, pop noise (PN), self-attentional (SA) residual network (ResNet), Smart homes, voice liveness detection},
	pages = {8257--8270},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\PK66RUT3\\9996089.html:text/html},
}

@article{hou_application_2024,
	title = {The application of improved densenet algorithm in accurate image recognition},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-58421-z},
	doi = {10.1038/s41598-024-58421-z},
	abstract = {Image recognition technology belongs to an important research field of artificial intelligence. In order to enhance the application value of image recognition technology in the field of computer vision and improve the technical dilemma of image recognition, the research improves the feature reuse method of dense convolutional network. Based on gradient quantization, traditional parallel algorithms have been improved. This improvement allows for independent parameter updates layer by layer, reducing communication time and data volume. The introduction of quantization error reduces the impact of gradient loss on model convergence. The test results show that the improvement strategy designed by the research improves the model parameter efficiency while ensuring the recognition effect. Narrowing the learning rate is conducive to refining the updating granularity of model parameters, and deepening the number of network layers can effectively improve the final recognition accuracy and convergence effect of the model. It is better than the existing state-of-the-art image recognition models, visual geometry group and EfficientNet. The parallel acceleration algorithm, which is improved by the gradient quantization, performs better than the traditional synchronous data parallel algorithm, and the improvement of the acceleration ratio is obvious. Compared with the traditional synchronous data parallel algorithm and stale synchronous parallel algorithm, the optimized parallel acceleration algorithm of the study ensures the image data training speed and solves the bottleneck problem of communication data. The model designed by the research improves the accuracy and training speed of image recognition technology and expands the use of image recognition technology in the field of computer vision.Please confirm the affiliation details of [1] is correct.The relevant detailed information in reference [1] has been confirmed to be correct.},
	language = {en},
	number = {1},
	urldate = {2025-02-21},
	journal = {Scientific Reports},
	author = {Hou, Yuntao and Wu, Zequan and Cai, Xiaohua and Zhu, Tianyu},
	month = apr,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Software},
	pages = {8645},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\MCEKVSDU\\Hou et al. - 2024 - The application of improved densenet algorithm in .pdf:application/pdf},
}

@article{narotamo_deep_2024,
	title = {Deep learning for {ECG} classification: {A} comparative study of {1D} and {2D} representations and multimodal fusion approaches},
	volume = {93},
	issn = {1746-8094},
	shorttitle = {Deep learning for {ECG} classification},
	url = {https://www.sciencedirect.com/science/article/pii/S174680942400199X},
	doi = {10.1016/j.bspc.2024.106141},
	abstract = {The improved diagnosis of cardiovascular diseases (CVD) from electrocardiograms (ECG) may help prevent their severity. Since Deep Learning (DL) became popular, several DL methods have been developed for ECG classification. In this work, we compare how different methods for ECG signal representation perform in the multi-label classification of CVDs, including recent attention-based strategies. Furthermore, multimodal fusion strategies are employed to improve the prediction capacity of individual representation networks. The publicly available PTB-XL ECG dataset, which contains 21,837 records and labels for the diagnosis of 4 CVDs, was used for the task. Two DL strategies using different processing approaches were compared. Recurrent Neural Network-based models take advantage of the temporal dependence between raw signal values, namely through Gated Recurrent Unit (GRU), Long Short Term Memory (LSTM) and 1D-Convolutional Neural Network models. Additionally, the raw ECG was converted into image representations, based on recent work, and the classification was performed using distinct 2D-Convolutional Neural Networks. The potential of multimodal DL was then studied through early, late and joint data fusion strategies, to evaluate the benefit of resorting to multiple representations. Results based on the 1D ECG representation outperform image-based approaches and multimodal models. The best model, GRU, achieved sensitivity and specificity of 79.67\% and 81.04\%, respectively.},
	urldate = {2025-02-24},
	journal = {Biomedical Signal Processing and Control},
	author = {Narotamo, Hemaxi and Dias, Mariana and Santos, Ricardo and Carreiro, André V. and Gamboa, Hugo and Silveira, Margarida},
	month = jul,
	year = {2024},
	keywords = {Cardiovascular diseases, Convolutional neural networks, Deep learning, Electrocardiogram classification, Multimodal artificial intelligence, Recurrent neural networks},
	pages = {106141},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\8MJW8S7D\\S174680942400199X.html:text/html},
}

@article{zhang_heart_2024,
	title = {Heart failure classification using deep learning to extract spatiotemporal features from {ECG}},
	volume = {24},
	issn = {1472-6947},
	url = {https://doi.org/10.1186/s12911-024-02415-4},
	doi = {10.1186/s12911-024-02415-4},
	abstract = {Heart failure is a syndrome with complex clinical manifestations. Due to increasing population aging, heart failure has become a major medical problem worldwide. In this study, we used the MIMIC-III public database to extract the temporal and spatial characteristics of electrocardiogram (ECG) signals from patients with heart failure.},
	number = {1},
	urldate = {2025-02-24},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Zhang, Chang-Jiang and {Yuan-Lu} and Tang, Fu-Qin and Cai, Hai-Peng and Qian, Yin-Fen and {Chao-Wang}},
	month = jan,
	year = {2024},
	keywords = {CNN-LSTM-SE model, Deep learning, Heart failure, MIMIC- III},
	pages = {17},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\VBSMH8LR\\Zhang et al. - 2024 - Heart failure classification using deep learning t.pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\HJC5TKSV\\s12911-024-02415-4.html:text/html},
}

@article{mohd_noor_deep_2022,
	title = {Deep {Temporal} {Conv}-{LSTM} for {Activity} {Recognition}},
	volume = {54},
	issn = {1573-773X},
	url = {https://doi.org/10.1007/s11063-022-10799-5},
	doi = {10.1007/s11063-022-10799-5},
	abstract = {Human activity recognition has gained interest from the research community due to the advancements in sensor technology and the improved machine learning algorithm. Wearable sensors have become more ubiquitous, and most of the wearable sensor data contain rich temporal structural information that describes the distinct underlying patterns and relationships of various activity types. The nature of those activities is typically sequential, with each subsequent activity window being the result of the preceding activity window. However, the state-of-the-art methods usually model the temporal characteristic of the sensor data and ignore the relationship of the sliding window. This research proposes a novel deep temporal Conv-LSTM architecture to enhance activity recognition performance by utilizing both temporal characteristics from sensor data and the relationship of sliding windows. The proposed architecture is evaluated based on the dataset consisting of transition activities—Smartphone-Based Recognition of Human Activities and Postural Transitions dataset. The proposed hybrid architecture with parallel features learning pipelines has demonstrated the ability to model the temporal relationship of the activity windows where the transition of activities is captured accurately. Besides that, the size of sliding windows is studied, and it has shown that the selection of window size is affecting the accuracy of the activity recognition. The proposed deep temporal Conv-LSTM architecture can achieve an accuracy score of 0.916, which outperformed the state-of-the-art accuracy.},
	language = {en},
	number = {5},
	urldate = {2025-02-24},
	journal = {Neural Processing Letters},
	author = {Mohd Noor, Mohd Halim and Tan, Sen Yan and Ab Wahab, Mohd Nadhir},
	month = oct,
	year = {2022},
	keywords = {Activity recognition, Artificial Intelligence, Deep learning, LSTM, Temporal model},
	pages = {4027--4049},
}

@article{lee_resnet-lstm_2024,
	title = {A {ResNet}-{LSTM} hybrid model for predicting epileptic seizures using a pretrained model with supervised contrastive learning},
	volume = {14},
	copyright = {2023 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-43328-y},
	doi = {10.1038/s41598-023-43328-y},
	abstract = {In this paper, we propose a method for predicting epileptic seizures using a pre-trained model utilizing supervised contrastive learning and a hybrid model combining residual networks (ResNet) and long short-term memory (LSTM). The proposed training approach encompasses three key phases: pre-processing, pre-training as a pretext task, and training as a downstream task. In the pre-processing phase, the data is transformed into a spectrogram image using short time Fourier transform (STFT), which extracts both time and frequency information. This step compensates for the inherent complexity and irregularity of electroencephalography (EEG) data, which often hampers effective data analysis. During the pre-training phase, augmented data is generated from the original dataset using techniques such as band-stop filtering and temporal cutout. Subsequently, a ResNet model is pre-trained alongside a supervised contrastive loss model, learning the representation of the spectrogram image. In the training phase, a hybrid model is constructed by combining ResNet, initialized with weight values from the pre-trained model, and LSTM. This hybrid model extracts image features and time information to enhance prediction accuracy. The proposed method’s effectiveness is validated using datasets from CHB-MIT and Seoul National University Hospital (SNUH). The method’s generalization ability is confirmed through Leave-one-out cross-validation. From the experimental results measuring accuracy, sensitivity, and false positive rate (FPR), CHB-MIT was 91.90\%, 89.64\%, 0.058 and SNUH was 83.37\%, 79.89\%, and 0.131. The experimental results demonstrate that the proposed method outperforms the conventional methods.},
	language = {en},
	number = {1},
	urldate = {2025-02-24},
	journal = {Scientific Reports},
	author = {Lee, Dohyun and Kim, Byunghyun and Kim, Taejoon and Joe, Inwhee and Chong, Jongwha and Min, Kyeongyuk and Jung, Kiyoung},
	month = jan,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Diseases of the nervous system, Epilepsy, Neuroscience},
	pages = {1319},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\Q7C89XBW\\Lee et al. - 2024 - A ResNet-LSTM hybrid model for predicting epilepti.pdf:application/pdf},
}

@inproceedings{jia_detrs_2023,
	title = {{DETRs} with {Hybrid} {Matching}},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Jia, Ding and Yuan, Yuhui and He, Haodi and Wu, Xiaopei and Yu, Haojun and Lin, Weihong and Sun, Lei and Zhang, Chao and Hu, Han},
	year = {2023},
	pages = {19702--19712},
}

@article{husham_al-badri_adaptive_2023,
	title = {Adaptive {Non}-{Maximum} {Suppression} for improving performance of \textit{{Rumex}} detection},
	volume = {219},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417423001355},
	doi = {10.1016/j.eswa.2023.119634},
	abstract = {A crucial post-processing stage in numerous object detection methods is Non-Maximum Suppression (NMS). The key idea of this technique is to rank the detected bounding boxes according to their scores. Subsequently, selecting the bounding box with the maximum score represents the one best-fitted to the object and suppresses the remaining significant boxes. Conventional NMS suffers from locating objects with accurate bounding boxes as there are multiple boxes in a certain region. This issue reduces the detection performance of automated weed applications in the real world. Weed detection methods based on Region-Convolutional Neural Network (R-CNN) frameworks remain suffer from a lack of detection rate due to overlapping and occlusion leaves issues. This paper presents an Ensemble-Region Convolutional Neural Networks (E-RCNN) model of three state-of-the-art networks to detect Rumex obtusifolius L. (R. obtu.) weeds under various conditions, especially overlapping. The proposed E-RCNN model is used due to its novelty of using ensemble classifiers with the combination of three extractors at its backbone. Adaptive Non-Maximum Suppression (ANMS) is proposed with the Region Proposal Network (RPN) to enhance the detection performance of overlapping and occluded objects by overcoming the drawbacks of conventional Non-Maximum Suppression (NMS). A hybrid model of three CNN extractor networks is used as the backbone in the classification stage. Thus, integrating three networks into one robust model increases the recognition capability by extracting additional useful features more efficiently than those from an individual network. For detection, RPN is used to generate multi-proposed boxes, whereas ANMS is used to select the best box that has a high score rate to match the target object. Our proposed model has trained and tested two standard benchmarking datasets of Rumex weeds under real-world data. The proposed model tested each dataset separately to evaluate the detection rate in terms of Intersection over Union (IoU). For comparing the evaluation of the detection rate, AlexNet, Single-Shot Detector (SSD), DetectNet and Faster R-CNN with conventional NMS models are used to compare the results.},
	urldate = {2025-02-25},
	journal = {Expert Systems with Applications},
	author = {Husham Al-Badri, Ahmed and Azman Ismail, Nor and Al-Dulaimi, Khamael and Ahmed Salman, Ghalib and Sah Hj Salam, Md},
	month = jun,
	year = {2023},
	keywords = {Deep Learning (DL), Ensemble Learning, Non-Maximum Suppression (NMS), Real-world data, Weed Detection},
	pages = {119634},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\XJSJWN2K\\S0957417423001355.html:text/html},
}

@article{jiang_non-maximum_2024,
	title = {Non-{Maximum} {Suppression} {Guided} {Label} {Assignment} for {Object} {Detection} in {Crowd} {Scenes}},
	volume = {26},
	issn = {1941-0077},
	url = {https://ieeexplore.ieee.org/document/10175568},
	doi = {10.1109/TMM.2023.3293333},
	abstract = {The detection performance in crowd scenes is limited by recalling hard objects (e.g., occluded objects). It requires that this kind of objects can be successfully detected and retained by the non-maximum suppression (NMS) while controlling false positives. The existing dynamic label assignment algorithms can help recall these objects by adaptively allocating appropriate positive samples, however, they ignore the alignment with the selecting rules of NMS. This leads to the fact that detecting objects in crowd scenes are still very sensitive to the NMS threshold setting. As a result, the existing methods can only set a low NMS threshold to avoid the excessive false positives, causing some objects failed to be recalled. And these methods also generally lack more excitation for positive samples, which hinders further facilitating the recall of hard instances in crowd scenes. This article proposes a novel dynamic label assignment strategy for object detection in crowd scenes, called non-maximum suppression guided label assignment (NGLA), which aligns the assignment strategy with NMS process and learns more prominent positive samples. Following NMS, NGLA introduces the IoU between samples with their corresponding best samples to define positive and negative samples. To cooperate with NGLA, an NMS-aware loss is proposed to dynamically assign sample weights when supervising sample predictions, which also considers the IoU with the best sample. In addition, for better classification prediction, a regression assisted classification branch is designed to help detectors perceive the relation between the regression predictions of each sample and the corresponding best sample. Experiments demonstrate that NGLA outperforms other label assignment methods on CrowdHuman and Citypersons, and is less sensitive to the NMS threshold in crowd scenes.},
	urldate = {2025-02-26},
	journal = {IEEE Transactions on Multimedia},
	author = {Jiang, Hangzhi and Zhang, Xin and Xiang, Shiming},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Annotations, Crowd scenes, Detectors, Feature extraction, Heuristic algorithms, Label assignment, Non-maximum suppression, Object detection, Task analysis, Training},
	pages = {2207--2218},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\UPGGJQNZ\\10175568.html:text/html},
}

@inproceedings{shi_similarity_2024,
	title = {Similarity {Distance}-{Based} {Label} {Assignment} for {Tiny} {Object} {Detection}},
	url = {https://ieeexplore.ieee.org/document/10801448},
	doi = {10.1109/IROS58592.2024.10801448},
	abstract = {Tiny object detection is becoming one of the most challenging tasks in computer vision because of the limited object size and lack of information. The label assignment strategy is a key factor affecting the accuracy of object detection. Although there are some effective label assignment strategies for tiny objects, most of them focus on reducing the sensitivity to the bounding boxes to increase the number of positive samples and have some fixed hyperparameters need to set. However, more positive samples may not necessarily lead to better detection results, in fact, excessive positive samples may lead to more false positives. In this paper, we introduce a simple but effective strategy named the Similarity Distance (SimD) to evaluate the similarity between bounding boxes. This proposed strategy not only considers both location and shape similarity but also learns hyperparameters adaptively, ensuring that it can adapt to different datasets and various object sizes in a dataset. Our approach can be simply applied in common anchor-based detectors in place of the IoU for label assignment and Non Maximum Suppression (NMS). Extensive experiments on four mainstream tiny object detection datasets demonstrate superior performance of our method, especially, 1.8 AP points and 4.1 AP points of very tiny higher than the state-of-the-art competitors on AI-TOD. Code is available at: https://github.com/cszzshi/simd.},
	urldate = {2025-02-26},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Shi, Shuohao and Fang, Qiang and Xu, Xin and Zhao, Tong},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Accuracy, Codes, Computer vision, Detectors, Intelligent robots, Object detection, Sensitivity, Shape},
	pages = {13711--13718},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\CBQSTDRQ\\10801448.html:text/html;Submitted Version:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\SGXR39GM\\Shi et al. - 2024 - Similarity Distance-Based Label Assignment for Tin.pdf:application/pdf},
}

### Speech recognition begin

@misc{radford_robust_2022,
	title = {Robust {Speech} {Recognition} via {Large}-{Scale} {Weak} {Supervision}},
	url = {http://arxiv.org/abs/2212.04356},
	doi = {10.48550/arXiv.2212.04356},
	abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
	urldate = {2025-02-27},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
	month = dec,
	year = {2022},
	note = {arXiv:2212.04356 [eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\2XB4WDF9\\2212.html:text/html},
}

@misc{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	shorttitle = {wav2vec 2.0},
	url = {http://arxiv.org/abs/2006.11477},
	doi = {10.48550/arXiv.2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2025-02-27},
	publisher = {arXiv},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	month = oct,
	year = {2020},
	note = {arXiv:2006.11477 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\DHWV9F62\\2006.html:text/html},
}

@misc{gulati_conformer_2020,
	title = {Conformer: {Convolution}-augmented {Transformer} for {Speech} {Recognition}},
	shorttitle = {Conformer},
	url = {http://arxiv.org/abs/2005.08100},
	doi = {10.48550/arXiv.2005.08100},
	abstract = {Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1\%/4.3\% without using a language model and 1.9\%/3.9\% with an external language model on test/testother. We also observe competitive performance of 2.7\%/6.3\% with a small model of only 10M parameters.},
	urldate = {2025-02-28},
	publisher = {arXiv},
	author = {Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and Pang, Ruoming},
	month = may,
	year = {2020},
	note = {arXiv:2005.08100 [eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\6JKCSLSP\\Gulati et al. - 2020 - Conformer Convolution-augmented Transformer for S.pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\V468W85H\\2005.html:text/html},
}

@misc{stooke_aligner-encoders_2025,
	title = {Aligner-{Encoders}: {Self}-{Attention} {Transformers} {Can} {Be} {Self}-{Transducers}},
	shorttitle = {Aligner-{Encoders}},
	url = {http://arxiv.org/abs/2502.05232},
	doi = {10.48550/arXiv.2502.05232},
	abstract = {Modern systems for automatic speech recognition, including the RNN-Transducer and Attention-based Encoder-Decoder (AED), are designed so that the encoder is not required to alter the time-position of information from the audio sequence into the embedding; alignment to the final text output is processed during decoding. We discover that the transformer-based encoder adopted in recent years is actually capable of performing the alignment internally during the forward pass, prior to decoding. This new phenomenon enables a simpler and more efficient model, the "Aligner-Encoder". To train it, we discard the dynamic programming of RNN-T in favor of the frame-wise cross-entropy loss of AED, while the decoder employs the lighter text-only recurrence of RNN-T without learned cross-attention -- it simply scans embedding frames in order from the beginning, producing one token each until predicting the end-of-message. We conduct experiments demonstrating performance remarkably close to the state of the art, including a special inference configuration enabling long-form recognition. In a representative comparison, we measure the total inference time for our model to be 2x faster than RNN-T and 16x faster than AED. Lastly, we find that the audio-text alignment is clearly visible in the self-attention weights of a certain layer, which could be said to perform "self-transduction".},
	urldate = {2025-02-27},
	publisher = {arXiv},
	author = {Stooke, Adam and Prabhavalkar, Rohit and Sim, Khe Chai and Mengibar, Pedro Moreno},
	month = feb,
	year = {2025},
	note = {arXiv:2502.05232 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\FZXCUV79\\Stooke et al. - 2025 - Aligner-Encoders Self-Attention Transformers Can .pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\EEEJVNAD\\2502.html:text/html},
}

@misc{zhang_breaking_2025,
	title = {Breaking {Through} the {Spike}: {Spike} {Window} {Decoding} for {Accelerated} and {Precise} {Automatic} {Speech} {Recognition}},
	shorttitle = {Breaking {Through} the {Spike}},
	url = {http://arxiv.org/abs/2501.03257},
	doi = {10.48550/arXiv.2501.03257},
	abstract = {Recently, end-to-end automatic speech recognition has become the mainstream approach in both industry and academia. To optimize system performance in specific scenarios, the Weighted Finite-State Transducer (WFST) is extensively used to integrate acoustic and language models, leveraging its capacity to implicitly fuse language models within static graphs, thereby ensuring robust recognition while also facilitating rapid error correction. However, WFST necessitates a frame-by-frame search of CTC posterior probabilities through autoregression, which significantly hampers inference speed. In this work, we thoroughly investigate the spike property of CTC outputs and further propose the conjecture that adjacent frames to non-blank spikes carry semantic information beneficial to the model. Building on this, we propose the Spike Window Decoding algorithm, which greatly improves the inference speed by making the number of frames decoded in WFST linearly related to the number of spiking frames in the CTC output, while guaranteeing the recognition performance. Our method achieves SOTA recognition accuracy with significantly accelerates decoding speed, proven across both AISHELL-1 and large-scale In-House datasets, establishing a pioneering approach for integrating CTC output with WFST.},
	urldate = {2025-02-27},
	publisher = {arXiv},
	author = {Zhang, Wei and Zhang, Tian-Hao and Luo, Chao and Zhou, Hui and Yang, Chao and Qian, Xinyuan and Yin, Xu-Cheng},
	month = jan,
	year = {2025},
	note = {arXiv:2501.03257 [eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
	file = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\RF3F4L95\\2501.html:text/html},
}

@misc{wang_disentangled-transformer_2024,
	title = {Disentangled-{Transformer}: {An} {Explainable} {End}-to-{End} {Automatic} {Speech} {Recognition} {Model} with {Speech} {Content}-{Context} {Separation}},
	shorttitle = {Disentangled-{Transformer}},
	url = {http://arxiv.org/abs/2411.17846},
	doi = {10.48550/arXiv.2411.17846},
	abstract = {End-to-end transformer-based automatic speech recognition (ASR) systems often capture multiple speech traits in their learned representations that are highly entangled, leading to a lack of interpretability. In this study, we propose the explainable Disentangled-Transformer, which disentangles the internal representations into sub-embeddings with explicit content and speaker traits based on varying temporal resolutions. Experimental results show that the proposed Disentangled-Transformer produces a clear speaker identity, separated from the speech content, for speaker diarization while improving ASR performance.},
	urldate = {2025-02-28},
	publisher = {arXiv},
	author = {Wang, Pu and hamme, Hugo Van},
	month = nov,
	year = {2024},
	note = {arXiv:2411.17846 [eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\IUHPGKBK\\Wang and hamme - 2024 - Disentangled-Transformer An Explainable End-to-En.pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\SWAC4FEQ\\2411.html:text/html},
}

@misc{shakhadri_samba-asr_2025,
	title = {Samba-{ASR}: {State}-{Of}-{The}-{Art} {Speech} {Recognition} {Leveraging} {Structured} {State}-{Space} {Models}},
	shorttitle = {Samba-{ASR}},
	url = {http://arxiv.org/abs/2501.02832},
	doi = {10.48550/arXiv.2501.02832},
	abstract = {We propose Samba ASR,the first state of the art Automatic Speech Recognition(ASR)model leveraging the novel Mamba architecture as both encoder and decoder,built on the foundation of state space models(SSMs).Unlike transformerbased ASR models,which rely on self-attention mechanisms to capture dependencies,Samba ASR effectively models both local and global temporal dependencies using efficient statespace dynamics,achieving remarkable performance gains.By addressing the limitations of transformers,such as quadratic scaling with input length and difficulty in handling longrange dependencies,Samba ASR achieves superior accuracy and efficiency.Experimental results demonstrate that Samba ASR surpasses existing opensource transformerbased ASR models across various standard benchmarks,establishing it as the new state of theart in ASR.Extensive evaluations on the benchmark dataset show significant improvements in Word Error Rate(WER),with competitive performance even in lowresource scenarios.Furthermore,the inherent computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.Our contributions include the development of a new Samba ASR architecture for automatic speech recognition(ASR),demonstrating the superiority of structured statespace models(SSMs)over transformer based models for speech sequence processing.We provide a comprehensive evaluation on public benchmarks,showcasing stateoftheart(SOTA)performance,and present an indepth analysis of computational efficiency,robustness to noise,and sequence generalization.This work highlights the viability of Mamba SSMs as a transformerfree alternative for efficient and accurate ASR.By leveraging the advancements of statespace modeling,Samba ASR redefines ASR performance standards and sets a new benchmark for future research in this field.},
	urldate = {2025-02-27},
	publisher = {arXiv},
	author = {Shakhadri, Syed Abdul Gaffar and KR, Kruthika and Angadi, Kartik Basavaraj},
	month = jan,
	year = {2025},
	note = {arXiv:2501.02832 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
	file = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\ZVBZT86D\\2501.html:text/html},
}

### Speech recognition end

### Finance begin

@article{wang_sudf-rs_2023,
	title = {{SUDF}-{RS}: {A} new foreign exchange rate prediction method considering the complementarity of supervised and unsupervised deep representation features},
	volume = {214},
	issn = {0957-4174},
	shorttitle = {{SUDF}-{RS}},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422021704},
	doi = {10.1016/j.eswa.2022.119152},
	abstract = {The supervised and unsupervised deep representation features have been adopted and verified their effective roles separately in the foreign exchange rate prediction (FERP). However, the complementarity of these two types of features for FERP remained unexplored. To address this problem, we proposed a novel method for one day ahead FERP, which improves Random Subspace by simultaneously considering both Supervised and Unsupervised Deep representation Features, namely, SUDF-RS. Feature extraction and model construction are two important stages in the SUDF-RS method. Firstly, the supervised and unsupervised deep representation features are extracted by the long short-term memory networks and deep belief networks respectively. Secondly, an improved RS method, which incorporates random forest-based feature weighting mechanism is developed to generate high-quality feature subsets. Thirdly, each feature subset is used to train the corresponding base learner and the final results are obtained by averaging the results of each base learner. Experiments on three exchange rate datasets, namely EUR/USD, GBP/USD and USD/JPY validate that the proposed SUDF-RS significantly outperforms the benchmark methods in terms of both MAPE and RMSE.},
	urldate = {2025-03-07},
	journal = {Expert Systems with Applications},
	author = {Wang, Gang and Ma, Jingling and Wang, Ying and Tao, Tao and Ren, Gang and Zhu, Hegong},
	month = mar,
	year = {2023},
	keywords = {Deep belief network, Deep learning, Ensemble learning, Exchange rates prediction, Long short-term memory, Random Forest, Random Subspace},
	pages = {119152},
}

@article{chen_deep_2024,
	title = {A deep fusion model for stock market prediction with news headlines and time series data},
	volume = {36},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-024-10303-1},
	doi = {10.1007/s00521-024-10303-1},
	abstract = {Time series forecasting models are essential decision support tools in real-world domains. Stock market is a remarkably complex domain, due to its quickly evolving temporal nature, as well as the multiple factors having an impact on stock prices. To date, a number of machine learning-based approaches have been proposed in the literature to tackle stock trend prediction. However, they typically tend to analyze a single data source or modality, or consider multiple modalities in isolation and rely on simple combination strategies, with a potential reduction in their modeling power. In this paper, we propose a multimodal deep fusion model to predict stock trends, leveraging daily stock prices, technical indicators, and sentiment in daily news headlines published by media outlets. The proposed architecture leverages a BERT-based model branch fine-tuned on financial news and a long short-term memory (LSTM) branch that captures relevant temporal patterns in multivariate data, including stock prices and technical indicators. Our experiments on 12 different stock datasets with prices and news headlines demonstrate that our proposed model is more effective than popular baseline approaches, both in terms of accuracy and trading performance in a portfolio analysis simulation, highlighting the positive impact of multimodal deep learning for stock trend prediction.},
	language = {en},
	number = {34},
	urldate = {2025-03-07},
	journal = {Neural Computing and Applications},
	author = {Chen, Pinyu and Boukouvalas, Zois and Corizzo, Roberto},
	month = dec,
	year = {2024},
	keywords = {Artificial Intelligence, Deep learning, Multimodal learning, Portfolio analysis, Sentiment analysis, Stock market analysis, Time series prediction},
	pages = {21229--21271},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\MVUW6F5K\\Chen et al. - 2024 - A deep fusion model for stock market prediction wi.pdf:application/pdf},
}

@inproceedings{mozaffari_predictive_2024,
	address = {New York, NY, USA},
	series = {{ICMLT} '24},
	title = {Predictive {Modeling} of {Stock} {Prices} {Using} {Transformer} {Model}},
	isbn = {9798400716379},
	url = {https://dl.acm.org/doi/10.1145/3674029.3674037},
	doi = {10.1145/3674029.3674037},
	abstract = {Financial market prediction utilizing deep learning has attracted the attention of both investors and researchers. Deep learning methods, such as convolutional neural networks and recurrent neural networks work well at predicting stock indices based on the non-linear characteristics of stock markets. The goal of this work is to predict the stock index using the latest deep learning framework, Transformer. This paper presents a comprehensive analysis of stock closing price prediction using three distinct machine learning models: Long Short-Term Memory (LSTM), Prophet, and Transformer. Using the encoder-decoder architecture and the multi-head attention mechanism, Transformer is able to better characterize stock market dynamics. The present study uses data from Yahoo Finance. The Transformer model demonstrated superior performance in comparison with LSTM and Prophet. In this work, we handle the complexities of market dynamics to improve stock price predictions.},
	urldate = {2025-03-01},
	booktitle = {Proceedings of the 2024 9th {International} {Conference} on {Machine} {Learning} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Mozaffari, Leila and Zhang, Jianhua},
	month = sep,
	year = {2024},
	pages = {41--48},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\KBI4YXX7\\Mozaffari and Zhang - 2024 - Predictive Modeling of Stock Prices Using Transfor.pdf:application/pdf},
}

@misc{li_transformer_2025,
	title = {Transformer {Based} {Time}-{Series} {Forecasting} for {Stock}},
	url = {http://arxiv.org/abs/2502.09625},
	doi = {10.48550/arXiv.2502.09625},
	abstract = {To the naked eye, stock prices are considered chaotic, dynamic, and unpredictable. Indeed, it is one of the most difficult forecasting tasks that hundreds of millions of retail traders and professional traders around the world try to do every second even before the market opens. With recent advances in the development of machine learning and the amount of data the market generated over years, applying machine learning techniques such as deep learning neural networks is unavoidable. In this work, we modeled the task as a multivariate forecasting problem, instead of a naive autoregression problem. The multivariate analysis is done using the attention mechanism via applying a mutated version of the Transformer, "Stockformer", which we created.},
	urldate = {2025-03-02},
	publisher = {arXiv},
	author = {Li, Shuozhe and Schulwol, Zachery B. and Miikkulainen, Risto},
	month = jan,
	year = {2025},
	note = {arXiv:2502.09625 [q-fin]},
	keywords = {Computer Science - Machine Learning, Quantitative Finance - Computational Finance},
	file = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\NDIQL35U\\2502.html:text/html},
}

@misc{zhou_informer_2021,
	title = {Informer: {Beyond} {Efficient} {Transformer} for {Long} {Sequence} {Time}-{Series} {Forecasting}},
	shorttitle = {Informer},
	url = {http://arxiv.org/abs/2012.07436},
	doi = {10.48550/arXiv.2012.07436},
	abstract = {Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a \$ProbSparse\$ self-attention mechanism, which achieves \$O(L {\textbackslash}log L)\$ in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.},
	urldate = {2025-03-02},
	publisher = {arXiv},
	author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
	month = mar,
	year = {2021},
	note = {arXiv:2012.07436 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\XUJ59ZX9\\2012.html:text/html},
}

@misc{berti_tlob_2025,
	title = {{TLOB}: {A} {Novel} {Transformer} {Model} with {Dual} {Attention} for {Stock} {Price} {Trend} {Prediction} with {Limit} {Order} {Book} {Data}},
	shorttitle = {{TLOB}},
	url = {http://arxiv.org/abs/2502.15757},
	doi = {10.48550/arXiv.2502.15757},
	abstract = {Stock Price Trend Prediction (SPTP) based on Limit Order Book (LOB) data is a fundamental challenge in financial markets. Despite advances in deep learning, existing models fail to generalize across different market conditions and struggle to reliably predict short-term trends. Surprisingly, by adapting a simple MLP-based architecture to LOB, we show that we surpass SoTA performance; thus, challenging the necessity of complex architectures. Unlike past work that shows robustness issues, we propose TLOB, a transformer-based model that uses a dual attention mechanism to capture spatial and temporal dependencies in LOB data. This allows it to adaptively focus on the market microstructure, making it particularly effective for longer-horizon predictions and volatile market conditions. We also introduce a new labeling method that improves on previous ones, removing the horizon bias. We evaluate TLOB's effectiveness using the established FI-2010 benchmark, which exceeds the state-of-the-art by an average of 3.7 F1-score({\textbackslash}\%). Additionally, TLOB shows improvements on Tesla and Intel with a 1.3 and 7.7 increase in F1-score({\textbackslash}\%), respectively. Additionally, we empirically show how stock price predictability has declined over time (-6.68 absolute points in F1-score({\textbackslash}\%)), highlighting the growing market efficiencies. Predictability must be considered in relation to transaction costs, so we experimented with defining trends using an average spread, reflecting the primary transaction cost. The resulting performance deterioration underscores the complexity of translating trend classification into profitable trading strategies. We argue that our work provides new insights into the evolving landscape of stock price trend prediction and sets a strong foundation for future advancements in financial AI. We release the code at https://github.com/LeonardoBerti00/TLOB.},
	urldate = {2025-03-02},
	publisher = {arXiv},
	author = {Berti, Leonardo and Kasneci, Gjergji},
	month = feb,
	year = {2025},
	note = {arXiv:2502.15757 [q-fin]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Finance - Statistical Finance, Quantitative Finance - Trading and Market Microstructure},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\L56MSJ4V\\Berti and Kasneci - 2025 - TLOB A Novel Transformer Model with Dual Attentio.pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\PRUYDXAS\\2502.html:text/html},
}

### Finance end


### Robotics begin

@article{le_application_2025,
	title = {Application of the vision-based deep learning technique for waste classification using the robotic manipulation system},
	volume = {6},
	issn = {2666-3074},
	url = {https://www.sciencedirect.com/science/article/pii/S2666307425000154},
	doi = {10.1016/j.ijcce.2025.02.005},
	abstract = {To maintain a green society, efficient waste management is crucial. Traditional manual trash sorting presents several challenges, including inaccuracies in classification and potential health risks for workers. To address these issues, this paper proposes an intelligent and automated waste classification system that integrates deep learning with robotic kinematic control. Our approach significantly improves classification accuracy, speed, and reliability compared to manual sorting. A diverse dataset containing various waste objects, including durian peels, was collected and labelled by experts. Using deep learning, the system was trained to recognize and classify objects with high precision. A camera mounted on the end-effector of robot identifies the position and orientation of object, enabling the robot to precisely pick up and sort waste items. The key advancements of our approach include (i) development of a robotic waste classification platform that enhances sorting efficiency and reduces human involvement, (ii) implementation of a model-based learning approach that achieves rapid and accurate object detection, (iii) validation through real-world experiments, demonstrating the feasibility and effectiveness of the system in complex environments. Experimental results confirm that the proposed system significantly enhances waste classification accuracy and efficiency, paving the way for safer and more intelligent waste management in smart manufacturing and environmental sustainability applications.},
	urldate = {2025-03-05},
	journal = {International Journal of Cognitive Computing in Engineering},
	author = {Le, Huu Tran Nhat and Ngo, Ha Quang Thinh},
	month = dec,
	year = {2025},
	keywords = {Artificial intelligence, Computer vision, Pick and place, Robot kinematics, Robotics},
	pages = {391--400},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\TEG3SHYI\\S2666307425000154.html:text/html},
}

@article{liu_automatic_2025,
	title = {Automatic tile position and orientation detection combining deep-learning and rule-based computer vision algorithms},
	volume = {171},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S092658052500041X},
	doi = {10.1016/j.autcon.2025.106001},
	abstract = {Increasing interest in a tile-paving robot calls for a robust tile detection algorithm. This paper proposes the Ultra Clear Tile (UC-Tile) algorithm to detect corners and edges and assist tile paving automation in positioning and installation tasks. UC-Tile is designed to incorporate deep learning for semantic segmentation with rule-based post-processing algorithms. The semantic segmentation algorithm investigated herein is a fine-tuned version of YOLOv8. UC-Tile mainly contributes to refitting the edges and locating the tile corners with tailored algorithms. A dataset is developed comprising 1486 images exhibiting varying patterns of tiles, captured under disparate heights and illumination conditions. Results indicate that UC-Tile outperforms common benchmark algorithms, and can achieve the highest mIoU 98.68 \%, F1-Score 99.31 \%, and the lowest 95-HD. Further, UC-Tile can accurately predict real distance and angles with small differences to ground truth values, thereby informing robotic movement control. This algorithm is expected to enable precise automatic tile paving.},
	urldate = {2025-01-31},
	journal = {Automation in Construction},
	author = {Liu, Wenyao and Chen, Jinhua and Lyu, Zemin and Feng, Rui and Hu, Tong and Deng, Lu},
	month = mar,
	year = {2025},
	keywords = {Deep learning, Construction robots, Intelligent construction, Segmentation, Tile paving},
	pages = {106001},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\IL3U356S\\S092658052500041X.html:text/html},
}

@article{vukicevic_versatile_2025,
	title = {Versatile waste sorting in small batch and flexible manufacturing industries using deep learning techniques},
	volume = {15},
	copyright = {2025 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-025-87226-x},
	doi = {10.1038/s41598-025-87226-x},
	abstract = {The expansion of LEAN and small batch manufacturing demands flexible automated workstations capable of switching between sorting various wastes over time. To address this challenge, our study is focused on assessing the ability of the Segment Anything Model (SAM) family of deep learning architectures to separate highly variable objects during robotic waste sorting. The proposed two-step procedure for generic versatile visual waste sorting is based on the SAM architectures (original SAM, FastSAM, MobileSAMv2, and EfficientSAM) for waste object extraction from raw images, and the use of classification architecture (MobileNetV2, VGG19, Dense-Net, Squeeze-Net, ResNet, and Inception-v3) for accurate waste sorting. Such a pipeline brings two key advantages that make it more applicable in industry practice by: 1) eliminating the necessity for developing dedicated waste detection and segmentation algorithms for waste object localization, and 2) significantly reducing the time and costs required for adapting the solution to different use cases. With the proposed procedure, switching to a new waste type sorting is reduced to only two steps: The use of SAM for the automatic object extraction, followed by their separation into corresponding classes used to fine-tune the classifier. Validation on four use cases (floating waste, municipal waste, e-waste, and smart bins) shows robust results, with accuracy ranging from 86 to 97\% when using the MobileNetV2 with SAM and FastSAM architectures. The proposed approach has a high potential to facilitate deployment, increase productivity, lower expenses, and minimize errors in robotic waste sorting while enhancing overall recycling and material utilization in the manufacturing industry.},
	language = {en},
	number = {1},
	urldate = {2025-03-05},
	journal = {Scientific Reports},
	author = {Vukicevic, Arso M. and Petrovic, Milos and Jurisevic, Nebojsa and Djapan, Marko and Knezevic, Nikola and Novakovic, Aleksandar and Jovanovic, Kosta},
	month = jan,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Environmental impact, Software},
	pages = {3756},
}

@misc{kirillov_segment_2023,
	title = {Segment {Anything}},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	urldate = {2025-03-05},
	publisher = {arXiv},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02643 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{ge_deep_2025,
	title = {Deep learning-enhanced smart ground robotic system for automated structural damage inspection and mapping},
	volume = {170},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580524006873},
	doi = {10.1016/j.autcon.2024.105951},
	abstract = {Ground robotic systems are essential for structural inspection, enhancing mobility, efficiency, and safety while minimizing risks in manual inspections. These systems automate 3D mapping and defect assessment in aging. However, current robotic platforms often require the integration of various sensors and complex parameter tuning, raising costs and limiting efficiency. This paper proposes a streamlined unmanned ground vehicle-based inspection platform, integrating only LiDAR and a low-cost monocular camera. Operated via the Robot Operating System, the platform deploys efficient instance segmentation, Simultaneous Localization and Mapping, and fusion algorithms, eliminating complex tuning across environments. A self-attention-enhanced YOLOv7 algorithm is proposed for accurate damage segmentation with limited datasets, while an enhanced KISS-ICP (Keep It Small and Simple-Iterative Closest Point) algorithm is developed to optimize point cloud odometry for efficient mapping and localization. By introducing camera-LiDAR information fusion, the proposed platform achieves structural mapping, damage localization, quantification, and 3D visualization. Laboratory and full-scale bridge tests demonstrated its high accuracy, efficiency, and robustness.},
	urldate = {2025-01-31},
	journal = {Automation in Construction},
	author = {Ge, Liangfu and Sadhu, Ayan},
	month = feb,
	year = {2025},
	keywords = {Computer vision, 3D visualization, Information fusion, Smart robotic system, Structural damage assessment, Structural health monitoring, Unmanned ground vehicle},
	pages = {105951},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\U6BLVPHV\\S0926580524006873.html:text/html},
}

@article{dai_advanced_2024,
	title = {An {Advanced} {Approach} to {Object} {Detection} and {Tracking} in {Robotics} and {Autonomous} {Vehicles} {Using} {YOLOv8} and {LiDAR} {Data} {Fusion}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/13/12/2250},
	doi = {10.3390/electronics13122250},
	abstract = {Accurately and reliably perceiving the environment is a major challenge in autonomous driving and robotics research. Traditional vision-based methods often suffer from varying lighting conditions, occlusions, and complex environments. This paper addresses these challenges by combining a deep learning-based object detection algorithm, YOLOv8, with LiDAR data fusion technology. The principle of this combination is to merge the advantages of these technologies: YOLOv8 excels in real-time object detection and classification through RGB images, while LiDAR provides accurate distance measurement and 3D spatial information, regardless of lighting conditions. The integration aims to apply the high accuracy and robustness of YOLOv8 in identifying and classifying objects, as well as the depth data provided by LiDAR. This combination enhances the overall environmental perception, which is critical for the reliability and safety of autonomous systems. However, this fusion brings some research challenges, including data calibration between different sensors, filtering ground points from LiDAR point clouds, and managing the computational complexity of processing large datasets. This paper presents a comprehensive approach to address these challenges. Firstly, a simple algorithm is introduced to filter out ground points from LiDAR point clouds, which are essential for accurate object detection, by setting different threshold heights based on the terrain. Secondly, YOLOv8, trained on a customized dataset, is utilized for object detection in images, generating 2D bounding boxes around detected objects. Thirdly, a calibration algorithm is developed to transform 3D LiDAR coordinates to image pixel coordinates, which are vital for correlating LiDAR data with image-based object detection results. Fourthly, a method for clustering different objects based on the fused data is proposed, followed by an object tracking algorithm to compute the 3D poses of objects and their relative distances from a robot. The Agilex Scout Mini robot, equipped with Velodyne 16-channel LiDAR and an Intel D435 camera, is employed for data collection and experimentation. Finally, the experimental results validate the effectiveness of the proposed algorithms and methods.},
	language = {en},
	number = {12},
	urldate = {2025-03-05},
	journal = {Electronics},
	author = {Dai, Yanyan and Kim, Deokgyu and Lee, Kidong},
	month = jan,
	year = {2024},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {calibrations, ground threshold, object detection and tracking, onboard sensors},
	pages = {2250},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\LM4K64VT\\Dai et al. - 2024 - An Advanced Approach to Object Detection and Track.pdf:application/pdf},
}

@article{alotaibi_deep_2024,
	title = {Deep {Learning}-{Based} {Vision} {Systems} for {Robot} {Semantic} {Navigation}: {An} {Experimental} {Study}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7080},
	shorttitle = {Deep {Learning}-{Based} {Vision} {Systems} for {Robot} {Semantic} {Navigation}},
	url = {https://www.mdpi.com/2227-7080/12/9/157},
	doi = {10.3390/technologies12090157},
	abstract = {Robot semantic navigation has received significant attention recently, as it aims to achieve reliable mapping and navigation accuracy. Object detection tasks are vital in this endeavor, as a mobile robot needs to detect and recognize the objects in the area of interest to build an effective semantic map. To achieve this goal, this paper classifies and discusses recently developed object detection approaches and then presents the available vision datasets that can be employed in robot semantic navigation applications. In addition, this paper discusses several experimental studies that have validated the efficiency of object detection algorithms, including Faster R-CNN, YOLO v5, and YOLO v8. These studies also utilized a vision dataset to design and develop efficient robot semantic navigation systems, which is also discussed. According to several experiments conducted in a Fablab area, the YOLO v8 object classification model achieved the best results in terms of classification accuracy and processing speed.},
	language = {en},
	number = {9},
	urldate = {2025-01-31},
	journal = {Technologies},
	author = {Alotaibi, Albandari and Alatawi, Hanan and Binnouh, Aseel and Duwayriat, Lamaa and Alhmiedat, Tareq and Alia, Osama Moh’d},
	month = sep,
	year = {2024},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning-based vision systems, robot semantic navigation, vision datasets},
	pages = {157},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\428T7VUD\\Alotaibi et al. - 2024 - Deep Learning-Based Vision Systems for Robot Seman.pdf:application/pdf},
}

@article{misir_drivable_2024,
	title = {Drivable path detection for a mobile robot with differential drive using a deep {Learning} based segmentation method for indoor navigation},
	volume = {10},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-2514},
	doi = {10.7717/peerj-cs.2514},
	abstract = {The integration of artificial intelligence into the field of robotics enables robots to perform their tasks more meaningfully. In particular, deep-learning methods contribute significantly to robots becoming intelligent cybernetic systems. The effective use of deep-learning mobile cyber-physical systems has enabled mobile robots to become more intelligent. This effective use of deep learning can also help mobile robots determine a safe path. The drivable pathfinding problem involves a mobile robot finding the path to a target in a challenging environment with obstacles. In this paper, a semantic-segmentation-based drivable path detection method is presented for use in the indoor navigation of mobile robots. The proposed method uses a perspective transformation strategy based on transforming high-accuracy segmented images into real-world space. This transformation enables the motion space to be divided into grids, based on the image perceived in a real-world space. A grid-based RRT* navigation strategy was developed that uses images divided into grids to enable the mobile robot to avoid obstacles and meet the optimal path requirements. Smoothing was performed to improve the path planning of the grid-based RRT* and avoid unnecessary turning angles of the mobile robot. Thus, the mobile robot could reach the target in an optimum manner in the drivable area determined by segmentation. Deeplabv3+ and ResNet50 backbone architecture with superior segmentation ability are proposed for accurate determination of drivable path. Gaussian filter was used to reduce the noise caused by segmentation. In addition, multi-otsu thresholding was used to improve the masked images in multiple classes. The segmentation model and backbone architecture were compared in terms of their performance using different methods. DeepLabv3+ and ResNet50 backbone architectures outperformed the other compared methods by 0.21\%–4.18\% on many metrics. In addition, a mobile robot design is presented to test the proposed drivable path determination method. This design validates the proposed method by using different scenarios in an indoor environment.},
	language = {en},
	urldate = {2025-03-05},
	journal = {PeerJ Computer Science},
	author = {Mısır, Oğuz},
	month = nov,
	year = {2024},
	note = {Publisher: PeerJ Inc.},
	pages = {e2514},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\7QCJKTZN\\Mısır - 2024 - Drivable path detection for a mobile robot with di.pdf:application/pdf},
}

@misc{chen_rethinking_2017,
	title = {Rethinking {Atrous} {Convolution} for {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1706.05587},
	doi = {10.48550/arXiv.1706.05587},
	abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
	urldate = {2025-03-05},
	publisher = {arXiv},
	author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	month = dec,
	year = {2017},
	note = {arXiv:1706.05587 [cs]
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\ENULRI4G\\Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image S.pdf:application/pdf},
}

@article{cao_orchard_2024,
	title = {Orchard {Vision} {Navigation} {Line} {Extraction} {Based} on {YOLOv8}-{Trunk} {Detection}},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10583867},
	doi = {10.1109/ACCESS.2024.3422422},
	abstract = {Visual navigation is the pivotal technology for enabling autonomous operations of orchard robots. To obtain orchard navigation lines, the robot needs to quickly identify the positions of tree trunks. For this, we proposed a detection model called YOLOv8-Trunk in this study. Based on the detection results of vine tree trunks by YOLOv8-Trunk, the network generates a series of center point coordinates at the bottom of the detection boxes. Subsequently, the least square method is employed to fit reference lines on both sides of the trunk, thereby determining the navigation path for the orchard robot. To enhance the focus on the target, an efficient multi-scale attention (EMA) mechanism is introduced into traditional YOLOv8 network. On the data level, we adopted a novel Mix-Shelter method to augment the datasets for training the detection model, thereby bolstering the robustness. In addition, we also explored the impact of loss functions and optimizers on the performance of the detection model. A comprehensive set of ablation and comparison experiments is conducted in this study. The experimental results affirm that the YOLOv8-Trunk network adeptly detects vine tree trunks, achieving an accuracy rate of 92.7\%. The obtained navigation path based on the detect result is reliable. This study provides valuable reference for the realization of intelligent inspection in orchards.},
	urldate = {2025-03-05},
	journal = {IEEE Access},
	author = {Cao, Ziang and Gong, Changzhi and Meng, Junjie and Liu, Lu and Rao, Yuan and Hou, Wenhui},
	year = {2024},
	note = {Conference Name: IEEE Access},
	keywords = {Convolutional neural networks, EMA, Feature extraction, least square method, Least squares approximations, Lion, MPDIoU, Navigation, navigation line extraction, Object segmentation, Robot kinematics, Robustness, trunk detection, YOLO, YOLOv8},
	pages = {104126--104137},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\FK9PZ966\\10583867.html:text/html},
}

@article{liu_single-stage_2025,
	title = {A {Single}-{Stage} {Navigation} {Path} {Extraction} {Network} for agricultural robots in orchards},
	volume = {229},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169924010780},
	doi = {10.1016/j.compag.2024.109687},
	abstract = {The real-time and precise extraction of navigation paths holds significant importance in ensuring the autonomous navigation of agricultural robots. Although widely used in orchards, path extraction for agricultural robots remains a complex, multi-stage process. To address the limitations of current vision-based algorithms, this paper proposes a novel approach: the Single-Stage Navigation Path Extraction Network (NPENet). NPENet simplifies the path extraction process by reducing unnecessary parameterization and redefining the road centerline as the neural network’s primary prediction target, with a corresponding tailored loss function. Utilizing residual modules, NPENet effectively extracts navigation path features in orchard environments. The model’s performance is further enhanced by optimizing the network structure. A dataset of 25,720 images from various orchard scenes was used to train and test the model. Experimental results demonstrate that NPENet achieves 92.14\% accuracy in road centerline detection and 91.6\% recall, with a detection speed of 10.1 ms per 448x448 pixel frame on a Jetson Xavier, and a parameter size of only 1.5 M. These findings show that NPENet outperforms existing visual detection and segmentation methods, providing efficient and accurate road information for mobile robots in orchard environments. This approach offers a promising solution for autonomous navigation in agriculture.},
	urldate = {2025-03-05},
	journal = {Computers and Electronics in Agriculture},
	author = {Liu, Hui and Zeng, Xiao and Shen, Yue and Xu, Jie and Khan, Zohaib},
	month = feb,
	year = {2025},
	keywords = {Deep learning, Machine vision, Navigation path detection, Orchard mobile robots},
	pages = {109687},
}

@article{asuzu_humanrobot_2025,
	title = {Human–robot interaction through joint robot planning with large language models},
	issn = {1861-2784},
	url = {https://doi.org/10.1007/s11370-024-00570-1},
	doi = {10.1007/s11370-024-00570-1},
	abstract = {Large language models (LLMs) have demonstrated remarkable zero-shot generalisation capabilities, expanding their utility beyond natural language processing into various applications. Leveraging extensive web knowledge, these models generate meaningful text data in response to user-defined prompts, introducing a novel mode of interaction with software applications. Recent investigations have extended the generalisability of LLMs into the domain of robotics, addressing challenges in existing robot learning techniques such as reinforcement learning and imitation learning. This paper explores the application of LLMs for robot planning as an alternative approach to generate high-level robot plans based on prompts provided to the language model. The proposed methodology facilitates continuous user interaction and adjustment of task execution plans in real time. A pre-trained LLM is utilised for collaborative human–robot planning using natural language, complemented by vision language models (VLMs) responsible for generating scene descriptions incorporated into the prompt for contextual grounding. Evaluation of the system is conducted within the VIMABench benchmark simulated environment. Further practicality assessment involves experimentation with the system on a robotic arm engaged in tabletop manipulation activities. The observed results reveal that soliciting human feedback for zero-shot plans generated by LLMs in robot manipulation yields an 8.6\% performance increase in simulated evaluations and a 14\% increase in physical evaluations compared to the baseline, showcasing the efficacy of the proposed approach.},
	language = {en},
	urldate = {2025-03-05},
	journal = {Intelligent Service Robotics},
	author = {Asuzu, Kosi and Singh, Harjinder and Idrissi, Moad},
	month = jan,
	year = {2025},
	keywords = {Natural language processing, Computer vision, Artificial Intelligence, Robotics, Large language models, Human–robot interaction, Robot arm},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\MRFNBWYQ\\Asuzu et al. - 2025 - Human–robot interaction through joint robot planni.pdf:application/pdf},
}

@incollection{allgeuer_when_2024,
	title = {When {Robots} {Get} {Chatty}: {Grounding} {Multimodal} {Human}-{Robot} {Conversation} and {Collaboration}},
	volume = {15019},
	shorttitle = {When {Robots} {Get} {Chatty}},
	url = {http://arxiv.org/abs/2407.00518},
	abstract = {We investigate the use of Large Language Models (LLMs) to equip neural robotic agents with human-like social and cognitive competencies, for the purpose of open-ended human-robot conversation and collaboration. We introduce a modular and extensible methodology for grounding an LLM with the sensory perceptions and capabilities of a physical robot, and integrate multiple deep learning models throughout the architecture in a form of system integration. The integrated models encompass various functions such as speech recognition, speech generation, open-vocabulary object detection, human pose estimation, and gesture detection, with the LLM serving as the central text-based coordinating unit. The qualitative and quantitative results demonstrate the huge potential of LLMs in providing emergent cognition and interactive language-oriented control of robots in a natural and social manner.},
	urldate = {2025-03-05},
	author = {Allgeuer, Philipp and Ali, Hassan and Wermter, Stefan},
	year = {2024},
	doi = {10.1007/978-3-031-72341-4_21},
	note = {arXiv:2407.00518 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {306--321},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\DVC8LS44\\Allgeuer et al. - 2024 - When Robots Get Chatty Grounding Multimodal Human.pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\7LGLPJYW\\2407.html:text/html},
}

@article{huang_survey_2025,
	title = {A {Survey} on {Hallucination} in {Large} {Language} {Models}: {Principles}, {Taxonomy}, {Challenges}, and {Open} {Questions}},
	volume = {43},
	issn = {1046-8188},
	shorttitle = {A {Survey} on {Hallucination} in {Large} {Language} {Models}},
	url = {https://doi.org/10.1145/3703155},
	doi = {10.1145/3703155},
	abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
	number = {2},
	urldate = {2025-03-06},
	journal = {ACM Trans. Inf. Syst.},
	author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
	month = jan,
	year = {2025},
	pages = {42:1--42:55},
	file = {Submitted Version:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\SDTV27US\\Huang et al. - 2025 - A Survey on Hallucination in Large Language Models.pdf:application/pdf},
}

@misc{wang_i_2024,
	title = {I {Can} {Tell} {What} {I} am {Doing}: {Toward} {Real}-{World} {Natural} {Language} {Grounding} of {Robot} {Experiences}},
	shorttitle = {I {Can} {Tell} {What} {I} am {Doing}},
	url = {http://arxiv.org/abs/2411.12960},
	doi = {10.48550/arXiv.2411.12960},
	abstract = {Understanding robot behaviors and experiences through natural language is crucial for developing intelligent and transparent robotic systems. Recent advancement in large language models (LLMs) makes it possible to translate complex, multi-modal robotic experiences into coherent, human-readable narratives. However, grounding real-world robot experiences into natural language is challenging due to many reasons, such as multi-modal nature of data, differing sample rates, and data volume. We introduce RONAR, an LLM-based system that generates natural language narrations from robot experiences, aiding in behavior announcement, failure analysis, and human interaction to recover failure. Evaluated across various scenarios, RONAR outperforms state-of-the-art methods and improves failure recovery efficiency. Our contributions include a multi-modal framework for robot experience narration, a comprehensive real-robot dataset, and empirical evidence of RONAR's effectiveness in enhancing user experience in system transparency and failure analysis.},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Wang, Zihan and Liang, Brian and Dhat, Varad and Brumbaugh, Zander and Walker, Nick and Krishna, Ranjay and Cakmak, Maya},
	month = nov,
	year = {2024},
	note = {arXiv:2411.12960 [cs]
version: 1},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\XKH2XL6A\\Wang et al. - 2024 - I Can Tell What I am Doing Toward Real-World Natu.pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\C47373U4\\2411.html:text/html},
}

@misc{chen_synergai_2024,
	title = {{SYNERGAI}: {Perception} {Alignment} for {Human}-{Robot} {Collaboration}},
	shorttitle = {{SYNERGAI}},
	url = {http://arxiv.org/abs/2409.15684},
	doi = {10.48550/arXiv.2409.15684},
	abstract = {Recently, large language models (LLMs) have shown strong potential in facilitating human-robotic interaction and collaboration. However, existing LLM-based systems often overlook the misalignment between human and robot perceptions, which hinders their effective communication and real-world robot deployment. To address this issue, we introduce SYNERGAI, a unified system designed to achieve both perceptual alignment and human-robot collaboration. At its core, SYNERGAI employs 3D Scene Graph (3DSG) as its explicit and innate representation. This enables the system to leverage LLM to break down complex tasks and allocate appropriate tools in intermediate steps to extract relevant information from the 3DSG, modify its structure, or generate responses. Importantly, SYNERGAI incorporates an automatic mechanism that enables perceptual misalignment correction with users by updating its 3DSG with online interaction. SYNERGAI achieves comparable performance with the data-driven models in ScanQA in a zero-shot manner. Through comprehensive experiments across 10 real-world scenes, SYNERGAI demonstrates its effectiveness in establishing common ground with humans, realizing a success rate of 61.9\% in alignment tasks. It also significantly improves the success rate from 3.7\% to 45.68\% on novel tasks by transferring the knowledge acquired during alignment.},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Chen, Yixin and Zhang, Guoxi and Zhang, Yaowei and Xu, Hongming and Zhi, Peiyuan and Li, Qing and Huang, Siyuan},
	month = sep,
	year = {2024},
	note = {arXiv:2409.15684 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\5ZFQ8YVM\\Chen et al. - 2024 - SYNERGAI Perception Alignment for Human-Robot Col.pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\V9PXL4ND\\2409.html:text/html},
}

### Robotics end 

### Interactive tools end

@misc{noauthor_poloclubcnn-explainer_2025,
	title = {poloclub/cnn-explainer},
	copyright = {MIT},
	url = {https://github.com/poloclub/cnn-explainer},
	abstract = {Learning Convolutional Neural Networks with Interactive Visualization.},
	urldate = {2025-03-07},
	publisher = {Polo Club of Data Science, Georgia Tech},
	month = mar,
	year = {2025},
	note = {original-date: 2019-11-03T23:15:24Z},
	keywords = {deep-learning, interactive-visualizations, machine-learning, visual-learning, visualization},
}

@misc{noauthor_poloclubtransformer-explainer_2025,
	title = {poloclub/transformer-explainer},
	copyright = {MIT},
	url = {https://github.com/poloclub/transformer-explainer},
	abstract = {Transformer Explained Visually: Learn How LLM Transformer Models Work with Interactive Visualization},
	urldate = {2025-03-05},
	publisher = {Polo Club of Data Science, Georgia Tech},
	month = mar,
	year = {2025},
	note = {original-date: 2024-05-16T21:42:10Z},
	keywords = {deep-learning, generative-ai, gpt, langauge-model, llm, visualization},
}

@misc{carter_tensorflow_nodate,
	title = {Tensorflow — {Neural} {Network} {Playground}},
	url = {http://playground.tensorflow.org},
	abstract = {Tinker with a real neural network right here in your browser.},
	urldate = {2025-03-05},
	author = {Carter, Daniel Smilkov {and} Shan},
	file = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\EZFWF5Z2\\playground.tensorflow.org.html:text/html},
}

@misc{guo_ai_nodate,
	title = {{AI} {Notes}: {Initializing} neural networks},
	shorttitle = {{AI} {Notes}},
	url = {https://www.deeplearning.ai/ai-notes/initialization/},
	abstract = {In this post, we'll explain how to initialize neural network parameters effectively. Initialization can have a significant impact on convergence in training deep neural networks...},
	urldate = {2025-03-07},
	journal = {deeplearning.ai},
	author = {Guo, Jingru},
	file = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\37QYFV4Q\\index.html:text/html},
}

### Interactive tools end

### Future trend begin

@article{marra_statistical_2024,
	title = {From statistical relational to neurosymbolic artificial intelligence: {A} survey},
	volume = {328},
	issn = {0004-3702},
	shorttitle = {From statistical relational to neurosymbolic artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370223002084},
	doi = {10.1016/j.artint.2023.104062},
	abstract = {This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neurosymbolic and statistical relational artificial intelligence. Neurosymbolic artificial intelligence (NeSy) studies the integration of symbolic reasoning and neural networks, while statistical relational artificial intelligence (StarAI) focuses on integrating logic with probabilistic graphical models. This survey identifies seven shared dimensions between these two subfields of AI. These dimensions can be used to characterize different NeSy and StarAI systems. They are concerned with (1) the approach to logical inference, whether model or proof-based; (2) the syntax of the used logical theories; (3) the logical semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either parameter or structure learning; (5) the presence of symbolic and subsymbolic representations; (6) the degree to which systems capture the original logic, probabilistic, and neural paradigms; and (7) the classes of learning tasks the systems are applied to. By positioning various NeSy and StarAI systems along these dimensions and pointing out similarities and differences between them, this survey contributes fundamental concepts for understanding the integration of learning and reasoning.},
	urldate = {2025-03-10},
	journal = {Artificial Intelligence},
	author = {Marra, Giuseppe and Dumančić, Sebastijan and Manhaeve, Robin and De Raedt, Luc},
	month = mar,
	year = {2024},
	keywords = {Learning and reasoning, Neurosymbolic AI, Probabilistic logics, Statistical relational AI},
	pages = {104062},
	file = {Full Text:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\35VQ8KNJ\\Marra et al. - 2024 - From statistical relational to neurosymbolic artif.pdf:application/pdf;ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\GHRJ4ZVL\\S0004370223002084.html:text/html},
}

@misc{colelough_neuro-symbolic_2025,
	title = {Neuro-{Symbolic} {AI} in 2024: {A} {Systematic} {Review}},
	shorttitle = {Neuro-{Symbolic} {AI} in 2024},
	url = {http://arxiv.org/abs/2501.05435},
	doi = {10.48550/arXiv.2501.05435},
	abstract = {Background: The field of Artificial Intelligence has undergone cyclical periods of growth and decline, known as AI summers and winters. Currently, we are in the third AI summer, characterized by significant advancements and commercialization, particularly in the integration of Symbolic AI and Sub-Symbolic AI, leading to the emergence of Neuro-Symbolic AI. Methods: The review followed the PRISMA methodology, utilizing databases such as IEEE Explore, Google Scholar, arXiv, ACM, and SpringerLink. The inclusion criteria targeted peer-reviewed papers published between 2020 and 2024. Papers were screened for relevance to Neuro-Symbolic AI, with further inclusion based on the availability of associated codebases to ensure reproducibility. Results: From an initial pool of 1,428 papers, 167 met the inclusion criteria and were analyzed in detail. The majority of research efforts are concentrated in the areas of learning and inference (63\%), logic and reasoning (35\%), and knowledge representation (44\%). Explainability and trustworthiness are less represented (28\%), with Meta-Cognition being the least explored area (5\%). The review identifies significant interdisciplinary opportunities, particularly in integrating explainability and trustworthiness with other research areas. Conclusion: Neuro-Symbolic AI research has seen rapid growth since 2020, with concentrated efforts in learning and inference. Significant gaps remain in explainability, trustworthiness, and Meta-Cognition. Addressing these gaps through interdisciplinary research will be crucial for advancing the field towards more intelligent, reliable, and context-aware AI systems.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Colelough, Brandon C. and Regli, William},
	month = jan,
	year = {2025},
	note = {arXiv:2501.05435 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\VYGXQFZK\\Colelough and Regli - 2025 - Neuro-Symbolic AI in 2024 A Systematic Review.pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\X9F4VKDQ\\2501.html:text/html},
}

@article{bhuyan_neuro-symbolic_2024,
	title = {Neuro-symbolic artificial intelligence: a survey},
	volume = {36},
	issn = {1433-3058},
	shorttitle = {Neuro-symbolic artificial intelligence},
	url = {https://doi.org/10.1007/s00521-024-09960-z},
	doi = {10.1007/s00521-024-09960-z},
	abstract = {The goal of the growing discipline of neuro-symbolic artificial intelligence (AI) is to develop AI systems with more human-like reasoning capabilities by combining symbolic reasoning with connectionist learning. We survey the literature on neuro-symbolic AI during the last two decades, including books, monographs, review papers, contribution pieces, opinion articles, foundational workshops/talks, and related PhD theses. Four main features of neuro-symbolic AI are discussed, including representation, learning, reasoning, and decision-making. Finally, we discuss the many applications of neuro-symbolic AI, including question answering, robotics, computer vision, healthcare, and more. Scalability, explainability, and ethical considerations are also covered, as well as other difficulties and limits of neuro-symbolic AI. This study summarizes the current state of the art in neuro-symbolic artificial intelligence.},
	language = {en},
	number = {21},
	urldate = {2025-03-10},
	journal = {Neural Computing and Applications},
	author = {Bhuyan, Bikram Pratim and Ramdane-Cherif, Amar and Tomar, Ravi and Singh, T. P.},
	month = jul,
	year = {2024},
	keywords = {Artificial intelligence, Artificial Intelligence, Knowledge representation and reasoning, Machine learning, Neural networks, Neuro-symbolic artificial intelligence, Spatial-temporal data},
	pages = {12809--12844},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\52KJ9HZV\\Bhuyan et al. - 2024 - Neuro-symbolic artificial intelligence a survey.pdf:application/pdf},
}

@article{kudithipudi_neuromorphic_2025,
	title = {Neuromorphic computing at scale},
	volume = {637},
	copyright = {2025 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-08253-8},
	doi = {10.1038/s41586-024-08253-8},
	abstract = {Neuromorphic computing is a brain-inspired approach to hardware and algorithm design that efficiently realizes artificial neural networks. Neuromorphic designers apply the principles of biointelligence discovered by neuroscientists to design efficient computational systems, often for applications with size, weight and power constraints. With this research field at a critical juncture, it is crucial to chart the course for the development of future large-scale neuromorphic systems. We describe approaches for creating scalable neuromorphic architectures and identify key features. We discuss potential applications that can benefit from scaling and the main challenges that need to be addressed. Furthermore, we examine a comprehensive ecosystem necessary to sustain growth and the new opportunities that lie ahead when scaling neuromorphic systems. Our work distils ideas from several computing sub-fields, providing guidance to researchers and practitioners of neuromorphic computing who aim to push the frontier forward.},
	language = {en},
	number = {8047},
	urldate = {2025-03-09},
	journal = {Nature},
	author = {Kudithipudi, Dhireesha and Schuman, Catherine and Vineyard, Craig M. and Pandit, Tej and Merkel, Cory and Kubendran, Rajkumar and Aimone, James B. and Orchard, Garrick and Mayr, Christian and Benosman, Ryad and Hays, Joe and Young, Cliff and Bartolozzi, Chiara and Majumdar, Amitava and Cardwell, Suma George and Payvand, Melika and Buckley, Sonia and Kulkarni, Shruti and Gonzalez, Hector A. and Cauwenberghs, Gert and Thakur, Chetan Singh and Subramoney, Anand and Furber, Steve},
	month = jan,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Electrical and electronic engineering, Electronic devices, Learning algorithms, Network models},
	pages = {801--812},
}

@article{wu_survey_2025,
	title = {A survey on quantum deep learning},
	volume = {81},
	issn = {1573-0484},
	url = {https://doi.org/10.1007/s11227-025-07083-3},
	doi = {10.1007/s11227-025-07083-3},
	abstract = {Quantum deep learning (QDL), which combines the unique strengths of quantum computing and deep learning, is gradually becoming a focal point. It offers new ideas for addressing the many challenges currently faced. In this survey, we review the representative algorithms that have combined quantum computing and deep learning in recent years. Firstly, we categorize the discussion based on data types into three areas: text, image, and multimodal data. We focus on QDL algorithms within these categories and explore their characteristics. Secondly, this paper compares the performance of the QDL model with the traditional model. By comparison, QDL not only demonstrates enhanced feature extraction capabilities but is also able to handle more complex data. In addition, the unique properties of quantum computing, such as quantum superposition and quantum entanglement, can accelerate calculations and improve model performance. These advantages demonstrate its potential efficiency over traditional methods. Finally, a summary and outlook on the prevailing research conditions in QDL have been given. This article integrates current research findings in QDL, providing a clear research background for subsequent researchers.},
	language = {en},
	number = {4},
	urldate = {2025-03-09},
	journal = {The Journal of Supercomputing},
	author = {Wu, Huaiguang and Zhang, Jiahui and Wang, Lijie and Li, Daiyi and Kong, Delong and Han, Yucan},
	month = mar,
	year = {2025},
	keywords = {Machine learning, Quantum computing, Quantum Computing, Quantum deep learning, Quantum machine learning},
	pages = {564},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\RDP9TUB9\\Wu et al. - 2025 - A survey on quantum deep learning.pdf:application/pdf},
}

@article{fei_towards_2022,
	title = {Towards artificial general intelligence via a multimodal foundation model},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-30761-2},
	doi = {10.1038/s41467-022-30761-2},
	abstract = {The fundamental goal of artificial intelligence (AI) is to mimic the core cognitive activities of human. Despite tremendous success in the AI research, most of existing methods have only single-cognitive ability. To overcome this limitation and take a solid step towards artificial general intelligence (AGI), we develop a foundation model pre-trained with huge multimodal data, which can be quickly adapted for various downstream cognitive tasks. To achieve this goal, we propose to pre-train our foundation model by self-supervised learning with weak semantic correlation data crawled from the Internet and show that promising results can be obtained on a wide range of downstream tasks. Particularly, with the developed model-interpretability tools, we demonstrate that strong imagination ability is now possessed by our foundation model. We believe that our work makes a transformative stride towards AGI, from our common practice of “weak or narrow AI” to that of “strong or generalized AI”.},
	language = {en},
	number = {1},
	urldate = {2025-03-09},
	journal = {Nature Communications},
	author = {Fei, Nanyi and Lu, Zhiwu and Gao, Yizhao and Yang, Guoxing and Huo, Yuqi and Wen, Jingyuan and Lu, Haoyu and Song, Ruihua and Gao, Xin and Xiang, Tao and Sun, Hao and Wen, Ji-Rong},
	month = jun,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Computational science, Learning algorithms},
	pages = {3094},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\P2E42E9D\\Fei et al. - 2022 - Towards artificial general intelligence via a mult.pdf:application/pdf},
}

@misc{butlin_consciousness_2023,
	title = {Consciousness in {Artificial} {Intelligence}: {Insights} from the {Science} of {Consciousness}},
	shorttitle = {Consciousness in {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2308.08708},
	doi = {10.48550/arXiv.2308.08708},
	abstract = {Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.},
	urldate = {2025-03-09},
	publisher = {arXiv},
	author = {Butlin, Patrick and Long, Robert and Elmoznino, Eric and Bengio, Yoshua and Birch, Jonathan and Constant, Axel and Deane, George and Fleming, Stephen M. and Frith, Chris and Ji, Xu and Kanai, Ryota and Klein, Colin and Lindsay, Grace and Michel, Matthias and Mudrik, Liad and Peters, Megan A. K. and Schwitzgebel, Eric and Simon, Jonathan and VanRullen, Rufin},
	month = aug,
	year = {2023},
	note = {arXiv:2308.08708 [cs]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\VKSEAZ9W\\Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\W7IJ5XW2\\2308.html:text/html},
}

@article{klusch_quantum_2024,
	title = {Quantum {Artificial} {Intelligence}: {A} {Brief} {Survey}},
	volume = {38},
	issn = {1610-1987},
	shorttitle = {Quantum {Artificial} {Intelligence}},
	url = {https://doi.org/10.1007/s13218-024-00871-8},
	doi = {10.1007/s13218-024-00871-8},
	abstract = {Quantum Artificial Intelligence (QAI) is the intersection of quantum computing and AI, a technological synergy with expected significant benefits for both. In this paper, we provide a brief overview of what has been achieved in QAI so far and point to some open questions for future research. In particular, we summarize some major key findings on the feasability and the potential of using quantum computing for solving computationally hard problems in various subfields of AI, and vice versa, the leveraging of AI methods for building and operating quantum computing devices.},
	language = {en},
	number = {4},
	urldate = {2025-03-09},
	journal = {KI - Künstliche Intelligenz},
	author = {Klusch, Matthias and Lässig, Jörg and Müssig, Daniel and Macaluso, Antonio and Wilhelm, Frank K.},
	month = dec,
	year = {2024},
	keywords = {Artificial Intelligence, Quantum computing, Quantum Computing, 68Q12: Quantum algorithms, 68T01: General AI, AI, Quantum AI},
	pages = {257--276},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\5X6FZ7UR\\Klusch et al. - 2024 - Quantum Artificial Intelligence A Brief Survey.pdf:application/pdf},
}

@article{shrestha_survey_2022,
	title = {A {Survey} on {Neuromorphic} {Computing}: {Models} and {Hardware}},
	volume = {22},
	issn = {1558-0830},
	shorttitle = {A {Survey} on {Neuromorphic} {Computing}},
	url = {https://ieeexplore.ieee.org/document/9782767},
	doi = {10.1109/MCAS.2022.3166331},
	abstract = {The explosion of “big data” applications imposes severe challenges of speed and scalability on traditional computer systems. As the performance of traditional Von Neumann machines is greatly hindered by the increasing performance gap between CPU and memory (“known as the memory wall”), neuromorphic computing systems have gained considerable attention. The biology-plausible computing paradigm carries out computing by emulating the charging/discharging process of neuron and synapse potential. The unique spike domain information encoding enables asynchronous event driven computation and communication, and hence has the potential for very high energy efficiency. This survey reviews computing models and hardware platforms of existing neuromorphic computing systems. Neuron and synapse models are first introduced, followed by the discussion on how they will affect hardware design. Case studies of several representative hardware platforms, including their architecture and software ecosystems, are further presented. Lastly we present several future research directions.},
	number = {2},
	urldate = {2025-03-10},
	journal = {IEEE Circuits and Systems Magazine},
	author = {Shrestha, Amar and Fang, Haowen and Mei, Zaidao and Rider, Daniel Patrick and Wu, Qing and Qiu, Qinru},
	year = {2022},
	note = {Conference Name: IEEE Circuits and Systems Magazine},
	keywords = {Biological system modeling, Computational modeling, Computer architecture, Encoding, Hardware, Neuromorphic engineering, Neurons},
	pages = {6--35},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\PUFT6RJV\\9782767.html:text/html},
}

@article{jimale_subject_2023,
	title = {Subject variability in sensor-based activity recognition},
	volume = {14},
	issn = {1868-5145},
	url = {https://doi.org/10.1007/s12652-021-03465-6},
	doi = {10.1007/s12652-021-03465-6},
	abstract = {Building classification models in activity recognition is based on the concept of exchangeability. While splitting the dataset into training and test sets, we assume that the training set is exchangeable with the test set and expect good classification performance. However, this assumption is invalid due to subject variability of the training and test sets due to age differences. This happens when the classification models are trained with adult dataset and tested it with elderly dataset. This study investigates the effects of subject variability on activity recognition using inertial sensor. Two different datasets—one locally collected from 15 elders and another public from 30 adults with eight types of activities—were used to evaluate the assessment techniques using ten-fold cross-validation. Three sets of experiments have been conducted: experiments on the public dataset only, experiments on the local dataset only, and experiments on public (as training) and local (as test) datasets using machine learning and deep learning classifiers including single classifiers (Support Vector Machine, Decision Tree, K-Nearest Neighbors), ensemble classifiers (Adaboost, Random Forest, and XGBoost), and Convolutional Neural Network. The experimental results show that there is a significant performance drop in activity recognition on different subjects with different age groups. It demonstrates that on average the drop in recognition accuracy is 9.75 and 12\% for machine learning and deep learning models respectively. This confirms that subject variability concerning age is a valid problem that degrades the performance of activity recognition models.},
	language = {en},
	number = {4},
	urldate = {2025-03-16},
	journal = {Journal of Ambient Intelligence and Humanized Computing},
	author = {Jimale, Ali Olow and Mohd Noor, Mohd Halim},
	month = apr,
	year = {2023},
	keywords = {Activity recognition, Artificial Intelligence, Deep learning, Machine learning, Subject variability},
	pages = {3261--3274},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\W8Q4MBZN\\Jimale and Mohd Noor - 2023 - Subject variability in sensor-based activity recog.pdf:application/pdf},
}

@article{zhang_large_2023,
	title = {Large language models for human–robot interaction: {A} review},
	volume = {3},
	issn = {2667-3797},
	shorttitle = {Large language models for human–robot interaction},
	url = {https://www.sciencedirect.com/science/article/pii/S2667379723000451},
	doi = {10.1016/j.birob.2023.100131},
	abstract = {The fusion of large language models and robotic systems has introduced a transformative paradigm in human–robot interaction, offering unparalleled capabilities in natural language understanding and task execution. This review paper offers a comprehensive analysis of this nascent but rapidly evolving domain, spotlighting the recent advances of Large Language Models (LLMs) in enhancing their structures and performances, particularly in terms of multimodal input handling, high-level reasoning, and plan generation. Moreover, it probes the current methodologies that integrate LLMs into robotic systems for complex task completion, from traditional probabilistic models to the utilization of value functions and metrics for optimal decision-making. Despite these advancements, the paper also reveals the formidable challenges that confront the field, such as contextual understanding, data privacy and ethical considerations. To our best knowledge, this is the first study to comprehensively analyze the advances and considerations of LLMs in Human–Robot Interaction (HRI) based on recent progress, which provides potential avenues for further research.},
	number = {4},
	urldate = {2025-03-16},
	journal = {Biomimetic Intelligence and Robotics},
	author = {Zhang, Ceng and Chen, Junxin and Li, Jiatong and Peng, Yanhong and Mao, Zebing},
	month = dec,
	year = {2023},
	keywords = {Considerations and challenges, Human–robot interaction, Large language models, Task completion},
	pages = {100131},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\GQ4MBZJZ\\S2667379723000451.html:text/html},
}

### Future trend end

### Challenges domain-specific annotation begin

@misc{li_survey_2024,
	title = {A {Survey} on {Deep} {Active} {Learning}: {Recent} {Advances} and {New} {Frontiers}},
	shorttitle = {A {Survey} on {Deep} {Active} {Learning}},
	url = {http://arxiv.org/abs/2405.00334},
	doi = {10.48550/arXiv.2405.00334},
	abstract = {Active learning seeks to achieve strong performance with fewer training samples. It does this by iteratively asking an oracle to label new selected samples in a human-in-the-loop manner. This technique has gained increasing popularity due to its broad applicability, yet its survey papers, especially for deep learning-based active learning (DAL), remain scarce. Therefore, we conduct an advanced and comprehensive survey on DAL. We first introduce reviewed paper collection and filtering. Second, we formally define the DAL task and summarize the most influential baselines and widely used datasets. Third, we systematically provide a taxonomy of DAL methods from five perspectives, including annotation types, query strategies, deep model architectures, learning paradigms, and training processes, and objectively analyze their strengths and weaknesses. Then, we comprehensively summarize main applications of DAL in Natural Language Processing (NLP), Computer Vision (CV), and Data Mining (DM), etc. Finally, we discuss challenges and perspectives after a detailed analysis of current studies. This work aims to serve as a useful and quick guide for researchers in overcoming difficulties in DAL. We hope that this survey will spur further progress in this burgeoning field.},
	urldate = {2025-03-13},
	publisher = {arXiv},
	author = {Li, Dongyuan and Wang, Zhen and Chen, Yankai and Jiang, Renhe and Ding, Weiping and Okumura, Manabu},
	month = jul,
	year = {2024},
	note = {arXiv:2405.00334 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\PXDB7WJP\\Li et al. - 2024 - A Survey on Deep Active Learning Recent Advances .pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\NEFWP2KF\\2405.html:text/html},
}

@article{cacciarelli_active_2024,
	title = {Active learning for data streams: a survey},
	volume = {113},
	issn = {1573-0565},
	shorttitle = {Active learning for data streams},
	url = {https://doi.org/10.1007/s10994-023-06454-2},
	doi = {10.1007/s10994-023-06454-2},
	abstract = {Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream. The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form. Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models. These approaches can be broadly divided into two categories: static pool-based and stream-based active learning. Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews. However, the growing availability of data streams has led to an increase in the number of approaches that focus on online active learning, which involves continuously selecting and labeling observations as they arrive in a stream. This work aims to provide an overview of the most recently proposed approaches for selecting the most informative observations from data streams in real time. We review the various techniques that have been proposed and discuss their strengths and limitations, as well as the challenges and opportunities that exist in this area of research.},
	language = {en},
	number = {1},
	urldate = {2025-03-13},
	journal = {Machine Learning},
	author = {Cacciarelli, Davide and Kulahci, Murat},
	month = jan,
	year = {2024},
	keywords = {Artificial Intelligence, Bandits, Concept drift, Data streams, Experimental design, Online active learning, Online learning, Query strategies, Selective sampling, Stream-based active learning, Unlabeled data},
	pages = {185--239},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\W8KJEJX4\\Cacciarelli and Kulahci - 2024 - Active learning for data streams a survey.pdf:application/pdf},
}

@misc{tan_large_2024,
	title = {Large {Language} {Models} for {Data} {Annotation}: {A} {Survey}},
	shorttitle = {Large {Language} {Models} for {Data} {Annotation}},
	url = {http://arxiv.org/abs/2402.13446},
	doi = {10.48550/arXiv.2402.13446},
	abstract = {Data annotation generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to automate the complicated process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, we uniquely focus on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment, and LLM-Generated Annotations Utilization. Furthermore, this survey includes an in-depth taxonomy of data types that LLMs can annotate, a comprehensive review of learning strategies for models utilizing LLM-generated annotations, and a detailed discussion of the primary challenges and limitations associated with using LLMs for data annotation. Serving as a key guide, this survey aims to assist researchers and practitioners in exploring the potential of the latest LLMs for data annotation, thereby fostering future advancements in this critical field.},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Tan, Zhen and Li, Dawei and Wang, Song and Beigi, Alimohammad and Jiang, Bohan and Bhattacharjee, Amrita and Karami, Mansooreh and Li, Jundong and Cheng, Lu and Liu, Huan},
	month = jun,
	year = {2024},
	note = {arXiv:2402.13446 [cs]
version: 2},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\F7HHM9JH\\Tan et al. - 2024 - Large Language Models for Data Annotation A Surve.pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\4DLB9NZH\\2402.html:text/html},
}

@article{altabrawee_stclr_2025,
	title = {{STCLR}: {Sparse} {Temporal} {Contrastive} {Learning} for {Video} {Representation}},
	volume = {630},
	issn = {0925-2312},
	shorttitle = {{STCLR}},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231225003662},
	doi = {10.1016/j.neucom.2025.129694},
	abstract = {Temporal Contrastive Learning for Video Representation (TCLR) is the first contrastive framework that uses temporal losses to enforce the temporal distinctiveness of the features explicitly. However, its local–local temporal contrastive loss contrasts non-overlapping dense adjacent clips that cover small motion dynamics due to temporal coherence in videos. These adjacent local clips could represent the same action phase, and that makes the local–local loss enforce temporal distinctiveness of the features within the same action phase, whereas much better distinctiveness comes from contrasting different action phases. To overcome these limitations, Sparse TCLR framework (STCLR) is proposed, which uses random sparse sampling to sample local clips that cover different motion dynamics of the video. Using sparse local clips with the local–local loss overcomes the need for the second TCLR temporal loss, the global–local loss. In addition, a novel temporal pretext (Same Speed Localization) is proposed for intra/inter action temporal self-supervised learning and combined with the temporal loss. We prove that TCLR can be simplified to a framework that uses two losses only instead of three. STCLR achieves better feature transferability by a significant margin than TCLR on video retrieval. STCLR outperforms TCLR by 0.58\%, 1.21\%, 0.88\%, and 0.98\% and by 4.32\%, 5.97\%, 6.51\%, and 2.78\% on Top 1, Top 5, Top 10, and Top 20 retrieval measures on UCF-101 and HMDB-51, respectively. On action recognition, STCLR outperforms TCLR on Diving-48 by 9.68\%, which is a significant improvement, and on HMDB-51 by 0.83\%, while it achieves a lower accuracy by 0.72\% on UCF-101.},
	urldate = {2025-03-13},
	journal = {Neurocomputing},
	author = {Altabrawee, Hussein and Mohd Noor, Mohd Halim},
	month = may,
	year = {2025},
	keywords = {Action recognition, Self-supervised learning, TCLR, Visual representations learning},
	pages = {129694},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\X5KZEB9R\\S0925231225003662.html:text/html},
}

@article{altabrawee_repeat_2024,
	title = {Repeat and learn: {Self}-supervised visual representations learning by {Repeated} {Scene} {Localization}},
	volume = {156},
	issn = {0031-3203},
	shorttitle = {Repeat and learn},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320324005557},
	doi = {10.1016/j.patcog.2024.110804},
	abstract = {Large labeled datasets are crucial for video understanding progress. However, the labeling process is time-consuming, expensive, and tiresome. To overcome this impediment, various pretexts use the temporal coherence in videos to learn visual representations in a self-supervised manner. However, these pretexts (order verification and sequence sorting) struggle when encountering cyclic actions due to the label ambiguity problem. To overcome these limitations, we present a novel temporal pretext task to address self-supervised learning of visual representations from unlabeled videos. Repeated Scene Localization (RSL) is a multi-class classification pretext that involves changing the temporal order of the frames in a video by repeating a scene. Then, the network is trained to identify the modified video, localize the location of the repeated scene, and identify the unmodified original videos that do not have repeated scenes. We evaluated the proposed pretext on two benchmark datasets, UCF-101 and HMDB-51. The experimental results show that the proposed pretext achieves state-of-the-art results in action recognition and video retrieval tasks. In action recognition, our S3D model achieves 88.15\% and 56.86\% on UCF-101 and HMDB-51, respectively. It outperforms the current state-of-the-art by 1.05\% and 3.26\%. Our R(2+1)D-Adjacent model achieves 83.52\% and 54.50\% on UCF-101 and HMDB-51, respectively. It outperforms the single pretext tasks by 8.7\% and 13.9\%. In video retrieval, our R(2+1)D-Offset model outperforms the single pretext tasks by 4.68\% and 1.1\% Top 1 accuracies on UCF-101 and HMDB-51, respectively. The source code and the trained models are publicly available at https://github.com/Hussein-A-Hassan/RSL-Pretext.},
	urldate = {2025-03-13},
	journal = {Pattern Recognition},
	author = {Altabrawee, Hussein and Mohd Noor, Mohd Halim},
	month = dec,
	year = {2024},
	keywords = {Action recognition, Self-supervised learning, Visual representations learning},
	pages = {110804},
	file = {ScienceDirect Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\M23DTDV6\\S0031320324005557.html:text/html},
}

@article{gui_survey_2024,
	title = {A {Survey} on {Self}-{Supervised} {Learning}: {Algorithms}, {Applications}, and {Future} {Trends}},
	volume = {46},
	issn = {1939-3539},
	shorttitle = {A {Survey} on {Self}-{Supervised} {Learning}},
	url = {https://ieeexplore.ieee.org/document/10559458},
	doi = {10.1109/TPAMI.2024.3415112},
	abstract = {Deep supervised learning algorithms typically require a large volume of labeled data to achieve satisfactory performance. However, the process of collecting and labeling such data can be expensive and time-consuming. Self-supervised learning (SSL), a subset of unsupervised learning, aims to learn discriminative features from unlabeled data without relying on human-annotated labels. SSL has garnered significant attention recently, leading to the development of numerous related algorithms. However, there is a dearth of comprehensive studies that elucidate the connections and evolution of different SSL variants. This paper presents a review of diverse SSL methods, encompassing algorithmic aspects, application domains, three key trends, and open research questions. First, we provide a detailed introduction to the motivations behind most SSL algorithms and compare their commonalities and differences. Second, we explore representative applications of SSL in domains such as image processing, computer vision, and natural language processing. Lastly, we discuss the three primary trends observed in SSL research and highlight the open questions that remain.},
	number = {12},
	urldate = {2025-03-13},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gui, Jie and Chen, Tuo and Zhang, Jing and Cao, Qiong and Sun, Zhenan and Luo, Hao and Tao, Dacheng},
	month = dec,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {contrastive learning, generative model, Image recognition, Machine learning algorithms, Market research, representation learning, Reviews, Self-supervised learning, Supervised learning, Task analysis, Training, transfer learning},
	pages = {9052--9071},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\2WFS9GUV\\10559458.html:text/html;Submitted Version:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\FUS5J4WY\\Gui et al. - 2024 - A Survey on Self-Supervised Learning Algorithms, .pdf:application/pdf},
}

@inproceedings{lee_predicting_2021,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '21},
	title = {Predicting what you already know helps: provable self-supervised learning},
	isbn = {978-1-71384-539-3},
	shorttitle = {Predicting what you already know helps},
	abstract = {Self-supervised representation learning solves auxiliary prediction tasks (known as pretext tasks) without requiring labeled data to learn useful semantic representations. These pretext tasks are created solely using the input features, such as predicting a missing image patch, recovering the color channels of an image from context, or predicting missing words in text; yet predicting this known information helps in learning representations effective for downstream prediction tasks.We posit a mechanism exploiting the statistical connections between certain reconstruction-based pretext tasks that guarantee to learn a good representation. Formally, we quantify how the approximate independence between the components of the pretext task (conditional on the label and latent variables) allows us to learn representations that can solve the downstream task by just training a linear layer on top of the learned representation. We prove the linear layer yields small approximation error even for complex ground truth function class and will drastically reduce labeled sample complexity.},
	urldate = {2025-03-12},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Lee, Jason D. and Lei, Qi and Saunshi, Nikunj and Zhuo, Jiacheng},
	month = dec,
	year = {2021},
	pages = {309--323},
}

### Challenges domain-specific annotation end

### Challenges technical training begin

@article{ying_enhancing_2024,
	title = {Enhancing deep neural network training efficiency and performance through linear prediction},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-65691-0},
	doi = {10.1038/s41598-024-65691-0},
	abstract = {Deep neural networks have achieved remarkable success in various fields. However, training an effective deep neural network still poses challenges. This paper aims to propose a method to optimize the training effectiveness of deep neural networks, with the goal of improving their performance. Firstly, based on the observation that parameters (weights and bias) of deep neural network change in certain rules during training process, the potential of parameters prediction for improving training efficiency is discovered. Secondly, the potential of parameters prediction to improve the performance of deep neural network by noise injection introduced by prediction errors is revealed. And then, considering the limitations comprehensively, a deep neural network Parameters Linear Prediction method is exploit. Finally, performance and hyperparameter sensitivity validations are carried out on some representative backbones. Experimental results show that by employing proposed Parameters Linear Prediction method, as opposed to SGD, has led to an approximate 1\% increase in accuracy for optimal model, along with a reduction of about 0.01 in top-1/top-5 error. Moreover, it also exhibits stable performance under various hyperparameter settings, shown the effectiveness of the proposed method and validated its capacity in enhancing network’s training efficiency and performance.},
	language = {en},
	number = {1},
	urldate = {2025-03-13},
	journal = {Scientific Reports},
	author = {Ying, Hejie and Song, Mengmeng and Tang, Yaohong and Xiao, Shungen and Xiao, Zimin},
	month = jul,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Software},
	pages = {15197},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\T6QLFWE7\\Ying et al. - 2024 - Enhancing deep neural network training efficiency .pdf:application/pdf},
}

@misc{yan_triple-inertial_2025,
	title = {A {Triple}-{Inertial} {Accelerated} {Alternating} {Optimization} {Method} for {Deep} {Learning} {Training}},
	url = {http://arxiv.org/abs/2503.08489},
	doi = {10.48550/arXiv.2503.08489},
	abstract = {The stochastic gradient descent (SGD) algorithm has achieved remarkable success in training deep learning models. However, it has several limitations, including susceptibility to vanishing gradients, sensitivity to input data, and a lack of robust theoretical guarantees. In recent years, alternating minimization (AM) methods have emerged as a promising alternative for model training by employing gradient-free approaches to iteratively update model parameters. Despite their potential, these methods often exhibit slow convergence rates. To address this challenge, we propose a novel Triple-Inertial Accelerated Alternating Minimization (TIAM) framework for neural network training. The TIAM approach incorporates a triple-inertial acceleration strategy with a specialized approximation method, facilitating targeted acceleration of different terms in each sub-problem optimization. This integration improves the efficiency of convergence, achieving superior performance with fewer iterations. Additionally, we provide a convergence analysis of the TIAM algorithm, including its global convergence properties and convergence rate. Extensive experiments validate the effectiveness of the TIAM method, showing significant improvements in generalization capability and computational efficiency compared to existing approaches, particularly when applied to the rectified linear unit (ReLU) and its variants.},
	urldate = {2025-03-13},
	publisher = {arXiv},
	author = {Yan, Chengcheng and Xu, Jiawei and Wang, Qingsong and Peng, Zheng},
	month = mar,
	year = {2025},
	note = {arXiv:2503.08489 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\DUJLECPM\\Yan et al. - 2025 - A Triple-Inertial Accelerated Alternating Optimiza.pdf:application/pdf;Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\DJPQ2GVG\\2503.html:text/html},
}

@article{krasnoproshin_random_2024,
	title = {Random {Search} in {Neural} {Networks} {Training}},
	volume = {34},
	issn = {1555-6212},
	url = {https://doi.org/10.1134/S105466182470010X},
	doi = {10.1134/S105466182470010X},
	abstract = {The paper deals with a state-of-art applied problem related to the neural networks training. It is shown that, given the expansion of the range of practical problems, gradient methods do not always satisfy the conditions of the subject area, which contributes to the development of alternative training methods. An original training algorithm is proposed that implements the annealing method, for which convergence to the optimal solution is proven. A modified version of the algorithm has been developed that is invariant to the size of the training sample. Experimental studies (using the example of solving problems of image classification and color image compression) confirm the effectiveness of the proposed approach.},
	language = {en},
	number = {2},
	urldate = {2025-03-13},
	journal = {Pattern Recognition and Image Analysis},
	author = {Krasnoproshin, V. V. and Matskevich, V. V.},
	month = jun,
	year = {2024},
	keywords = {annealing method, Artificial Intelligence, genetic algorithm, gradient descent method, neural networks, random search, training},
	pages = {309--316},
	file = {Full Text PDF:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\YLVAN3T4\\Krasnoproshin and Matskevich - 2024 - Random Search in Neural Networks Training.pdf:application/pdf},
}

@article{wang_optimizing_2024,
	title = {Optimizing {Neural} {Network} {Training}: {A} {Markov} {Chain} {Approach} for {Resource} {Conservation}},
	issn = {2691-4581},
	shorttitle = {Optimizing {Neural} {Network} {Training}},
	url = {https://ieeexplore.ieee.org/document/10557251},
	doi = {10.1109/TAI.2024.3413688},
	abstract = {The additional resource consumption generated during the repeated training processes of neural networks, including time and computational power costs, is a problem of significant concern. We, therefore, propose a method that utilizes Markov Chain to predict the training outcomes of neural networks with the same structure. This method’s training is based on prior experience to optimize the parameter adjustment process, thereby reducing the number of times training must be started from scratch and lowering time costs. By predicting training outcomes and reducing forward and backward propagation computations, among other factors, computational resource consumption significantly decreases. Simultaneously, since Markov Chain represents a clear mathematical model, the properties of probability transition offer greater interpretability compared to traditional methods. In an era where explainable artificial intelligence is equally crucial, a more transparent training method could have greater application potential in many important scenarios. The dual benefits they provide exemplify the advantage of our approach. Regarding the critical part, we have theoretically and experimentally demonstrated that, under certain conditions, the neural network training process possesses Markov property and becomes a Markov process after clustering.},
	urldate = {2025-03-13},
	journal = {IEEE Transactions on Artificial Intelligence},
	author = {Wang, Ke and Huang, Xianting and Tan, Cong and Yiu, Siu-Ming and Chen, Zicong and Lei, Xiaolin},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Artificial Intelligence},
	keywords = {Artificial intelligence, Biological neural networks, Computational modeling, markov chain, Mathematical models, neural network, Neural networks, Predictive models, Training, Training prediction},
	pages = {1--14},
	file = {IEEE Xplore Abstract Record:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\ILT42EZ9\\10557251.html:text/html},
}

### Challenges technical training end

@article{raiaan_systematic_2024,
	title = {A systematic review of hyperparameter optimization techniques in {Convolutional} {Neural} {Networks}},
	volume = {11},
	issn = {2772-6622},
	url = {https://www.sciencedirect.com/science/article/pii/S2772662224000742},
	doi = {10.1016/j.dajour.2024.100470},
	abstract = {Convolutional Neural Network (CNN) is a prevalent topic in deep learning (DL) research for their architectural advantages. CNN relies heavily on hyperparameter configurations, and manually tuning these hyperparameters can be time-consuming for researchers, therefore we need efficient optimization techniques. In this systematic review, we explore a range of well used algorithms, including metaheuristic, statistical, sequential, and numerical approaches, to fine-tune CNN hyperparameters. Our research offers an exhaustive categorization of these hyperparameter optimization (HPO) algorithms and investigates the fundamental concepts of CNN, explaining the role of hyperparameters and their variants. Furthermore, an exhaustive literature review of HPO algorithms in CNN employing the above mentioned algorithms is undertaken. A comparative analysis is conducted based on their HPO strategies, error evaluation approaches, and accuracy results across various datasets to assess the efficacy of these methods. In addition to addressing current challenges in HPO, our research illuminates unresolved issues in the field. By providing insightful evaluations of the merits and demerits of various HPO algorithms, our objective is to assist researchers in determining a suitable method for a particular problem and dataset. By highlighting future research directions and synthesizing diversified knowledge, our survey contributes significantly to the ongoing development of CNN hyperparameter optimization.},
	urldate = {2025-03-10},
	journal = {Decision Analytics Journal},
	author = {Raiaan, Mohaimenul Azam Khan and Sakib, Sadman and Fahad, Nur Mohammad and Mamun, Abdullah Al and Rahman, Md. Anisur and Shatabda, Swakkhar and Mukta, Md. Saddam Hossain},
	month = jun,
	year = {2024},
	keywords = {Activation function, Convolutional Neural Network, Hyperparameter optimization Analysis, Optimizer},
	pages = {100470},
}

@misc{noauthor_alphafold_2025,
	title = {{AlphaFold}},
	url = {https://deepmind.google/technologies/alphafold/},
	abstract = {AlphaFold has revealed millions of intricate 3D protein structures, and is helping scientists understand how all of life’s molecules interact.},
	language = {en},
	urldate = {2025-03-10},
	journal = {Google DeepMind},
	month = feb,
	year = {2025},
	file = {Snapshot:D\:\\OneDrive - Universiti Sains Malaysia\\Research\\DL Survey\\Zotero\\storage\\H49L3L2H\\alphafold.html:text/html},
}